{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92b0a66-5f8b-4c01-a846-b9c3b76485ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mglur2\n",
      "\n",
      "torch.Size([791, 115])\n",
      "{'name': 'mglur2', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fccac5f1580>, 'pre_filter': None, '_indices': [21, 18, 1, 0, 11, 10, 17, 15, 14, 25, 3, 22, 5, 16, 9, 12, 6, 20, 8, 23, 19, 2, 24, 13, 4, 7], 'data': Data(x=[791, 115], edge_index=[2, 1734], edge_attr=[1734, 7], y=[26, 1], smiles=[26]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([  0,  33,  54,  74, 102, 135, 169, 203, 230, 251, 285, 320, 348, 376,\n",
      "        410, 444, 472, 506, 540, 576, 611, 632, 661, 695, 730, 764, 791]), 'edge_index': tensor([   0,   72,  118,  162,  224,  296,  370,  444,  504,  552,  626,  702,\n",
      "         764,  826,  900,  974, 1036, 1110, 1184, 1262, 1338, 1386, 1450, 1524,\n",
      "        1600, 1674, 1734]), 'edge_attr': tensor([   0,   72,  118,  162,  224,  296,  370,  444,  504,  552,  626,  702,\n",
      "         764,  826,  900,  974, 1036, 1110, 1184, 1262, 1338, 1386, 1450, 1524,\n",
      "        1600, 1674, 1734]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26])}), '_data_list': None}\n",
      "minv: 5.570000171661377\n",
      "maxv: 8.050000190734863\n",
      "emb_dim: 300\n",
      "16\n",
      "Epoch: 000, Loss: 1.0609 Val: 1.3054 Test: 1.8275\n",
      "16\n",
      "Epoch: 001, Loss: 1.0706 Val: 1.3165 Test: 1.8418\n",
      "16\n",
      "Epoch: 002, Loss: 1.0053 Val: 1.2897 Test: 1.8342\n",
      "16\n",
      "Epoch: 003, Loss: 0.9400 Val: 1.2776 Test: 1.8110\n",
      "16\n",
      "Epoch: 004, Loss: 0.8850 Val: 1.2684 Test: 1.7970\n",
      "16\n",
      "Epoch: 005, Loss: 0.9047 Val: 1.2679 Test: 1.7866\n",
      "16\n",
      "Epoch: 006, Loss: 0.8131 Val: 1.2636 Test: 1.7847\n",
      "16\n",
      "Epoch: 007, Loss: 0.7737 Val: 1.2605 Test: 1.7677\n",
      "16\n",
      "Epoch: 008, Loss: 0.7349 Val: 1.2531 Test: 1.7608\n",
      "16\n",
      "Epoch: 009, Loss: 0.6867 Val: 1.2477 Test: 1.7506\n",
      "16\n",
      "Epoch: 010, Loss: 0.7085 Val: 1.2410 Test: 1.7470\n",
      "16\n",
      "Epoch: 011, Loss: 0.6514 Val: 1.2333 Test: 1.7481\n",
      "16\n",
      "Epoch: 012, Loss: 0.8516 Val: 1.2248 Test: 1.7288\n",
      "16\n",
      "Epoch: 013, Loss: 0.7206 Val: 1.2202 Test: 1.7171\n",
      "16\n",
      "Epoch: 014, Loss: 0.6059 Val: 1.2145 Test: 1.6996\n",
      "16\n",
      "Epoch: 015, Loss: 0.6901 Val: 1.2083 Test: 1.6857\n",
      "16\n",
      "Epoch: 016, Loss: 0.6793 Val: 1.2037 Test: 1.6857\n",
      "16\n",
      "Epoch: 017, Loss: 0.7655 Val: 1.1994 Test: 1.6820\n",
      "16\n",
      "Epoch: 018, Loss: 0.6465 Val: 1.1944 Test: 1.6726\n",
      "16\n",
      "Epoch: 019, Loss: 0.6514 Val: 1.1883 Test: 1.6665\n",
      "16\n",
      "Epoch: 020, Loss: 0.6537 Val: 1.1809 Test: 1.6552\n",
      "16\n",
      "Epoch: 021, Loss: 0.5882 Val: 1.1719 Test: 1.6445\n",
      "16\n",
      "Epoch: 022, Loss: 0.6932 Val: 1.1650 Test: 1.6357\n",
      "16\n",
      "Epoch: 023, Loss: 0.5735 Val: 1.1544 Test: 1.6288\n",
      "16\n",
      "Epoch: 024, Loss: 0.6777 Val: 1.1463 Test: 1.6297\n",
      "16\n",
      "Epoch: 025, Loss: 0.6003 Val: 1.1417 Test: 1.6254\n",
      "16\n",
      "Epoch: 026, Loss: 0.5895 Val: 1.1391 Test: 1.6255\n",
      "16\n",
      "Epoch: 027, Loss: 0.5512 Val: 1.1369 Test: 1.6249\n",
      "16\n",
      "Epoch: 028, Loss: 0.5332 Val: 1.1332 Test: 1.6214\n",
      "16\n",
      "Epoch: 029, Loss: 0.6202 Val: 1.1283 Test: 1.6126\n",
      "16\n",
      "Epoch: 030, Loss: 0.5320 Val: 1.1252 Test: 1.6008\n",
      "16\n",
      "Epoch: 031, Loss: 0.5432 Val: 1.1220 Test: 1.5898\n",
      "16\n",
      "Epoch: 032, Loss: 0.5368 Val: 1.1163 Test: 1.5743\n",
      "16\n",
      "Epoch: 033, Loss: 0.5333 Val: 1.1117 Test: 1.5655\n",
      "16\n",
      "Epoch: 034, Loss: 0.5657 Val: 1.1083 Test: 1.5478\n",
      "16\n",
      "Epoch: 035, Loss: 0.6435 Val: 1.1055 Test: 1.5474\n",
      "16\n",
      "Epoch: 036, Loss: 0.5777 Val: 1.1012 Test: 1.5367\n",
      "16\n",
      "Epoch: 037, Loss: 0.6125 Val: 1.0988 Test: 1.5284\n",
      "16\n",
      "Epoch: 038, Loss: 0.5663 Val: 1.0953 Test: 1.5251\n",
      "16\n",
      "Epoch: 039, Loss: 0.5114 Val: 1.0944 Test: 1.5142\n",
      "16\n",
      "Epoch: 040, Loss: 0.5282 Val: 1.0946 Test: 1.5144\n",
      "16\n",
      "Epoch: 041, Loss: 0.5705 Val: 1.0919 Test: 1.5216\n",
      "16\n",
      "Epoch: 042, Loss: 0.5070 Val: 1.0900 Test: 1.5220\n",
      "16\n",
      "Epoch: 043, Loss: 0.6785 Val: 1.0852 Test: 1.4839\n",
      "16\n",
      "Epoch: 044, Loss: 0.5553 Val: 1.0816 Test: 1.4752\n",
      "16\n",
      "Epoch: 045, Loss: 0.9528 Val: 1.0801 Test: 1.4623\n",
      "16\n",
      "Epoch: 046, Loss: 0.8537 Val: 1.0796 Test: 1.4494\n",
      "16\n",
      "Epoch: 047, Loss: 0.5872 Val: 1.0760 Test: 1.4440\n",
      "16\n",
      "Epoch: 048, Loss: 0.6293 Val: 1.0704 Test: 1.4460\n",
      "16\n",
      "Epoch: 049, Loss: 0.5785 Val: 1.0624 Test: 1.4446\n",
      "16\n",
      "Epoch: 050, Loss: 0.5889 Val: 1.0524 Test: 1.4385\n",
      "16\n",
      "Epoch: 051, Loss: 0.4751 Val: 1.0417 Test: 1.4312\n",
      "16\n",
      "Epoch: 052, Loss: 0.5800 Val: 1.0314 Test: 1.4120\n",
      "16\n",
      "Epoch: 053, Loss: 0.6450 Val: 1.0272 Test: 1.4087\n",
      "16\n",
      "Epoch: 054, Loss: 0.6302 Val: 1.0259 Test: 1.4054\n",
      "16\n",
      "Epoch: 055, Loss: 0.5912 Val: 1.0263 Test: 1.3971\n",
      "16\n",
      "Epoch: 056, Loss: 0.5731 Val: 1.0343 Test: 1.3867\n",
      "16\n",
      "Epoch: 057, Loss: 0.5297 Val: 1.0420 Test: 1.3749\n",
      "16\n",
      "Epoch: 058, Loss: 0.5250 Val: 1.0448 Test: 1.3633\n",
      "16\n",
      "Epoch: 059, Loss: 0.5158 Val: 1.0446 Test: 1.3534\n",
      "16\n",
      "Epoch: 060, Loss: 0.5245 Val: 1.0431 Test: 1.3454\n",
      "16\n",
      "Epoch: 061, Loss: 0.6116 Val: 1.0404 Test: 1.3362\n",
      "16\n",
      "Epoch: 062, Loss: 0.4982 Val: 1.0368 Test: 1.3283\n",
      "16\n",
      "Epoch: 063, Loss: 0.5007 Val: 1.0333 Test: 1.3230\n",
      "16\n",
      "Epoch: 064, Loss: 0.5050 Val: 1.0295 Test: 1.3172\n",
      "16\n",
      "Epoch: 065, Loss: 0.4790 Val: 1.0251 Test: 1.3121\n",
      "16\n",
      "Epoch: 066, Loss: 0.4736 Val: 1.0172 Test: 1.3153\n",
      "16\n",
      "Epoch: 067, Loss: 0.5988 Val: 1.0085 Test: 1.3172\n",
      "16\n",
      "Epoch: 068, Loss: 0.5114 Val: 0.9678 Test: 1.2965\n",
      "16\n",
      "Epoch: 069, Loss: 0.5579 Val: 0.9394 Test: 1.2892\n",
      "16\n",
      "Epoch: 070, Loss: 0.9924 Val: 0.9613 Test: 1.2774\n",
      "16\n",
      "Epoch: 071, Loss: 0.5265 Val: 0.9612 Test: 1.2629\n",
      "16\n",
      "Epoch: 072, Loss: 0.4832 Val: 0.9596 Test: 1.2482\n",
      "16\n",
      "Epoch: 073, Loss: 0.4856 Val: 0.9572 Test: 1.2352\n",
      "16\n",
      "Epoch: 074, Loss: 0.4932 Val: 0.9552 Test: 1.2243\n",
      "16\n",
      "Epoch: 075, Loss: 0.5173 Val: 0.9569 Test: 1.2171\n",
      "16\n",
      "Epoch: 076, Loss: 0.4990 Val: 0.9673 Test: 1.2116\n",
      "16\n",
      "Epoch: 077, Loss: 0.4617 Val: 0.9767 Test: 1.2081\n",
      "16\n",
      "Epoch: 078, Loss: 0.4610 Val: 0.9796 Test: 1.2036\n",
      "16\n",
      "Epoch: 079, Loss: 0.4285 Val: 0.9800 Test: 1.2001\n",
      "16\n",
      "Epoch: 080, Loss: 0.5096 Val: 0.9795 Test: 1.1931\n",
      "16\n",
      "Epoch: 081, Loss: 0.4462 Val: 0.9790 Test: 1.1835\n",
      "16\n",
      "Epoch: 082, Loss: 0.4881 Val: 0.9786 Test: 1.1681\n",
      "16\n",
      "Epoch: 083, Loss: 0.5011 Val: 0.9658 Test: 1.1723\n",
      "16\n",
      "Epoch: 084, Loss: 0.4530 Val: 0.9550 Test: 1.1593\n",
      "16\n",
      "Epoch: 085, Loss: 0.4249 Val: 0.9512 Test: 1.1429\n",
      "16\n",
      "Epoch: 086, Loss: 0.5610 Val: 0.9475 Test: 1.1306\n",
      "16\n",
      "Epoch: 087, Loss: 0.5303 Val: 0.9422 Test: 1.1207\n",
      "16\n",
      "Epoch: 088, Loss: 0.4254 Val: 0.9470 Test: 1.1135\n",
      "16\n",
      "Epoch: 089, Loss: 0.4093 Val: 0.9502 Test: 1.1060\n",
      "16\n",
      "Epoch: 090, Loss: 0.5201 Val: 0.9523 Test: 1.0930\n",
      "16\n",
      "Epoch: 091, Loss: 0.4041 Val: 0.9524 Test: 1.0800\n",
      "16\n",
      "Epoch: 092, Loss: 0.4011 Val: 0.9517 Test: 1.0684\n",
      "16\n",
      "Epoch: 093, Loss: 0.3980 Val: 0.9506 Test: 1.0570\n",
      "16\n",
      "Epoch: 094, Loss: 0.4294 Val: 0.9466 Test: 1.0520\n",
      "16\n",
      "Epoch: 095, Loss: 0.4023 Val: 0.9407 Test: 1.0467\n",
      "16\n",
      "Epoch: 096, Loss: 0.4461 Val: 0.9364 Test: 1.0326\n",
      "16\n",
      "Epoch: 097, Loss: 0.3895 Val: 0.9319 Test: 1.0204\n",
      "16\n",
      "Epoch: 098, Loss: 0.3869 Val: 0.9275 Test: 1.0079\n",
      "16\n",
      "Epoch: 099, Loss: 0.4278 Val: 0.9241 Test: 1.0036\n",
      "16\n",
      "Epoch: 100, Loss: 0.3825 Val: 0.9220 Test: 0.9997\n",
      "16\n",
      "Epoch: 101, Loss: 0.3809 Val: 0.9191 Test: 0.9959\n",
      "16\n",
      "Epoch: 102, Loss: 0.3860 Val: 0.9202 Test: 0.9935\n",
      "16\n",
      "Epoch: 103, Loss: 0.3973 Val: 0.9215 Test: 0.9944\n",
      "16\n",
      "Epoch: 104, Loss: 0.3763 Val: 0.9218 Test: 0.9928\n",
      "16\n",
      "Epoch: 105, Loss: 0.3753 Val: 0.9203 Test: 0.9891\n",
      "16\n",
      "Epoch: 106, Loss: 0.4244 Val: 0.9182 Test: 0.9863\n",
      "16\n",
      "Epoch: 107, Loss: 0.4098 Val: 0.9156 Test: 0.9847\n",
      "16\n",
      "Epoch: 108, Loss: 0.3699 Val: 0.9124 Test: 0.9820\n",
      "16\n",
      "Epoch: 109, Loss: 0.3677 Val: 0.9092 Test: 0.9819\n",
      "16\n",
      "Epoch: 110, Loss: 0.3653 Val: 0.9061 Test: 0.9836\n",
      "16\n",
      "Epoch: 111, Loss: 0.3629 Val: 0.9031 Test: 0.9889\n",
      "16\n",
      "Epoch: 112, Loss: 0.3627 Val: 0.9051 Test: 0.9854\n",
      "16\n",
      "Epoch: 113, Loss: 0.3566 Val: 0.9071 Test: 0.9782\n",
      "16\n",
      "Epoch: 114, Loss: 0.3530 Val: 0.9090 Test: 0.9692\n",
      "16\n",
      "Epoch: 115, Loss: 0.3872 Val: 0.9118 Test: 0.9316\n",
      "16\n",
      "Epoch: 116, Loss: 0.3464 Val: 0.9172 Test: 0.9094\n",
      "16\n",
      "Epoch: 117, Loss: 0.3390 Val: 0.9219 Test: 0.8887\n",
      "16\n",
      "Epoch: 118, Loss: 0.3340 Val: 0.9245 Test: 0.8660\n",
      "16\n",
      "Epoch: 119, Loss: 0.3288 Val: 0.9260 Test: 0.8412\n",
      "16\n",
      "Epoch: 120, Loss: 0.3715 Val: 0.9281 Test: 0.8443\n",
      "16\n",
      "Epoch: 121, Loss: 0.3213 Val: 0.9299 Test: 0.8441\n",
      "16\n",
      "Epoch: 122, Loss: 0.3187 Val: 0.9315 Test: 0.8406\n",
      "16\n",
      "Epoch: 123, Loss: 0.3156 Val: 0.9329 Test: 0.8364\n",
      "16\n",
      "Epoch: 124, Loss: 0.3157 Val: 0.9357 Test: 0.8234\n",
      "16\n",
      "Epoch: 125, Loss: 0.3076 Val: 0.9386 Test: 0.8097\n",
      "16\n",
      "Epoch: 126, Loss: 0.3077 Val: 0.9399 Test: 0.7775\n",
      "16\n",
      "Epoch: 127, Loss: 0.2984 Val: 0.9413 Test: 0.7619\n",
      "16\n",
      "Epoch: 128, Loss: 0.3538 Val: 0.9460 Test: 0.7434\n",
      "16\n",
      "Epoch: 129, Loss: 0.2905 Val: 0.9501 Test: 0.7247\n",
      "16\n",
      "Epoch: 130, Loss: 0.2873 Val: 0.9538 Test: 0.7063\n",
      "16\n",
      "Epoch: 131, Loss: 0.2843 Val: 0.9572 Test: 0.6887\n",
      "16\n",
      "Epoch: 132, Loss: 0.2812 Val: 0.9602 Test: 0.6726\n",
      "16\n",
      "Epoch: 133, Loss: 0.2784 Val: 0.9630 Test: 0.6580\n",
      "16\n",
      "Epoch: 134, Loss: 0.2757 Val: 0.9655 Test: 0.6445\n",
      "16\n",
      "Epoch: 135, Loss: 0.2727 Val: 0.9679 Test: 0.6312\n",
      "16\n",
      "Epoch: 136, Loss: 0.2696 Val: 0.9700 Test: 0.6183\n",
      "16\n",
      "Epoch: 137, Loss: 0.2669 Val: 0.9717 Test: 0.6055\n",
      "16\n",
      "Epoch: 138, Loss: 0.2645 Val: 0.9730 Test: 0.5928\n",
      "16\n",
      "Epoch: 139, Loss: 0.2618 Val: 0.9739 Test: 0.5802\n",
      "16\n",
      "Epoch: 140, Loss: 0.2593 Val: 0.9744 Test: 0.5678\n",
      "16\n",
      "Epoch: 141, Loss: 0.2566 Val: 0.9745 Test: 0.5557\n",
      "16\n",
      "Epoch: 142, Loss: 0.2536 Val: 0.9743 Test: 0.5437\n",
      "16\n",
      "Epoch: 143, Loss: 0.2504 Val: 0.9735 Test: 0.5317\n",
      "16\n",
      "Epoch: 144, Loss: 0.2652 Val: 0.9684 Test: 0.5524\n",
      "16\n",
      "Epoch: 145, Loss: 0.3430 Val: 0.9790 Test: 0.5444\n",
      "16\n",
      "Epoch: 146, Loss: 0.5901 Val: 0.9838 Test: 0.5296\n",
      "16\n",
      "Epoch: 147, Loss: 0.4095 Val: 0.9408 Test: 0.6937\n",
      "16\n",
      "Epoch: 148, Loss: 0.3706 Val: 0.9302 Test: 0.8106\n",
      "16\n",
      "Epoch: 149, Loss: 0.9984 Val: 0.9258 Test: 0.7892\n",
      "16\n",
      "Epoch: 150, Loss: 0.9458 Val: 0.9264 Test: 0.6984\n",
      "16\n",
      "Epoch: 151, Loss: 0.3797 Val: 0.9324 Test: 0.5802\n",
      "16\n",
      "Epoch: 152, Loss: 0.4059 Val: 0.9412 Test: 0.4801\n",
      "16\n",
      "Epoch: 153, Loss: 0.8193 Val: 0.9209 Test: 0.6902\n",
      "16\n",
      "Epoch: 154, Loss: 0.3897 Val: 0.9057 Test: 0.8417\n",
      "16\n",
      "Epoch: 155, Loss: 0.5138 Val: 0.8949 Test: 0.8647\n",
      "16\n",
      "Epoch: 156, Loss: 0.5223 Val: 0.9133 Test: 0.8239\n",
      "16\n",
      "Epoch: 157, Loss: 0.3423 Val: 0.9268 Test: 0.4771\n",
      "16\n",
      "Epoch: 158, Loss: 0.3539 Val: 0.9252 Test: 0.3945\n",
      "16\n",
      "Epoch: 159, Loss: 0.3324 Val: 0.9198 Test: 0.3985\n",
      "16\n",
      "Epoch: 160, Loss: 0.3242 Val: 0.9095 Test: 0.4016\n",
      "16\n",
      "Epoch: 161, Loss: 0.3382 Val: 0.9016 Test: 0.4017\n",
      "16\n",
      "Epoch: 162, Loss: 0.3435 Val: 0.8946 Test: 0.4023\n",
      "16\n",
      "Epoch: 163, Loss: 0.3330 Val: 0.8966 Test: 0.3864\n",
      "16\n",
      "Epoch: 164, Loss: 0.3369 Val: 0.8986 Test: 0.3722\n",
      "16\n",
      "Epoch: 165, Loss: 0.3188 Val: 0.9006 Test: 0.3610\n",
      "16\n",
      "Epoch: 166, Loss: 0.3384 Val: 0.9123 Test: 0.3538\n",
      "16\n",
      "Epoch: 167, Loss: 0.3809 Val: 0.9243 Test: 0.3465\n",
      "16\n",
      "Epoch: 168, Loss: 0.3796 Val: 0.9334 Test: 0.3386\n",
      "16\n",
      "Epoch: 169, Loss: 0.3230 Val: 0.9367 Test: 0.3199\n",
      "16\n",
      "Epoch: 170, Loss: 0.3693 Val: 0.9372 Test: 0.3022\n",
      "16\n",
      "Epoch: 171, Loss: 0.3197 Val: 0.9373 Test: 0.2867\n",
      "16\n",
      "Epoch: 172, Loss: 0.3552 Val: 0.9349 Test: 0.2713\n",
      "16\n",
      "Epoch: 173, Loss: 0.3017 Val: 0.9301 Test: 0.2555\n",
      "16\n",
      "Epoch: 174, Loss: 0.2983 Val: 0.9243 Test: 0.2409\n",
      "16\n",
      "Epoch: 175, Loss: 0.2683 Val: 0.9182 Test: 0.2288\n",
      "16\n",
      "Epoch: 176, Loss: 0.2541 Val: 0.9133 Test: 0.2199\n",
      "16\n",
      "Epoch: 177, Loss: 0.3125 Val: 0.9073 Test: 0.2107\n",
      "16\n",
      "Epoch: 178, Loss: 0.2941 Val: 0.9014 Test: 0.2020\n",
      "16\n",
      "Epoch: 179, Loss: 0.3323 Val: 0.8961 Test: 0.1992\n",
      "16\n",
      "Epoch: 180, Loss: 0.1988 Val: 0.8925 Test: 0.2074\n",
      "16\n",
      "Epoch: 181, Loss: 0.3593 Val: 0.8927 Test: 0.2059\n",
      "16\n",
      "Epoch: 182, Loss: 0.3085 Val: 0.8878 Test: 0.2015\n",
      "16\n",
      "Epoch: 183, Loss: 0.3655 Val: 0.9248 Test: 0.1979\n",
      "16\n",
      "Epoch: 184, Loss: 0.3146 Val: 0.9776 Test: 0.2354\n",
      "16\n",
      "Epoch: 185, Loss: 0.3488 Val: 0.9702 Test: 0.2736\n",
      "16\n",
      "Epoch: 186, Loss: 0.3676 Val: 0.9690 Test: 0.2860\n",
      "16\n",
      "Epoch: 187, Loss: 0.3484 Val: 0.9699 Test: 0.2703\n",
      "16\n",
      "Epoch: 188, Loss: 0.3961 Val: 0.9743 Test: 0.2493\n",
      "16\n",
      "Epoch: 189, Loss: 0.3401 Val: 0.9820 Test: 0.2149\n",
      "16\n",
      "Epoch: 190, Loss: 0.2878 Val: 0.9904 Test: 0.1751\n",
      "16\n",
      "Epoch: 191, Loss: 0.2765 Val: 0.9981 Test: 0.1441\n",
      "16\n",
      "Epoch: 192, Loss: 0.2517 Val: 1.0043 Test: 0.1326\n",
      "16\n",
      "Epoch: 193, Loss: 0.2482 Val: 1.0090 Test: 0.1339\n",
      "16\n",
      "Epoch: 194, Loss: 0.2683 Val: 1.0128 Test: 0.1372\n",
      "16\n",
      "Epoch: 195, Loss: 0.2829 Val: 1.0145 Test: 0.1349\n",
      "16\n",
      "Epoch: 196, Loss: 0.2697 Val: 1.0150 Test: 0.1336\n",
      "16\n",
      "Epoch: 197, Loss: 0.2657 Val: 1.0204 Test: 0.1341\n",
      "16\n",
      "Epoch: 198, Loss: 0.2837 Val: 1.0291 Test: 0.1366\n",
      "16\n",
      "Epoch: 199, Loss: 0.2781 Val: 1.0360 Test: 0.1402\n",
      "usp7\n",
      "\n",
      "torch.Size([1506, 115])\n",
      "{'name': 'usp7', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe05044f0>, 'pre_filter': None, '_indices': [9, 23, 42, 41, 15, 5, 26, 30, 13, 11, 19, 35, 31, 0, 21, 44, 8, 24, 38, 18, 39, 17, 40, 43, 33, 27, 20, 6, 12, 2, 16, 28, 1, 29, 37, 7, 3, 36, 10, 22, 34, 14, 4, 32, 25], 'data': Data(x=[1506, 115], edge_index=[2, 3304], edge_attr=[3304, 7], y=[45, 1], smiles=[45]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   37,   70,  103,  132,  162,  193,  230,  262,  291,  322,  361,\n",
      "         394,  426,  456,  485,  516,  550,  585,  621,  661,  697,  735,  768,\n",
      "         806,  849,  887,  916,  951,  990, 1027, 1060, 1094, 1122, 1151, 1181,\n",
      "        1215, 1247, 1280, 1308, 1345, 1373, 1399, 1427, 1468, 1506]), 'edge_index': tensor([   0,   82,  154,  224,  288,  354,  420,  502,  570,  634,  704,  790,\n",
      "         862,  930,  996, 1060, 1128, 1202, 1278, 1356, 1444, 1524, 1608, 1680,\n",
      "        1764, 1860, 1944, 2008, 2086, 2172, 2254, 2324, 2398, 2460, 2524, 2590,\n",
      "        2666, 2736, 2808, 2870, 2952, 3014, 3070, 3130, 3220, 3304]), 'edge_attr': tensor([   0,   82,  154,  224,  288,  354,  420,  502,  570,  634,  704,  790,\n",
      "         862,  930,  996, 1060, 1128, 1202, 1278, 1356, 1444, 1524, 1608, 1680,\n",
      "        1764, 1860, 1944, 2008, 2086, 2172, 2254, 2324, 2398, 2460, 2524, 2590,\n",
      "        2666, 2736, 2808, 2870, 2952, 3014, 3070, 3130, 3220, 3304]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45])}), '_data_list': None}\n",
      "minv: 4.050000190734863\n",
      "maxv: 8.220000267028809\n",
      "emb_dim: 300\n",
      "16\n",
      "11\n",
      "Epoch: 000, Loss: 1.0604 Val: 1.5886 Test: 2.5248\n",
      "16\n",
      "11\n",
      "Epoch: 001, Loss: 0.8855 Val: 1.5888 Test: 2.5206\n",
      "16\n",
      "11\n",
      "Epoch: 002, Loss: 0.9668 Val: 1.5582 Test: 2.4894\n",
      "16\n",
      "11\n",
      "Epoch: 003, Loss: 0.9688 Val: 1.5321 Test: 2.4729\n",
      "16\n",
      "11\n",
      "Epoch: 004, Loss: 0.9971 Val: 1.5109 Test: 2.4376\n",
      "16\n",
      "11\n",
      "Epoch: 005, Loss: 0.8981 Val: 1.4935 Test: 2.4021\n",
      "16\n",
      "11\n",
      "Epoch: 006, Loss: 0.9622 Val: 1.4803 Test: 2.3940\n",
      "16\n",
      "11\n",
      "Epoch: 007, Loss: 0.9260 Val: 1.4559 Test: 2.3718\n",
      "16\n",
      "11\n",
      "Epoch: 008, Loss: 0.9056 Val: 1.4321 Test: 2.3345\n",
      "16\n",
      "11\n",
      "Epoch: 009, Loss: 0.8812 Val: 1.4087 Test: 2.3127\n",
      "16\n",
      "11\n",
      "Epoch: 010, Loss: 0.9123 Val: 1.3822 Test: 2.2914\n",
      "16\n",
      "11\n",
      "Epoch: 011, Loss: 0.9187 Val: 1.3584 Test: 2.2657\n",
      "16\n",
      "11\n",
      "Epoch: 012, Loss: 1.0082 Val: 1.3480 Test: 2.2458\n",
      "16\n",
      "11\n",
      "Epoch: 013, Loss: 0.8707 Val: 1.3354 Test: 2.2206\n",
      "16\n",
      "11\n",
      "Epoch: 014, Loss: 0.8909 Val: 1.3222 Test: 2.1978\n",
      "16\n",
      "11\n",
      "Epoch: 015, Loss: 0.9411 Val: 1.3180 Test: 2.1820\n",
      "16\n",
      "11\n",
      "Epoch: 016, Loss: 0.9813 Val: 1.3164 Test: 2.1736\n",
      "16\n",
      "11\n",
      "Epoch: 017, Loss: 0.9839 Val: 1.3083 Test: 2.1792\n",
      "16\n",
      "11\n",
      "Epoch: 018, Loss: 0.9121 Val: 1.2897 Test: 2.1536\n",
      "16\n",
      "11\n",
      "Epoch: 019, Loss: 0.9138 Val: 1.2659 Test: 2.1513\n",
      "16\n",
      "11\n",
      "Epoch: 020, Loss: 0.8955 Val: 1.2221 Test: 2.1029\n",
      "16\n",
      "11\n",
      "Epoch: 021, Loss: 0.9438 Val: 1.2003 Test: 2.0739\n",
      "16\n",
      "11\n",
      "Epoch: 022, Loss: 0.9509 Val: 1.1937 Test: 2.0728\n",
      "16\n",
      "11\n",
      "Epoch: 023, Loss: 0.7697 Val: 1.1813 Test: 2.0708\n",
      "16\n",
      "11\n",
      "Epoch: 024, Loss: 0.9055 Val: 1.1698 Test: 2.0661\n",
      "16\n",
      "11\n",
      "Epoch: 025, Loss: 0.9393 Val: 1.1562 Test: 2.0556\n",
      "16\n",
      "11\n",
      "Epoch: 026, Loss: 0.9162 Val: 1.1354 Test: 2.0320\n",
      "16\n",
      "11\n",
      "Epoch: 027, Loss: 0.9636 Val: 1.1233 Test: 2.0044\n",
      "16\n",
      "11\n",
      "Epoch: 028, Loss: 0.9058 Val: 1.1089 Test: 1.9671\n",
      "16\n",
      "11\n",
      "Epoch: 029, Loss: 0.9313 Val: 1.0853 Test: 1.9339\n",
      "16\n",
      "11\n",
      "Epoch: 030, Loss: 0.8691 Val: 1.0632 Test: 1.9167\n",
      "16\n",
      "11\n",
      "Epoch: 031, Loss: 0.9031 Val: 1.0529 Test: 1.9132\n",
      "16\n",
      "11\n",
      "Epoch: 032, Loss: 0.8620 Val: 1.0322 Test: 1.8899\n",
      "16\n",
      "11\n",
      "Epoch: 033, Loss: 0.8569 Val: 1.0066 Test: 1.8619\n",
      "16\n",
      "11\n",
      "Epoch: 034, Loss: 0.8299 Val: 1.0050 Test: 1.8544\n",
      "16\n",
      "11\n",
      "Epoch: 035, Loss: 0.7828 Val: 0.9785 Test: 1.8165\n",
      "16\n",
      "11\n",
      "Epoch: 036, Loss: 0.8029 Val: 0.9587 Test: 1.7815\n",
      "16\n",
      "11\n",
      "Epoch: 037, Loss: 0.8327 Val: 0.9424 Test: 1.7551\n",
      "16\n",
      "11\n",
      "Epoch: 038, Loss: 0.8007 Val: 0.9306 Test: 1.7251\n",
      "16\n",
      "11\n",
      "Epoch: 039, Loss: 0.7599 Val: 0.8908 Test: 1.7098\n",
      "16\n",
      "11\n",
      "Epoch: 040, Loss: 0.7734 Val: 0.8575 Test: 1.6796\n",
      "16\n",
      "11\n",
      "Epoch: 041, Loss: 0.7298 Val: 0.8384 Test: 1.6421\n",
      "16\n",
      "11\n",
      "Epoch: 042, Loss: 0.7826 Val: 0.8432 Test: 1.6084\n",
      "16\n",
      "11\n",
      "Epoch: 043, Loss: 0.8267 Val: 0.8425 Test: 1.5858\n",
      "16\n",
      "11\n",
      "Epoch: 044, Loss: 0.6359 Val: 0.8431 Test: 1.5606\n",
      "16\n",
      "11\n",
      "Epoch: 045, Loss: 0.7404 Val: 0.8250 Test: 1.5463\n",
      "16\n",
      "11\n",
      "Epoch: 046, Loss: 0.6771 Val: 0.8014 Test: 1.5525\n",
      "16\n",
      "11\n",
      "Epoch: 047, Loss: 0.7069 Val: 0.7829 Test: 1.5391\n",
      "16\n",
      "11\n",
      "Epoch: 048, Loss: 0.7039 Val: 0.7985 Test: 1.5237\n",
      "16\n",
      "11\n",
      "Epoch: 049, Loss: 0.7399 Val: 0.7752 Test: 1.4964\n",
      "16\n",
      "11\n",
      "Epoch: 050, Loss: 0.6101 Val: 0.7465 Test: 1.4735\n",
      "16\n",
      "11\n",
      "Epoch: 051, Loss: 0.7266 Val: 0.7341 Test: 1.4468\n",
      "16\n",
      "11\n",
      "Epoch: 052, Loss: 0.7311 Val: 0.7215 Test: 1.4063\n",
      "16\n",
      "11\n",
      "Epoch: 053, Loss: 0.7357 Val: 0.7093 Test: 1.3977\n",
      "16\n",
      "11\n",
      "Epoch: 054, Loss: 0.6084 Val: 0.7092 Test: 1.3840\n",
      "16\n",
      "11\n",
      "Epoch: 055, Loss: 0.6398 Val: 0.7505 Test: 1.3380\n",
      "16\n",
      "11\n",
      "Epoch: 056, Loss: 0.7174 Val: 0.7740 Test: 1.3085\n",
      "16\n",
      "11\n",
      "Epoch: 057, Loss: 0.7112 Val: 0.8081 Test: 1.2539\n",
      "16\n",
      "11\n",
      "Epoch: 058, Loss: 0.6091 Val: 0.7976 Test: 1.2367\n",
      "16\n",
      "11\n",
      "Epoch: 059, Loss: 0.5834 Val: 0.7968 Test: 1.2261\n",
      "16\n",
      "11\n",
      "Epoch: 060, Loss: 0.7603 Val: 0.8049 Test: 1.2220\n",
      "16\n",
      "11\n",
      "Epoch: 061, Loss: 0.7414 Val: 0.8081 Test: 1.2110\n",
      "16\n",
      "11\n",
      "Epoch: 062, Loss: 0.6832 Val: 0.8392 Test: 1.1764\n",
      "16\n",
      "11\n",
      "Epoch: 063, Loss: 0.5740 Val: 0.8653 Test: 1.1978\n",
      "16\n",
      "11\n",
      "Epoch: 064, Loss: 0.6013 Val: 0.9024 Test: 1.1905\n",
      "16\n",
      "11\n",
      "Epoch: 065, Loss: 0.7086 Val: 0.9326 Test: 1.1517\n",
      "16\n",
      "11\n",
      "Epoch: 066, Loss: 0.6073 Val: 0.9729 Test: 1.0906\n",
      "16\n",
      "11\n",
      "Epoch: 067, Loss: 0.6399 Val: 1.0043 Test: 1.0352\n",
      "16\n",
      "11\n",
      "Epoch: 068, Loss: 0.6750 Val: 1.0358 Test: 0.9923\n",
      "16\n",
      "11\n",
      "Epoch: 069, Loss: 0.4916 Val: 1.0675 Test: 0.9800\n",
      "16\n",
      "11\n",
      "Epoch: 070, Loss: 0.6083 Val: 1.0887 Test: 0.9827\n",
      "16\n",
      "11\n",
      "Epoch: 071, Loss: 0.6416 Val: 1.0973 Test: 0.9833\n",
      "16\n",
      "11\n",
      "Epoch: 072, Loss: 0.7566 Val: 1.1166 Test: 0.9809\n",
      "16\n",
      "11\n",
      "Epoch: 073, Loss: 0.6731 Val: 1.1321 Test: 0.9589\n",
      "16\n",
      "11\n",
      "Epoch: 074, Loss: 0.5525 Val: 1.1360 Test: 0.9506\n",
      "16\n",
      "11\n",
      "Epoch: 075, Loss: 0.5578 Val: 1.1418 Test: 0.9640\n",
      "16\n",
      "11\n",
      "Epoch: 076, Loss: 0.5620 Val: 1.1291 Test: 0.9920\n",
      "16\n",
      "11\n",
      "Epoch: 077, Loss: 0.6772 Val: 1.1131 Test: 1.0017\n",
      "16\n",
      "11\n",
      "Epoch: 078, Loss: 0.5100 Val: 1.1177 Test: 1.0014\n",
      "16\n",
      "11\n",
      "Epoch: 079, Loss: 0.6353 Val: 1.1235 Test: 0.9998\n",
      "16\n",
      "11\n",
      "Epoch: 080, Loss: 0.5626 Val: 1.1403 Test: 0.9961\n",
      "16\n",
      "11\n",
      "Epoch: 081, Loss: 0.5696 Val: 1.1613 Test: 0.9901\n",
      "16\n",
      "11\n",
      "Epoch: 082, Loss: 0.5776 Val: 1.1812 Test: 0.9799\n",
      "16\n",
      "11\n",
      "Epoch: 083, Loss: 0.6188 Val: 1.1931 Test: 0.9696\n",
      "16\n",
      "11\n",
      "Epoch: 084, Loss: 0.6339 Val: 1.2014 Test: 0.9569\n",
      "16\n",
      "11\n",
      "Epoch: 085, Loss: 0.6644 Val: 1.2232 Test: 0.9407\n",
      "16\n",
      "11\n",
      "Epoch: 086, Loss: 0.6080 Val: 1.2500 Test: 0.9319\n",
      "16\n",
      "11\n",
      "Epoch: 087, Loss: 0.5089 Val: 1.2692 Test: 0.9275\n",
      "16\n",
      "11\n",
      "Epoch: 088, Loss: 0.6048 Val: 1.2658 Test: 0.9194\n",
      "16\n",
      "11\n",
      "Epoch: 089, Loss: 0.6982 Val: 1.2469 Test: 0.9142\n",
      "16\n",
      "11\n",
      "Epoch: 090, Loss: 0.6633 Val: 1.2419 Test: 0.9118\n",
      "16\n",
      "11\n",
      "Epoch: 091, Loss: 0.5226 Val: 1.2283 Test: 0.9205\n",
      "16\n",
      "11\n",
      "Epoch: 092, Loss: 0.6071 Val: 1.2148 Test: 0.9206\n",
      "16\n",
      "11\n",
      "Epoch: 093, Loss: 0.4693 Val: 1.2040 Test: 0.9120\n",
      "16\n",
      "11\n",
      "Epoch: 094, Loss: 0.4730 Val: 1.2185 Test: 0.9116\n",
      "16\n",
      "11\n",
      "Epoch: 095, Loss: 0.5531 Val: 1.2632 Test: 0.9244\n",
      "16\n",
      "11\n",
      "Epoch: 096, Loss: 0.5944 Val: 1.3296 Test: 0.9487\n",
      "16\n",
      "11\n",
      "Epoch: 097, Loss: 0.6603 Val: 1.3635 Test: 0.9634\n",
      "16\n",
      "11\n",
      "Epoch: 098, Loss: 0.4776 Val: 1.3895 Test: 0.9721\n",
      "16\n",
      "11\n",
      "Epoch: 099, Loss: 0.5898 Val: 1.4093 Test: 0.9708\n",
      "16\n",
      "11\n",
      "Epoch: 100, Loss: 0.6359 Val: 1.4227 Test: 0.9539\n",
      "16\n",
      "11\n",
      "Epoch: 101, Loss: 0.4847 Val: 1.4268 Test: 0.9438\n",
      "16\n",
      "11\n",
      "Epoch: 102, Loss: 0.5762 Val: 1.4215 Test: 0.9328\n",
      "16\n",
      "11\n",
      "Epoch: 103, Loss: 0.5105 Val: 1.4062 Test: 0.9254\n",
      "16\n",
      "11\n",
      "Epoch: 104, Loss: 0.3888 Val: 1.3925 Test: 0.9220\n",
      "16\n",
      "11\n",
      "Epoch: 105, Loss: 0.6143 Val: 1.3798 Test: 0.9177\n",
      "16\n",
      "11\n",
      "Epoch: 106, Loss: 0.3442 Val: 1.3564 Test: 0.9218\n",
      "16\n",
      "11\n",
      "Epoch: 107, Loss: 0.5343 Val: 1.3248 Test: 0.9434\n",
      "16\n",
      "11\n",
      "Epoch: 108, Loss: 0.4498 Val: 1.2959 Test: 0.9562\n",
      "16\n",
      "11\n",
      "Epoch: 109, Loss: 0.7577 Val: 1.2795 Test: 0.9651\n",
      "16\n",
      "11\n",
      "Epoch: 110, Loss: 0.6513 Val: 1.2743 Test: 0.9700\n",
      "16\n",
      "11\n",
      "Epoch: 111, Loss: 0.5348 Val: 1.2748 Test: 0.9763\n",
      "16\n",
      "11\n",
      "Epoch: 112, Loss: 0.5145 Val: 1.2791 Test: 0.9809\n",
      "16\n",
      "11\n",
      "Epoch: 113, Loss: 0.5453 Val: 1.2843 Test: 0.9853\n",
      "16\n",
      "11\n",
      "Epoch: 114, Loss: 0.7617 Val: 1.2763 Test: 0.9850\n",
      "16\n",
      "11\n",
      "Epoch: 115, Loss: 0.6048 Val: 1.2686 Test: 0.9781\n",
      "16\n",
      "11\n",
      "Epoch: 116, Loss: 0.5247 Val: 1.2676 Test: 0.9728\n",
      "16\n",
      "11\n",
      "Epoch: 117, Loss: 0.5473 Val: 1.2886 Test: 0.9689\n",
      "16\n",
      "11\n",
      "Epoch: 118, Loss: 0.6254 Val: 1.2942 Test: 0.9634\n",
      "16\n",
      "11\n",
      "Epoch: 119, Loss: 0.5012 Val: 1.2886 Test: 0.9683\n",
      "16\n",
      "11\n",
      "Epoch: 120, Loss: 0.5633 Val: 1.2820 Test: 0.9763\n",
      "16\n",
      "11\n",
      "Epoch: 121, Loss: 0.5412 Val: 1.2761 Test: 0.9821\n",
      "16\n",
      "11\n",
      "Epoch: 122, Loss: 0.4215 Val: 1.2640 Test: 0.9809\n",
      "16\n",
      "11\n",
      "Epoch: 123, Loss: 0.5482 Val: 1.2642 Test: 0.9857\n",
      "16\n",
      "11\n",
      "Epoch: 124, Loss: 0.4911 Val: 1.2612 Test: 0.9889\n",
      "16\n",
      "11\n",
      "Epoch: 125, Loss: 0.5846 Val: 1.2593 Test: 0.9950\n",
      "16\n",
      "11\n",
      "Epoch: 126, Loss: 0.4542 Val: 1.2537 Test: 1.0050\n",
      "16\n",
      "11\n",
      "Epoch: 127, Loss: 0.5587 Val: 1.2381 Test: 1.0111\n",
      "16\n",
      "11\n",
      "Epoch: 128, Loss: 0.6512 Val: 1.2141 Test: 1.0097\n",
      "16\n",
      "11\n",
      "Epoch: 129, Loss: 0.5465 Val: 1.1828 Test: 1.0035\n",
      "16\n",
      "11\n",
      "Epoch: 130, Loss: 0.4907 Val: 1.1415 Test: 0.9953\n",
      "16\n",
      "11\n",
      "Epoch: 131, Loss: 0.5591 Val: 1.1183 Test: 0.9854\n",
      "16\n",
      "11\n",
      "Epoch: 132, Loss: 0.5115 Val: 1.1079 Test: 0.9750\n",
      "16\n",
      "11\n",
      "Epoch: 133, Loss: 0.4916 Val: 1.1058 Test: 0.9683\n",
      "16\n",
      "11\n",
      "Epoch: 134, Loss: 0.6276 Val: 1.1127 Test: 0.9621\n",
      "16\n",
      "11\n",
      "Epoch: 135, Loss: 0.3326 Val: 1.1197 Test: 0.9577\n",
      "16\n",
      "11\n",
      "Epoch: 136, Loss: 0.5458 Val: 1.1485 Test: 0.9518\n",
      "16\n",
      "11\n",
      "Epoch: 137, Loss: 0.4284 Val: 1.1650 Test: 0.9460\n",
      "16\n",
      "11\n",
      "Epoch: 138, Loss: 0.5371 Val: 1.1758 Test: 0.9541\n",
      "16\n",
      "11\n",
      "Epoch: 139, Loss: 0.8233 Val: 1.1740 Test: 0.9512\n",
      "16\n",
      "11\n",
      "Epoch: 140, Loss: 0.5401 Val: 1.1575 Test: 0.9629\n",
      "16\n",
      "11\n",
      "Epoch: 141, Loss: 0.5037 Val: 1.1444 Test: 1.0204\n",
      "16\n",
      "11\n",
      "Epoch: 142, Loss: 0.7646 Val: 1.1139 Test: 1.0362\n",
      "16\n",
      "11\n",
      "Epoch: 143, Loss: 0.3302 Val: 1.1044 Test: 1.0336\n",
      "16\n",
      "11\n",
      "Epoch: 144, Loss: 0.6524 Val: 1.1343 Test: 1.0234\n",
      "16\n",
      "11\n",
      "Epoch: 145, Loss: 0.6076 Val: 1.1855 Test: 1.0173\n",
      "16\n",
      "11\n",
      "Epoch: 146, Loss: 0.6412 Val: 1.2294 Test: 1.0157\n",
      "16\n",
      "11\n",
      "Epoch: 147, Loss: 0.3846 Val: 1.2480 Test: 1.0140\n",
      "16\n",
      "11\n",
      "Epoch: 148, Loss: 0.5038 Val: 1.2479 Test: 1.0153\n",
      "16\n",
      "11\n",
      "Epoch: 149, Loss: 0.5363 Val: 1.2551 Test: 1.0104\n",
      "16\n",
      "11\n",
      "Epoch: 150, Loss: 0.5576 Val: 1.2783 Test: 1.0252\n",
      "16\n",
      "11\n",
      "Epoch: 151, Loss: 0.5508 Val: 1.2745 Test: 1.0647\n",
      "16\n",
      "11\n",
      "Epoch: 152, Loss: 0.4627 Val: 1.2500 Test: 1.0741\n",
      "16\n",
      "11\n",
      "Epoch: 153, Loss: 0.5328 Val: 1.2146 Test: 1.0686\n",
      "16\n",
      "11\n",
      "Epoch: 154, Loss: 0.5016 Val: 1.1910 Test: 1.0499\n",
      "16\n",
      "11\n",
      "Epoch: 155, Loss: 0.6111 Val: 1.1814 Test: 1.0198\n",
      "16\n",
      "11\n",
      "Epoch: 156, Loss: 0.5411 Val: 1.1715 Test: 0.9848\n",
      "16\n",
      "11\n",
      "Epoch: 157, Loss: 0.6143 Val: 1.1863 Test: 0.9599\n",
      "16\n",
      "11\n",
      "Epoch: 158, Loss: 0.5960 Val: 1.1946 Test: 0.9502\n",
      "16\n",
      "11\n",
      "Epoch: 159, Loss: 0.5625 Val: 1.2158 Test: 0.9344\n",
      "16\n",
      "11\n",
      "Epoch: 160, Loss: 0.4955 Val: 1.2305 Test: 0.9678\n",
      "16\n",
      "11\n",
      "Epoch: 161, Loss: 0.4502 Val: 1.2457 Test: 0.9932\n",
      "16\n",
      "11\n",
      "Epoch: 162, Loss: 0.5712 Val: 1.2769 Test: 0.9974\n",
      "16\n",
      "11\n",
      "Epoch: 163, Loss: 0.3897 Val: 1.3059 Test: 0.9865\n",
      "16\n",
      "11\n",
      "Epoch: 164, Loss: 0.5093 Val: 1.3290 Test: 0.9828\n",
      "16\n",
      "11\n",
      "Epoch: 165, Loss: 0.5754 Val: 1.3505 Test: 0.9765\n",
      "16\n",
      "11\n",
      "Epoch: 166, Loss: 0.3854 Val: 1.3460 Test: 0.9498\n",
      "16\n",
      "11\n",
      "Epoch: 167, Loss: 0.8696 Val: 1.3197 Test: 0.9399\n",
      "16\n",
      "11\n",
      "Epoch: 168, Loss: 0.8943 Val: 1.2710 Test: 0.9533\n",
      "16\n",
      "11\n",
      "Epoch: 169, Loss: 0.7532 Val: 1.2371 Test: 0.9661\n",
      "16\n",
      "11\n",
      "Epoch: 170, Loss: 0.7392 Val: 1.1980 Test: 0.9834\n",
      "16\n",
      "11\n",
      "Epoch: 171, Loss: 0.7978 Val: 1.1132 Test: 0.9718\n",
      "16\n",
      "11\n",
      "Epoch: 172, Loss: 0.5624 Val: 0.9483 Test: 0.9837\n",
      "16\n",
      "11\n",
      "Epoch: 173, Loss: 0.6789 Val: 0.9388 Test: 0.9895\n",
      "16\n",
      "11\n",
      "Epoch: 174, Loss: 0.5450 Val: 0.9405 Test: 1.0799\n",
      "16\n",
      "11\n",
      "Epoch: 175, Loss: 0.5103 Val: 1.0336 Test: 1.1119\n",
      "16\n",
      "11\n",
      "Epoch: 176, Loss: 0.6349 Val: 1.1148 Test: 1.0965\n",
      "16\n",
      "11\n",
      "Epoch: 177, Loss: 0.6089 Val: 1.1669 Test: 0.9587\n",
      "16\n",
      "11\n",
      "Epoch: 178, Loss: 0.4204 Val: 1.2289 Test: 0.9830\n",
      "16\n",
      "11\n",
      "Epoch: 179, Loss: 0.6879 Val: 1.3040 Test: 1.0058\n",
      "16\n",
      "11\n",
      "Epoch: 180, Loss: 0.5710 Val: 1.1655 Test: 1.0008\n",
      "16\n",
      "11\n",
      "Epoch: 181, Loss: 0.5405 Val: 1.1412 Test: 1.1001\n",
      "16\n",
      "11\n",
      "Epoch: 182, Loss: 0.5376 Val: 1.1233 Test: 1.1199\n",
      "16\n",
      "11\n",
      "Epoch: 183, Loss: 0.7290 Val: 1.1487 Test: 1.1197\n",
      "16\n",
      "11\n",
      "Epoch: 184, Loss: 0.5229 Val: 1.1485 Test: 1.1180\n",
      "16\n",
      "11\n",
      "Epoch: 185, Loss: 0.4950 Val: 1.1611 Test: 1.1049\n",
      "16\n",
      "11\n",
      "Epoch: 186, Loss: 0.6213 Val: 1.1693 Test: 1.0762\n",
      "16\n",
      "11\n",
      "Epoch: 187, Loss: 0.7394 Val: 1.1826 Test: 1.0427\n",
      "16\n",
      "11\n",
      "Epoch: 188, Loss: 0.5634 Val: 1.2254 Test: 0.9891\n",
      "16\n",
      "11\n",
      "Epoch: 189, Loss: 0.5216 Val: 1.2576 Test: 0.9754\n",
      "16\n",
      "11\n",
      "Epoch: 190, Loss: 0.5705 Val: 1.2741 Test: 0.9778\n",
      "16\n",
      "11\n",
      "Epoch: 191, Loss: 0.6071 Val: 1.2888 Test: 0.9937\n",
      "16\n",
      "11\n",
      "Epoch: 192, Loss: 0.5424 Val: 1.2762 Test: 1.0140\n",
      "16\n",
      "11\n",
      "Epoch: 193, Loss: 0.4346 Val: 1.2522 Test: 1.0241\n",
      "16\n",
      "11\n",
      "Epoch: 194, Loss: 0.5550 Val: 1.2394 Test: 1.0176\n",
      "16\n",
      "11\n",
      "Epoch: 195, Loss: 0.5590 Val: 1.2005 Test: 1.0163\n",
      "16\n",
      "11\n",
      "Epoch: 196, Loss: 0.4721 Val: 1.1803 Test: 1.0142\n",
      "16\n",
      "11\n",
      "Epoch: 197, Loss: 0.4138 Val: 1.2199 Test: 1.0229\n",
      "16\n",
      "11\n",
      "Epoch: 198, Loss: 0.4440 Val: 1.2756 Test: 1.0245\n",
      "16\n",
      "11\n",
      "Epoch: 199, Loss: 0.5410 Val: 1.2487 Test: 1.0216\n",
      "mth1\n",
      "\n",
      "torch.Size([804, 115])\n",
      "{'name': 'mth1', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe04ecd90>, 'pre_filter': None, '_indices': [8, 34, 37, 11, 5, 13, 17, 27, 0, 18, 20, 2, 32, 41, 4, 28, 33, 12, 10, 9, 16, 38, 21, 1, 22, 36, 29, 30, 14, 40, 35, 15, 25, 26, 44, 45, 23, 7, 42, 31, 24, 6, 39, 3, 43, 19], 'data': Data(x=[804, 115], edge_index=[2, 1746], edge_attr=[1746, 7], y=[46, 1], smiles=[46]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([  0,  19,  31,  50,  77,  98, 115, 137, 152, 173, 196, 210, 225, 238,\n",
      "        255, 272, 287, 307, 323, 339, 354, 370, 384, 399, 413, 433, 451, 470,\n",
      "        490, 508, 526, 545, 559, 573, 586, 601, 622, 643, 664, 680, 699, 718,\n",
      "        732, 747, 766, 784, 804]), 'edge_index': tensor([   0,   44,   68,  112,  174,  220,  256,  306,  340,  386,  438,  466,\n",
      "         496,  522,  558,  594,  624,  668,  704,  736,  766,  798,  826,  856,\n",
      "         884,  930,  970, 1012, 1058, 1100, 1140, 1182, 1210, 1240, 1266, 1296,\n",
      "        1344, 1390, 1438, 1474, 1516, 1558, 1586, 1616, 1658, 1700, 1746]), 'edge_attr': tensor([   0,   44,   68,  112,  174,  220,  256,  306,  340,  386,  438,  466,\n",
      "         496,  522,  558,  594,  624,  668,  704,  736,  766,  798,  826,  856,\n",
      "         884,  930,  970, 1012, 1058, 1100, 1140, 1182, 1210, 1240, 1266, 1296,\n",
      "        1344, 1390, 1438, 1474, 1516, 1558, 1586, 1616, 1658, 1700, 1746]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46])}), '_data_list': None}\n",
      "minv: 5.0\n",
      "maxv: 10.0\n",
      "emb_dim: 300\n",
      "16\n",
      "12\n",
      "Epoch: 000, Loss: 1.0388 Val: 2.6368 Test: 2.3400\n",
      "16\n",
      "12\n",
      "Epoch: 001, Loss: 0.9850 Val: 2.6221 Test: 2.3455\n",
      "16\n",
      "12\n",
      "Epoch: 002, Loss: 0.9524 Val: 2.5939 Test: 2.3334\n",
      "16\n",
      "12\n",
      "Epoch: 003, Loss: 0.8869 Val: 2.5701 Test: 2.3155\n",
      "16\n",
      "12\n",
      "Epoch: 004, Loss: 0.8170 Val: 2.5443 Test: 2.2785\n",
      "16\n",
      "12\n",
      "Epoch: 005, Loss: 0.9446 Val: 2.5228 Test: 2.2616\n",
      "16\n",
      "12\n",
      "Epoch: 006, Loss: 0.8267 Val: 2.4968 Test: 2.2470\n",
      "16\n",
      "12\n",
      "Epoch: 007, Loss: 0.8569 Val: 2.4761 Test: 2.2375\n",
      "16\n",
      "12\n",
      "Epoch: 008, Loss: 0.7097 Val: 2.4528 Test: 2.2129\n",
      "16\n",
      "12\n",
      "Epoch: 009, Loss: 0.7867 Val: 2.4299 Test: 2.1854\n",
      "16\n",
      "12\n",
      "Epoch: 010, Loss: 0.6717 Val: 2.4063 Test: 2.1620\n",
      "16\n",
      "12\n",
      "Epoch: 011, Loss: 0.7033 Val: 2.3862 Test: 2.1419\n",
      "16\n",
      "12\n",
      "Epoch: 012, Loss: 0.6774 Val: 2.3638 Test: 2.1273\n",
      "16\n",
      "12\n",
      "Epoch: 013, Loss: 0.6592 Val: 2.3397 Test: 2.1135\n",
      "16\n",
      "12\n",
      "Epoch: 014, Loss: 0.6323 Val: 2.3172 Test: 2.1057\n",
      "16\n",
      "12\n",
      "Epoch: 015, Loss: 0.6965 Val: 2.3291 Test: 2.0838\n",
      "16\n",
      "12\n",
      "Epoch: 016, Loss: 0.9223 Val: 2.3094 Test: 2.0554\n",
      "16\n",
      "12\n",
      "Epoch: 017, Loss: 0.9782 Val: 2.2889 Test: 2.0594\n",
      "16\n",
      "12\n",
      "Epoch: 018, Loss: 1.0464 Val: 2.2709 Test: 2.0233\n",
      "16\n",
      "12\n",
      "Epoch: 019, Loss: 1.0046 Val: 2.2528 Test: 2.0143\n",
      "16\n",
      "12\n",
      "Epoch: 020, Loss: 0.9597 Val: 2.2309 Test: 2.0036\n",
      "16\n",
      "12\n",
      "Epoch: 021, Loss: 0.5304 Val: 2.1945 Test: 1.9873\n",
      "16\n",
      "12\n",
      "Epoch: 022, Loss: 0.7497 Val: 2.1654 Test: 1.9709\n",
      "16\n",
      "12\n",
      "Epoch: 023, Loss: 0.6211 Val: 2.1464 Test: 1.9477\n",
      "16\n",
      "12\n",
      "Epoch: 024, Loss: 0.5779 Val: 2.1294 Test: 1.9243\n",
      "16\n",
      "12\n",
      "Epoch: 025, Loss: 0.7181 Val: 2.1218 Test: 1.8890\n",
      "16\n",
      "12\n",
      "Epoch: 026, Loss: 0.7857 Val: 2.1007 Test: 1.8725\n",
      "16\n",
      "12\n",
      "Epoch: 027, Loss: 0.8644 Val: 2.0863 Test: 1.8438\n",
      "16\n",
      "12\n",
      "Epoch: 028, Loss: 0.5533 Val: 2.0693 Test: 1.8049\n",
      "16\n",
      "12\n",
      "Epoch: 029, Loss: 0.6338 Val: 2.0417 Test: 1.7613\n",
      "16\n",
      "12\n",
      "Epoch: 030, Loss: 0.6286 Val: 2.0334 Test: 1.7290\n",
      "16\n",
      "12\n",
      "Epoch: 031, Loss: 0.5731 Val: 2.0163 Test: 1.7544\n",
      "16\n",
      "12\n",
      "Epoch: 032, Loss: 0.8968 Val: 1.9889 Test: 1.6895\n",
      "16\n",
      "12\n",
      "Epoch: 033, Loss: 0.6504 Val: 1.9289 Test: 1.6879\n",
      "16\n",
      "12\n",
      "Epoch: 034, Loss: 0.5733 Val: 1.8758 Test: 1.7023\n",
      "16\n",
      "12\n",
      "Epoch: 035, Loss: 0.6518 Val: 1.8540 Test: 1.7009\n",
      "16\n",
      "12\n",
      "Epoch: 036, Loss: 0.5773 Val: 1.8875 Test: 1.6942\n",
      "16\n",
      "12\n",
      "Epoch: 037, Loss: 0.4833 Val: 1.8697 Test: 1.6711\n",
      "16\n",
      "12\n",
      "Epoch: 038, Loss: 0.6607 Val: 1.8595 Test: 1.6553\n",
      "16\n",
      "12\n",
      "Epoch: 039, Loss: 0.5638 Val: 1.8610 Test: 1.6364\n",
      "16\n",
      "12\n",
      "Epoch: 040, Loss: 0.6618 Val: 1.8402 Test: 1.6223\n",
      "16\n",
      "12\n",
      "Epoch: 041, Loss: 0.6751 Val: 1.8163 Test: 1.6186\n",
      "16\n",
      "12\n",
      "Epoch: 042, Loss: 0.7468 Val: 1.8021 Test: 1.6324\n",
      "16\n",
      "12\n",
      "Epoch: 043, Loss: 0.7584 Val: 1.7857 Test: 1.5945\n",
      "16\n",
      "12\n",
      "Epoch: 044, Loss: 0.7225 Val: 1.6903 Test: 1.5456\n",
      "16\n",
      "12\n",
      "Epoch: 045, Loss: 0.6745 Val: 1.7786 Test: 1.4747\n",
      "16\n",
      "12\n",
      "Epoch: 046, Loss: 0.6090 Val: 1.7483 Test: 1.4310\n",
      "16\n",
      "12\n",
      "Epoch: 047, Loss: 0.6204 Val: 1.6703 Test: 1.3531\n",
      "16\n",
      "12\n",
      "Epoch: 048, Loss: 0.6914 Val: 1.6335 Test: 1.3481\n",
      "16\n",
      "12\n",
      "Epoch: 049, Loss: 0.6655 Val: 1.6134 Test: 1.3879\n",
      "16\n",
      "12\n",
      "Epoch: 050, Loss: 0.7681 Val: 1.5746 Test: 1.3612\n",
      "16\n",
      "12\n",
      "Epoch: 051, Loss: 0.6101 Val: 1.5351 Test: 1.3135\n",
      "16\n",
      "12\n",
      "Epoch: 052, Loss: 0.5937 Val: 1.5155 Test: 1.2512\n",
      "16\n",
      "12\n",
      "Epoch: 053, Loss: 0.5680 Val: 1.5051 Test: 1.1838\n",
      "16\n",
      "12\n",
      "Epoch: 054, Loss: 0.7164 Val: 1.3425 Test: 1.1458\n",
      "16\n",
      "12\n",
      "Epoch: 055, Loss: 0.7552 Val: 1.3178 Test: 1.1339\n",
      "16\n",
      "12\n",
      "Epoch: 056, Loss: 0.6204 Val: 1.3776 Test: 1.1033\n",
      "16\n",
      "12\n",
      "Epoch: 057, Loss: 0.5533 Val: 1.5112 Test: 1.0479\n",
      "16\n",
      "12\n",
      "Epoch: 058, Loss: 0.5362 Val: 1.4714 Test: 1.0210\n",
      "16\n",
      "12\n",
      "Epoch: 059, Loss: 0.5627 Val: 1.4425 Test: 1.0076\n",
      "16\n",
      "12\n",
      "Epoch: 060, Loss: 0.5041 Val: 1.4293 Test: 1.0827\n",
      "16\n",
      "12\n",
      "Epoch: 061, Loss: 0.6687 Val: 1.3974 Test: 0.9771\n",
      "16\n",
      "12\n",
      "Epoch: 062, Loss: 0.6108 Val: 1.2687 Test: 0.9351\n",
      "16\n",
      "12\n",
      "Epoch: 063, Loss: 0.6223 Val: 1.1222 Test: 0.9136\n",
      "16\n",
      "12\n",
      "Epoch: 064, Loss: 0.7023 Val: 1.0973 Test: 0.9045\n",
      "16\n",
      "12\n",
      "Epoch: 065, Loss: 0.5185 Val: 1.0917 Test: 0.9300\n",
      "16\n",
      "12\n",
      "Epoch: 066, Loss: 0.5626 Val: 1.0629 Test: 1.0101\n",
      "16\n",
      "12\n",
      "Epoch: 067, Loss: 0.5180 Val: 1.0433 Test: 1.1176\n",
      "16\n",
      "12\n",
      "Epoch: 068, Loss: 0.5071 Val: 1.0312 Test: 1.1730\n",
      "16\n",
      "12\n",
      "Epoch: 069, Loss: 0.7320 Val: 1.0168 Test: 1.1842\n",
      "16\n",
      "12\n",
      "Epoch: 070, Loss: 0.3886 Val: 1.0069 Test: 1.1859\n",
      "16\n",
      "12\n",
      "Epoch: 071, Loss: 0.6410 Val: 1.0014 Test: 1.1885\n",
      "16\n",
      "12\n",
      "Epoch: 072, Loss: 0.4784 Val: 1.2027 Test: 1.1997\n",
      "16\n",
      "12\n",
      "Epoch: 073, Loss: 0.4420 Val: 1.2428 Test: 1.2137\n",
      "16\n",
      "12\n",
      "Epoch: 074, Loss: 0.4583 Val: 1.2463 Test: 1.1973\n",
      "16\n",
      "12\n",
      "Epoch: 075, Loss: 0.5088 Val: 1.2024 Test: 1.1478\n",
      "16\n",
      "12\n",
      "Epoch: 076, Loss: 0.5251 Val: 1.1793 Test: 1.1288\n",
      "16\n",
      "12\n",
      "Epoch: 077, Loss: 0.5344 Val: 1.1837 Test: 1.2546\n",
      "16\n",
      "12\n",
      "Epoch: 078, Loss: 0.5903 Val: 0.9662 Test: 1.2740\n",
      "16\n",
      "12\n",
      "Epoch: 079, Loss: 0.4452 Val: 0.9751 Test: 1.3014\n",
      "16\n",
      "12\n",
      "Epoch: 080, Loss: 0.8133 Val: 0.9879 Test: 1.3273\n",
      "16\n",
      "12\n",
      "Epoch: 081, Loss: 0.7294 Val: 1.0130 Test: 1.3313\n",
      "16\n",
      "12\n",
      "Epoch: 082, Loss: 0.5374 Val: 1.0481 Test: 1.3069\n",
      "16\n",
      "12\n",
      "Epoch: 083, Loss: 0.4868 Val: 1.0637 Test: 1.2711\n",
      "16\n",
      "12\n",
      "Epoch: 084, Loss: 0.6675 Val: 1.0592 Test: 1.2026\n",
      "16\n",
      "12\n",
      "Epoch: 085, Loss: 0.6973 Val: 1.0561 Test: 1.1233\n",
      "16\n",
      "12\n",
      "Epoch: 086, Loss: 0.4926 Val: 1.0640 Test: 1.0894\n",
      "16\n",
      "12\n",
      "Epoch: 087, Loss: 0.5450 Val: 1.0656 Test: 1.1308\n",
      "16\n",
      "12\n",
      "Epoch: 088, Loss: 0.4823 Val: 1.0593 Test: 1.2554\n",
      "16\n",
      "12\n",
      "Epoch: 089, Loss: 0.4820 Val: 1.0510 Test: 1.3667\n",
      "16\n",
      "12\n",
      "Epoch: 090, Loss: 0.4062 Val: 1.0627 Test: 1.4444\n",
      "16\n",
      "12\n",
      "Epoch: 091, Loss: 0.3574 Val: 1.1605 Test: 1.4952\n",
      "16\n",
      "12\n",
      "Epoch: 092, Loss: 0.4479 Val: 1.3189 Test: 1.5249\n",
      "16\n",
      "12\n",
      "Epoch: 093, Loss: 0.5178 Val: 1.3886 Test: 1.5335\n",
      "16\n",
      "12\n",
      "Epoch: 094, Loss: 0.4077 Val: 1.4143 Test: 1.5487\n",
      "16\n",
      "12\n",
      "Epoch: 095, Loss: 0.4571 Val: 1.4100 Test: 1.5520\n",
      "16\n",
      "12\n",
      "Epoch: 096, Loss: 0.3150 Val: 1.3938 Test: 1.5422\n",
      "16\n",
      "12\n",
      "Epoch: 097, Loss: 0.2880 Val: 1.3771 Test: 1.5227\n",
      "16\n",
      "12\n",
      "Epoch: 098, Loss: 0.4106 Val: 1.3667 Test: 1.5011\n",
      "16\n",
      "12\n",
      "Epoch: 099, Loss: 0.2084 Val: 1.3554 Test: 1.4709\n",
      "16\n",
      "12\n",
      "Epoch: 100, Loss: 0.2327 Val: 1.3453 Test: 1.4612\n",
      "16\n",
      "12\n",
      "Epoch: 101, Loss: 0.2577 Val: 1.3371 Test: 1.4667\n",
      "16\n",
      "12\n",
      "Epoch: 102, Loss: 0.4060 Val: 1.3379 Test: 1.4726\n",
      "16\n",
      "12\n",
      "Epoch: 103, Loss: 0.4541 Val: 1.3423 Test: 1.4719\n",
      "16\n",
      "12\n",
      "Epoch: 104, Loss: 0.3860 Val: 1.3425 Test: 1.4768\n",
      "16\n",
      "12\n",
      "Epoch: 105, Loss: 0.5042 Val: 1.3531 Test: 1.5090\n",
      "16\n",
      "12\n",
      "Epoch: 106, Loss: 0.2784 Val: 1.3545 Test: 1.4955\n",
      "16\n",
      "12\n",
      "Epoch: 107, Loss: 0.7812 Val: 1.3275 Test: 1.5035\n",
      "16\n",
      "12\n",
      "Epoch: 108, Loss: 0.5504 Val: 1.2781 Test: 1.4820\n",
      "16\n",
      "12\n",
      "Epoch: 109, Loss: 0.5945 Val: 1.2533 Test: 1.4473\n",
      "16\n",
      "12\n",
      "Epoch: 110, Loss: 0.5347 Val: 1.5215 Test: 1.3876\n",
      "16\n",
      "12\n",
      "Epoch: 111, Loss: 0.4031 Val: 1.6139 Test: 1.2747\n",
      "16\n",
      "12\n",
      "Epoch: 112, Loss: 0.7619 Val: 1.3533 Test: 1.1718\n",
      "16\n",
      "12\n",
      "Epoch: 113, Loss: 0.3493 Val: 1.3357 Test: 1.2178\n",
      "16\n",
      "12\n",
      "Epoch: 114, Loss: 0.6232 Val: 1.3735 Test: 1.2692\n",
      "16\n",
      "12\n",
      "Epoch: 115, Loss: 0.6806 Val: 1.3963 Test: 1.2787\n",
      "16\n",
      "12\n",
      "Epoch: 116, Loss: 0.2840 Val: 1.3296 Test: 1.2660\n",
      "16\n",
      "12\n",
      "Epoch: 117, Loss: 0.4629 Val: 1.2353 Test: 1.3039\n",
      "16\n",
      "12\n",
      "Epoch: 118, Loss: 0.5835 Val: 1.1937 Test: 1.2024\n",
      "16\n",
      "12\n",
      "Epoch: 119, Loss: 0.4349 Val: 1.1626 Test: 0.9497\n",
      "16\n",
      "12\n",
      "Epoch: 120, Loss: 0.4178 Val: 1.1137 Test: 0.8974\n",
      "16\n",
      "12\n",
      "Epoch: 121, Loss: 0.3634 Val: 1.0904 Test: 0.8637\n",
      "16\n",
      "12\n",
      "Epoch: 122, Loss: 0.3393 Val: 1.0996 Test: 0.8683\n",
      "16\n",
      "12\n",
      "Epoch: 123, Loss: 0.4232 Val: 1.1405 Test: 0.9072\n",
      "16\n",
      "12\n",
      "Epoch: 124, Loss: 0.4330 Val: 1.1978 Test: 0.9593\n",
      "16\n",
      "12\n",
      "Epoch: 125, Loss: 0.5937 Val: 1.3249 Test: 1.0146\n",
      "16\n",
      "12\n",
      "Epoch: 126, Loss: 0.5947 Val: 1.3555 Test: 1.0244\n",
      "16\n",
      "12\n",
      "Epoch: 127, Loss: 0.5295 Val: 1.3478 Test: 0.9913\n",
      "16\n",
      "12\n",
      "Epoch: 128, Loss: 0.2500 Val: 1.3405 Test: 0.9739\n",
      "16\n",
      "12\n",
      "Epoch: 129, Loss: 0.5887 Val: 1.3260 Test: 0.9689\n",
      "16\n",
      "12\n",
      "Epoch: 130, Loss: 0.2521 Val: 1.3208 Test: 0.9739\n",
      "16\n",
      "12\n",
      "Epoch: 131, Loss: 0.3868 Val: 1.3154 Test: 0.9745\n",
      "16\n",
      "12\n",
      "Epoch: 132, Loss: 0.2839 Val: 1.3251 Test: 0.9809\n",
      "16\n",
      "12\n",
      "Epoch: 133, Loss: 0.3538 Val: 1.3395 Test: 1.0068\n",
      "16\n",
      "12\n",
      "Epoch: 134, Loss: 0.4379 Val: 1.3482 Test: 1.0273\n",
      "16\n",
      "12\n",
      "Epoch: 135, Loss: 0.6056 Val: 1.3187 Test: 0.9565\n",
      "16\n",
      "12\n",
      "Epoch: 136, Loss: 0.4136 Val: 1.2544 Test: 1.1084\n",
      "16\n",
      "12\n",
      "Epoch: 137, Loss: 0.4143 Val: 1.2615 Test: 1.2104\n",
      "16\n",
      "12\n",
      "Epoch: 138, Loss: 0.3042 Val: 1.2659 Test: 1.2163\n",
      "16\n",
      "12\n",
      "Epoch: 139, Loss: 0.4567 Val: 1.2824 Test: 1.2194\n",
      "16\n",
      "12\n",
      "Epoch: 140, Loss: 0.5202 Val: 1.2934 Test: 1.1300\n",
      "16\n",
      "12\n",
      "Epoch: 141, Loss: 0.3767 Val: 1.3524 Test: 1.0543\n",
      "16\n",
      "12\n",
      "Epoch: 142, Loss: 0.5452 Val: 1.4259 Test: 1.0782\n",
      "16\n",
      "12\n",
      "Epoch: 143, Loss: 0.5670 Val: 1.4389 Test: 1.0411\n",
      "16\n",
      "12\n",
      "Epoch: 144, Loss: 0.4326 Val: 1.4866 Test: 0.9827\n",
      "16\n",
      "12\n",
      "Epoch: 145, Loss: 0.4167 Val: 1.4396 Test: 0.9463\n",
      "16\n",
      "12\n",
      "Epoch: 146, Loss: 0.6682 Val: 1.3929 Test: 0.8793\n",
      "16\n",
      "12\n",
      "Epoch: 147, Loss: 0.6676 Val: 1.3843 Test: 0.8020\n",
      "16\n",
      "12\n",
      "Epoch: 148, Loss: 0.6888 Val: 1.4027 Test: 0.7781\n",
      "16\n",
      "12\n",
      "Epoch: 149, Loss: 0.6076 Val: 1.4326 Test: 0.7590\n",
      "16\n",
      "12\n",
      "Epoch: 150, Loss: 0.6247 Val: 1.4596 Test: 0.6897\n",
      "16\n",
      "12\n",
      "Epoch: 151, Loss: 0.5248 Val: 1.4733 Test: 0.6975\n",
      "16\n",
      "12\n",
      "Epoch: 152, Loss: 0.4051 Val: 1.4846 Test: 0.7369\n",
      "16\n",
      "12\n",
      "Epoch: 153, Loss: 0.4983 Val: 1.5093 Test: 0.8823\n",
      "16\n",
      "12\n",
      "Epoch: 154, Loss: 0.7199 Val: 1.4302 Test: 0.6355\n",
      "16\n",
      "12\n",
      "Epoch: 155, Loss: 0.3792 Val: 1.3099 Test: 0.6516\n",
      "16\n",
      "12\n",
      "Epoch: 156, Loss: 0.4774 Val: 1.3010 Test: 0.6491\n",
      "16\n",
      "12\n",
      "Epoch: 157, Loss: 0.3971 Val: 1.2705 Test: 0.6650\n",
      "16\n",
      "12\n",
      "Epoch: 158, Loss: 0.3505 Val: 1.2443 Test: 0.7016\n",
      "16\n",
      "12\n",
      "Epoch: 159, Loss: 0.9259 Val: 1.2271 Test: 0.7666\n",
      "16\n",
      "12\n",
      "Epoch: 160, Loss: 0.4105 Val: 1.2262 Test: 0.8438\n",
      "16\n",
      "12\n",
      "Epoch: 161, Loss: 0.4398 Val: 1.2347 Test: 0.8998\n",
      "16\n",
      "12\n",
      "Epoch: 162, Loss: 0.4095 Val: 1.2496 Test: 0.9268\n",
      "16\n",
      "12\n",
      "Epoch: 163, Loss: 0.5044 Val: 1.2637 Test: 0.9416\n",
      "16\n",
      "12\n",
      "Epoch: 164, Loss: 0.3652 Val: 1.2315 Test: 0.9321\n",
      "16\n",
      "12\n",
      "Epoch: 165, Loss: 0.1831 Val: 1.1681 Test: 0.9207\n",
      "16\n",
      "12\n",
      "Epoch: 166, Loss: 0.7217 Val: 1.1421 Test: 0.9434\n",
      "16\n",
      "12\n",
      "Epoch: 167, Loss: 0.4667 Val: 1.1375 Test: 0.9597\n",
      "16\n",
      "12\n",
      "Epoch: 168, Loss: 0.2967 Val: 1.1524 Test: 0.9855\n",
      "16\n",
      "12\n",
      "Epoch: 169, Loss: 0.4714 Val: 1.2161 Test: 1.0428\n",
      "16\n",
      "12\n",
      "Epoch: 170, Loss: 0.4918 Val: 1.2892 Test: 1.2586\n",
      "16\n",
      "12\n",
      "Epoch: 171, Loss: 0.3925 Val: 1.3795 Test: 1.2412\n",
      "16\n",
      "12\n",
      "Epoch: 172, Loss: 0.4341 Val: 1.4370 Test: 1.2292\n",
      "16\n",
      "12\n",
      "Epoch: 173, Loss: 0.4315 Val: 1.4675 Test: 1.2178\n",
      "16\n",
      "12\n",
      "Epoch: 174, Loss: 0.3338 Val: 1.4702 Test: 1.2374\n",
      "16\n",
      "12\n",
      "Epoch: 175, Loss: 0.2773 Val: 1.4644 Test: 1.2478\n",
      "16\n",
      "12\n",
      "Epoch: 176, Loss: 0.2885 Val: 1.4447 Test: 1.2763\n",
      "16\n",
      "12\n",
      "Epoch: 177, Loss: 0.4065 Val: 1.4150 Test: 1.2528\n",
      "16\n",
      "12\n",
      "Epoch: 178, Loss: 0.3840 Val: 1.3999 Test: 1.2209\n",
      "16\n",
      "12\n",
      "Epoch: 179, Loss: 0.8195 Val: 1.4184 Test: 1.2116\n",
      "16\n",
      "12\n",
      "Epoch: 180, Loss: 0.4263 Val: 1.4009 Test: 1.1931\n",
      "16\n",
      "12\n",
      "Epoch: 181, Loss: 0.7373 Val: 1.2800 Test: 0.9828\n",
      "16\n",
      "12\n",
      "Epoch: 182, Loss: 0.3385 Val: 1.2094 Test: 1.1857\n",
      "16\n",
      "12\n",
      "Epoch: 183, Loss: 0.3207 Val: 1.2197 Test: 1.3769\n",
      "16\n",
      "12\n",
      "Epoch: 184, Loss: 0.4965 Val: 1.2784 Test: 1.4249\n",
      "16\n",
      "12\n",
      "Epoch: 185, Loss: 0.4312 Val: 1.3180 Test: 1.3927\n",
      "16\n",
      "12\n",
      "Epoch: 186, Loss: 0.6213 Val: 1.2653 Test: 1.1852\n",
      "16\n",
      "12\n",
      "Epoch: 187, Loss: 0.4873 Val: 1.2772 Test: 1.0394\n",
      "16\n",
      "12\n",
      "Epoch: 188, Loss: 0.4018 Val: 1.2504 Test: 0.9696\n",
      "16\n",
      "12\n",
      "Epoch: 189, Loss: 0.4220 Val: 1.1929 Test: 0.8849\n",
      "16\n",
      "12\n",
      "Epoch: 190, Loss: 0.5681 Val: 1.2389 Test: 0.8132\n",
      "16\n",
      "12\n",
      "Epoch: 191, Loss: 0.5317 Val: 1.3269 Test: 0.8014\n",
      "16\n",
      "12\n",
      "Epoch: 192, Loss: 0.3437 Val: 1.4125 Test: 0.8337\n",
      "16\n",
      "12\n",
      "Epoch: 193, Loss: 0.4681 Val: 1.4313 Test: 0.8470\n",
      "16\n",
      "12\n",
      "Epoch: 194, Loss: 0.4482 Val: 1.4041 Test: 0.8374\n",
      "16\n",
      "12\n",
      "Epoch: 195, Loss: 0.4730 Val: 1.3678 Test: 0.8477\n",
      "16\n",
      "12\n",
      "Epoch: 196, Loss: 0.3843 Val: 1.3165 Test: 0.8627\n",
      "16\n",
      "12\n",
      "Epoch: 197, Loss: 0.5307 Val: 1.3385 Test: 0.8645\n",
      "16\n",
      "12\n",
      "Epoch: 198, Loss: 0.4392 Val: 1.2999 Test: 0.8241\n",
      "16\n",
      "12\n",
      "Epoch: 199, Loss: 0.4197 Val: 1.3095 Test: 0.8260\n",
      "rip2\n",
      "\n",
      "torch.Size([1428, 115])\n",
      "{'name': 'rip2', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe0531f40>, 'pre_filter': None, '_indices': [18, 44, 38, 48, 23, 51, 46, 33, 6, 10, 39, 9, 31, 26, 37, 13, 17, 43, 5, 30, 32, 22, 11, 40, 41, 28, 27, 1, 34, 50, 36, 42, 25, 0, 47, 4, 52, 16, 7, 20, 19, 14, 21, 3, 24, 15, 8, 45, 49, 35, 12, 2, 29], 'data': Data(x=[1428, 115], edge_index=[2, 3086], edge_attr=[3086, 7], y=[53, 1], smiles=[53]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   26,   52,   79,  107,  133,  159,  188,  215,  241,  269,  295,\n",
      "         322,  345,  372,  400,  425,  451,  477,  505,  530,  556,  580,  609,\n",
      "         636,  664,  694,  721,  751,  781,  806,  832,  857,  884,  903,  932,\n",
      "         959,  988, 1016, 1044, 1072, 1099, 1124, 1151, 1176, 1201, 1229, 1263,\n",
      "        1290, 1317, 1345, 1373, 1400, 1428]), 'edge_index': tensor([   0,   56,  112,  172,  234,  290,  346,  408,  466,  522,  582,  638,\n",
      "         698,  748,  808,  868,  922,  978, 1034, 1094, 1148, 1204, 1256, 1318,\n",
      "        1376, 1436, 1500, 1560, 1624, 1688, 1742, 1798, 1852, 1912, 1954, 2016,\n",
      "        2074, 2136, 2196, 2256, 2316, 2374, 2428, 2486, 2540, 2594, 2654, 2728,\n",
      "        2786, 2844, 2906, 2968, 3026, 3086]), 'edge_attr': tensor([   0,   56,  112,  172,  234,  290,  346,  408,  466,  522,  582,  638,\n",
      "         698,  748,  808,  868,  922,  978, 1034, 1094, 1148, 1204, 1256, 1318,\n",
      "        1376, 1436, 1500, 1560, 1624, 1688, 1742, 1798, 1852, 1912, 1954, 2016,\n",
      "        2074, 2136, 2196, 2256, 2316, 2374, 2428, 2486, 2540, 2594, 2654, 2728,\n",
      "        2786, 2844, 2906, 2968, 3026, 3086]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53])}), '_data_list': None}\n",
      "minv: 5.329999923706055\n",
      "maxv: 8.699999809265137\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 000, Loss: 1.0126 Val: 2.2801 Test: 2.2538\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 001, Loss: 0.9879 Val: 2.2680 Test: 2.2338\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 002, Loss: 0.9359 Val: 2.2254 Test: 2.1859\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 003, Loss: 0.9087 Val: 2.1997 Test: 2.1692\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 004, Loss: 0.9825 Val: 2.1818 Test: 2.1454\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 005, Loss: 0.9333 Val: 2.1551 Test: 2.1222\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 006, Loss: 0.9281 Val: 2.1244 Test: 2.0860\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 007, Loss: 0.9417 Val: 2.0853 Test: 2.0374\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 008, Loss: 0.9167 Val: 2.0407 Test: 1.9952\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 009, Loss: 0.8774 Val: 1.9936 Test: 1.9602\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 010, Loss: 0.8968 Val: 1.9442 Test: 1.8974\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 011, Loss: 0.8663 Val: 1.9145 Test: 1.8611\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 012, Loss: 0.8831 Val: 1.9001 Test: 1.8143\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 013, Loss: 0.8245 Val: 1.8682 Test: 1.8075\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 014, Loss: 0.8398 Val: 1.8303 Test: 1.7634\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 015, Loss: 0.8898 Val: 1.7782 Test: 1.7007\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 016, Loss: 0.8378 Val: 1.7162 Test: 1.6918\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 017, Loss: 0.8245 Val: 1.6596 Test: 1.6409\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 018, Loss: 0.7490 Val: 1.6375 Test: 1.6125\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 019, Loss: 0.6776 Val: 1.6065 Test: 1.5748\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 020, Loss: 0.7888 Val: 1.5423 Test: 1.4918\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 021, Loss: 0.8369 Val: 1.5540 Test: 1.4825\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 022, Loss: 0.7826 Val: 1.4472 Test: 1.3066\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 023, Loss: 0.7896 Val: 1.4069 Test: 1.3596\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 024, Loss: 0.7485 Val: 1.4149 Test: 1.3377\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 025, Loss: 0.7413 Val: 1.4047 Test: 1.3296\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 026, Loss: 0.7985 Val: 1.3855 Test: 1.3289\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 027, Loss: 0.7324 Val: 1.3114 Test: 1.2604\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 028, Loss: 0.7680 Val: 1.2737 Test: 1.2115\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 029, Loss: 0.7506 Val: 1.2330 Test: 1.1863\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 030, Loss: 0.7248 Val: 1.2026 Test: 1.1490\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 031, Loss: 0.7250 Val: 1.1977 Test: 1.0907\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 032, Loss: 0.7560 Val: 1.1215 Test: 1.0651\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 033, Loss: 0.6938 Val: 1.1042 Test: 1.0331\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 034, Loss: 0.6656 Val: 1.0746 Test: 1.0255\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 035, Loss: 0.6854 Val: 0.9439 Test: 0.9253\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 036, Loss: 0.6235 Val: 0.9229 Test: 0.8817\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 037, Loss: 0.5879 Val: 1.0070 Test: 0.9941\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 038, Loss: 0.6690 Val: 0.9207 Test: 0.8677\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 039, Loss: 0.6321 Val: 0.8557 Test: 0.8754\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 040, Loss: 0.6330 Val: 0.9697 Test: 1.0088\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 041, Loss: 0.6675 Val: 0.9170 Test: 0.9468\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 042, Loss: 0.8273 Val: 0.8448 Test: 0.7912\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 043, Loss: 0.7462 Val: 0.8256 Test: 0.9042\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 044, Loss: 0.7235 Val: 0.8163 Test: 0.8757\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 045, Loss: 0.7339 Val: 0.8579 Test: 0.9666\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 046, Loss: 0.7393 Val: 0.8692 Test: 0.9475\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 047, Loss: 0.9207 Val: 0.8528 Test: 0.8339\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 048, Loss: 0.9597 Val: 0.8424 Test: 0.8249\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 049, Loss: 0.6247 Val: 0.8041 Test: 0.8312\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 050, Loss: 0.6224 Val: 0.7869 Test: 0.8280\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 051, Loss: 0.6181 Val: 0.7268 Test: 0.7927\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 052, Loss: 0.5965 Val: 0.7416 Test: 0.7492\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 053, Loss: 0.6926 Val: 0.8134 Test: 0.7903\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 054, Loss: 0.6430 Val: 0.7695 Test: 0.7357\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 055, Loss: 0.5889 Val: 0.7421 Test: 0.6888\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 056, Loss: 0.5476 Val: 0.6916 Test: 0.7606\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 057, Loss: 0.6122 Val: 0.6671 Test: 0.7129\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 058, Loss: 0.6302 Val: 0.6681 Test: 0.6914\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 059, Loss: 0.5370 Val: 0.6705 Test: 0.6789\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 060, Loss: 0.5098 Val: 0.7270 Test: 0.5724\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 061, Loss: 0.5697 Val: 0.7546 Test: 0.5259\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 062, Loss: 0.5931 Val: 0.6813 Test: 0.5175\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 063, Loss: 0.5013 Val: 0.6506 Test: 0.5851\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 064, Loss: 0.5927 Val: 0.6534 Test: 0.6134\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 065, Loss: 0.6663 Val: 0.6246 Test: 0.6072\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 066, Loss: 0.4790 Val: 0.6392 Test: 0.5782\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 067, Loss: 0.5744 Val: 0.6463 Test: 0.5272\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 068, Loss: 0.5915 Val: 0.6257 Test: 0.5131\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 069, Loss: 0.5041 Val: 0.6061 Test: 0.5637\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 070, Loss: 0.4771 Val: 0.5956 Test: 0.5207\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 071, Loss: 0.5034 Val: 0.6126 Test: 0.5122\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 072, Loss: 0.4911 Val: 0.6529 Test: 0.5566\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 073, Loss: 0.5815 Val: 0.6756 Test: 0.6090\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 074, Loss: 0.6122 Val: 0.6766 Test: 0.6576\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 075, Loss: 0.5772 Val: 0.6916 Test: 0.6497\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 076, Loss: 0.4529 Val: 0.6924 Test: 0.5696\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 077, Loss: 0.6367 Val: 0.6888 Test: 0.4775\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 078, Loss: 0.4874 Val: 0.6695 Test: 0.4381\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 079, Loss: 0.4337 Val: 0.6596 Test: 0.4216\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 080, Loss: 0.4747 Val: 0.6609 Test: 0.4284\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 081, Loss: 0.5237 Val: 0.6784 Test: 0.4166\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 082, Loss: 0.5665 Val: 0.6745 Test: 0.4154\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 083, Loss: 0.4652 Val: 0.6595 Test: 0.4312\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 084, Loss: 0.5115 Val: 0.6844 Test: 0.4201\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 085, Loss: 0.5401 Val: 0.6991 Test: 0.4132\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 086, Loss: 0.5470 Val: 0.7102 Test: 0.4244\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 087, Loss: 0.5460 Val: 0.7169 Test: 0.4371\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 088, Loss: 0.4713 Val: 0.7143 Test: 0.4815\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 089, Loss: 0.3956 Val: 0.7343 Test: 0.5553\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 090, Loss: 0.4328 Val: 0.7588 Test: 0.5937\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 091, Loss: 0.5319 Val: 0.7959 Test: 0.5817\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 092, Loss: 0.4011 Val: 0.7855 Test: 0.6057\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 093, Loss: 0.4249 Val: 0.8031 Test: 0.6358\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 094, Loss: 0.5784 Val: 0.7708 Test: 0.6877\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 095, Loss: 0.6700 Val: 0.7247 Test: 0.4717\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 096, Loss: 0.6451 Val: 0.7119 Test: 0.3876\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 097, Loss: 0.7168 Val: 0.6302 Test: 0.4448\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 098, Loss: 0.5321 Val: 0.5858 Test: 0.4704\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 099, Loss: 0.5993 Val: 0.5495 Test: 0.4616\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 100, Loss: 0.6563 Val: 0.6298 Test: 0.4025\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 101, Loss: 0.7447 Val: 0.6313 Test: 0.3571\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 102, Loss: 0.5299 Val: 0.6287 Test: 0.3313\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 103, Loss: 0.6214 Val: 0.6547 Test: 0.3412\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 104, Loss: 0.5611 Val: 0.6530 Test: 0.3539\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 105, Loss: 0.4717 Val: 0.6664 Test: 0.4292\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 106, Loss: 0.5968 Val: 0.6508 Test: 0.4371\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 107, Loss: 0.6420 Val: 0.6147 Test: 0.4753\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 108, Loss: 0.6267 Val: 0.6112 Test: 0.4824\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 109, Loss: 0.7815 Val: 0.5609 Test: 0.4620\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 110, Loss: 0.8562 Val: 0.5935 Test: 0.5512\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 111, Loss: 0.8584 Val: 0.6628 Test: 0.6289\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 112, Loss: 0.7111 Val: 0.6603 Test: 0.6085\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 113, Loss: 0.6718 Val: 0.7208 Test: 0.5927\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 114, Loss: 0.9370 Val: 0.6851 Test: 0.5531\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 115, Loss: 0.7574 Val: 0.5777 Test: 0.4760\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 116, Loss: 0.7160 Val: 0.5399 Test: 0.5124\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 117, Loss: 0.6536 Val: 0.5413 Test: 0.6353\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 118, Loss: 0.6283 Val: 0.5522 Test: 0.7053\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 119, Loss: 0.6908 Val: 0.5602 Test: 0.4821\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 120, Loss: 0.6026 Val: 0.5745 Test: 0.6211\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 121, Loss: 0.5965 Val: 0.6325 Test: 0.6911\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 122, Loss: 0.6766 Val: 0.6625 Test: 0.5746\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 123, Loss: 0.6548 Val: 0.6468 Test: 0.5306\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 124, Loss: 0.4803 Val: 0.6390 Test: 0.5504\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 125, Loss: 0.6356 Val: 0.6201 Test: 0.5384\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 126, Loss: 0.4744 Val: 0.5904 Test: 0.5697\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 127, Loss: 0.5665 Val: 0.5526 Test: 0.6173\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 128, Loss: 0.5806 Val: 0.5484 Test: 0.5391\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 129, Loss: 0.6143 Val: 0.5416 Test: 0.4996\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 130, Loss: 0.6305 Val: 0.5448 Test: 0.4963\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 131, Loss: 0.5561 Val: 0.5691 Test: 0.4955\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 132, Loss: 0.5353 Val: 0.5987 Test: 0.5120\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 133, Loss: 0.5556 Val: 0.6362 Test: 0.5088\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 134, Loss: 0.5127 Val: 0.6248 Test: 0.4746\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 135, Loss: 0.5155 Val: 0.5855 Test: 0.4596\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 136, Loss: 0.5624 Val: 0.5794 Test: 0.4957\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 137, Loss: 0.5865 Val: 0.6183 Test: 0.5226\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 138, Loss: 0.5045 Val: 0.6485 Test: 0.5568\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 139, Loss: 0.4178 Val: 0.6743 Test: 0.7542\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 140, Loss: 0.5139 Val: 0.7117 Test: 0.7699\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 141, Loss: 0.5100 Val: 0.7372 Test: 0.7528\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 142, Loss: 0.5450 Val: 0.6875 Test: 0.9100\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 143, Loss: 0.5995 Val: 0.6537 Test: 0.5751\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 144, Loss: 0.6473 Val: 0.6435 Test: 0.5109\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 145, Loss: 0.6148 Val: 0.6067 Test: 0.4198\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 146, Loss: 0.4951 Val: 0.5865 Test: 0.3954\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 147, Loss: 0.5670 Val: 0.5946 Test: 0.3487\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 148, Loss: 0.6112 Val: 0.5994 Test: 0.3570\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 149, Loss: 0.6648 Val: 0.5973 Test: 0.3678\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 150, Loss: 0.5753 Val: 0.5787 Test: 0.3980\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 151, Loss: 0.6763 Val: 0.5690 Test: 0.4450\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 152, Loss: 0.6479 Val: 0.5681 Test: 0.4597\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 153, Loss: 0.6045 Val: 0.5541 Test: 0.4142\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 154, Loss: 0.5096 Val: 0.5542 Test: 0.3962\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 155, Loss: 0.6223 Val: 0.6315 Test: 0.7047\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 156, Loss: 0.7182 Val: 0.6209 Test: 0.7282\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 157, Loss: 0.7666 Val: 0.6025 Test: 0.7107\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 158, Loss: 0.6466 Val: 0.5917 Test: 0.6778\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 159, Loss: 0.7200 Val: 0.5823 Test: 0.5877\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 160, Loss: 0.7431 Val: 0.5914 Test: 0.5438\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 161, Loss: 0.6443 Val: 0.5911 Test: 0.5453\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 162, Loss: 0.7056 Val: 0.5834 Test: 0.5290\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 163, Loss: 0.6890 Val: 0.5743 Test: 0.5154\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 164, Loss: 0.6707 Val: 0.5724 Test: 0.5118\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 165, Loss: 0.6366 Val: 0.5749 Test: 0.4905\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 166, Loss: 0.7074 Val: 0.5774 Test: 0.4799\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 167, Loss: 0.6140 Val: 0.5831 Test: 0.4806\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 168, Loss: 0.6761 Val: 0.5907 Test: 0.4889\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 169, Loss: 0.6493 Val: 0.6065 Test: 0.4976\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 170, Loss: 0.5722 Val: 0.6148 Test: 0.4983\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 171, Loss: 0.6101 Val: 0.6140 Test: 0.4909\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 172, Loss: 0.6031 Val: 0.6088 Test: 0.5193\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 173, Loss: 0.5330 Val: 0.5904 Test: 0.5121\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 174, Loss: 0.5394 Val: 0.5656 Test: 0.5124\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 175, Loss: 0.5597 Val: 0.5572 Test: 0.5088\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 176, Loss: 0.5503 Val: 0.5711 Test: 0.5044\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 177, Loss: 0.5222 Val: 0.5811 Test: 0.5152\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 178, Loss: 0.5146 Val: 0.5747 Test: 0.5009\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 179, Loss: 0.5747 Val: 0.5594 Test: 0.4988\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 180, Loss: 0.4737 Val: 0.5517 Test: 0.4622\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 181, Loss: 0.3845 Val: 0.5589 Test: 0.4558\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 182, Loss: 0.5272 Val: 0.5382 Test: 0.4537\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 183, Loss: 0.6079 Val: 0.5434 Test: 0.3754\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 184, Loss: 0.6144 Val: 0.5337 Test: 0.3412\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 185, Loss: 0.4943 Val: 0.5339 Test: 0.3361\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 186, Loss: 0.4940 Val: 0.5667 Test: 0.3589\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 187, Loss: 0.4547 Val: 0.5763 Test: 0.3794\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 188, Loss: 0.4197 Val: 0.5802 Test: 0.3995\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 189, Loss: 0.5431 Val: 0.5902 Test: 0.4113\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 190, Loss: 0.4868 Val: 0.6071 Test: 0.4189\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 191, Loss: 0.4890 Val: 0.6193 Test: 0.4438\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 192, Loss: 0.5493 Val: 0.6139 Test: 0.4857\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 193, Loss: 0.5090 Val: 0.6027 Test: 0.5720\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 194, Loss: 0.4859 Val: 0.5944 Test: 0.5704\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 195, Loss: 0.4224 Val: 0.5948 Test: 0.5373\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 196, Loss: 0.3933 Val: 0.5822 Test: 0.4869\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 197, Loss: 0.3965 Val: 0.5703 Test: 0.4366\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 198, Loss: 0.5567 Val: 0.5402 Test: 0.3966\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 199, Loss: 0.4628 Val: 0.5434 Test: 0.3899\n",
      "pkci\n",
      "\n",
      "torch.Size([1270, 115])\n",
      "{'name': 'pkci', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe05383d0>, 'pre_filter': None, '_indices': [44, 41, 24, 21, 37, 16, 0, 13, 42, 12, 60, 11, 1, 23, 36, 47, 49, 43, 52, 56, 17, 20, 51, 57, 32, 26, 59, 54, 35, 6, 58, 18, 10, 8, 34, 45, 5, 39, 30, 27, 25, 19, 33, 50, 28, 61, 38, 40, 53, 15, 46, 3, 48, 29, 31, 9, 7, 4, 55, 2, 14, 22], 'data': Data(x=[1270, 115], edge_index=[2, 2870], edge_attr=[2870, 7], y=[62, 1], smiles=[62]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   10,   21,   32,   47,   57,   67,   78,   92,  104,  116,  128,\n",
      "         144,  158,  172,  186,  201,  219,  234,  253,  271,  290,  309,  327,\n",
      "         345,  364,  383,  405,  429,  453,  477,  502,  527,  553,  579,  607,\n",
      "         636,  665,  694,  723,  752,  781,  810,  842,  872,  902,  933,  965,\n",
      "         992, 1011, 1029, 1049, 1066, 1091, 1107, 1125, 1141, 1169, 1185, 1207,\n",
      "        1235, 1254, 1270]), 'edge_index': tensor([   0,   22,   46,   70,  102,  124,  146,  170,  200,  226,  252,  276,\n",
      "         310,  338,  366,  396,  430,  470,  504,  548,  590,  634,  678,  720,\n",
      "         762,  806,  850,  902,  958, 1014, 1070, 1128, 1186, 1246, 1306, 1370,\n",
      "        1438, 1506, 1574, 1642, 1710, 1778, 1846, 1920, 1990, 2060, 2132, 2206,\n",
      "        2270, 2312, 2350, 2392, 2428, 2482, 2516, 2554, 2588, 2650, 2684, 2732,\n",
      "        2794, 2836, 2870]), 'edge_attr': tensor([   0,   22,   46,   70,  102,  124,  146,  170,  200,  226,  252,  276,\n",
      "         310,  338,  366,  396,  430,  470,  504,  548,  590,  634,  678,  720,\n",
      "         762,  806,  850,  902,  958, 1014, 1070, 1128, 1186, 1246, 1306, 1370,\n",
      "        1438, 1506, 1574, 1642, 1710, 1778, 1846, 1920, 1990, 2060, 2132, 2206,\n",
      "        2270, 2312, 2350, 2392, 2428, 2482, 2516, 2554, 2588, 2650, 2684, 2732,\n",
      "        2794, 2836, 2870]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62])}), '_data_list': None}\n",
      "minv: 2.6544301509857178\n",
      "maxv: 8.568635940551758\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 000, Loss: 0.8739 Val: 3.3224 Test: 3.4193\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 001, Loss: 0.9384 Val: 3.2836 Test: 3.3737\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 002, Loss: 0.9550 Val: 3.2454 Test: 3.3383\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 003, Loss: 0.8912 Val: 3.2215 Test: 3.3141\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 004, Loss: 0.8244 Val: 3.1951 Test: 3.2784\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 005, Loss: 0.7962 Val: 3.1751 Test: 3.2401\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 006, Loss: 0.8354 Val: 3.1509 Test: 3.2082\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 007, Loss: 0.8637 Val: 3.1319 Test: 3.1891\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 008, Loss: 0.8838 Val: 3.1052 Test: 3.1678\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 009, Loss: 0.8688 Val: 3.0843 Test: 3.1373\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 010, Loss: 0.8086 Val: 3.0724 Test: 3.1059\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 011, Loss: 0.7966 Val: 3.0574 Test: 3.0429\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 012, Loss: 0.8942 Val: 3.0479 Test: 3.0444\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 013, Loss: 0.8565 Val: 3.0180 Test: 3.0240\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 014, Loss: 0.8545 Val: 2.8456 Test: 2.9911\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 015, Loss: 0.7696 Val: 2.8859 Test: 2.9706\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 016, Loss: 0.8136 Val: 2.9576 Test: 2.8951\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 017, Loss: 0.7614 Val: 2.9258 Test: 2.9084\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 018, Loss: 0.7424 Val: 2.8429 Test: 2.8546\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 019, Loss: 0.7163 Val: 2.7962 Test: 2.7967\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 020, Loss: 0.7580 Val: 2.7532 Test: 2.7614\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 021, Loss: 0.7188 Val: 2.7135 Test: 2.7145\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 022, Loss: 0.8789 Val: 2.7166 Test: 2.6696\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 023, Loss: 0.9005 Val: 2.6876 Test: 2.6285\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 024, Loss: 0.9122 Val: 2.6485 Test: 2.5918\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 025, Loss: 0.7887 Val: 2.6053 Test: 2.5890\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 026, Loss: 0.8122 Val: 2.5173 Test: 2.5612\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 027, Loss: 0.6726 Val: 2.4682 Test: 2.5384\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 028, Loss: 0.7468 Val: 2.4355 Test: 2.5189\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 029, Loss: 0.8036 Val: 2.3830 Test: 2.4677\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 030, Loss: 0.8496 Val: 2.3589 Test: 2.4172\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 031, Loss: 0.8012 Val: 2.3082 Test: 2.3533\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 032, Loss: 0.7023 Val: 2.2683 Test: 2.2768\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 033, Loss: 0.6339 Val: 2.2468 Test: 2.2712\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 034, Loss: 0.7734 Val: 2.1935 Test: 2.2474\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 035, Loss: 0.7285 Val: 2.2045 Test: 2.2435\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 036, Loss: 0.7531 Val: 2.1706 Test: 2.2407\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 037, Loss: 0.7674 Val: 2.1186 Test: 2.2246\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 038, Loss: 0.7677 Val: 2.0389 Test: 2.2402\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 039, Loss: 0.7111 Val: 1.9731 Test: 2.2749\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 040, Loss: 0.8094 Val: 1.9978 Test: 2.2586\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 041, Loss: 0.7188 Val: 2.0670 Test: 2.2659\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 042, Loss: 0.7934 Val: 2.1319 Test: 2.2880\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 043, Loss: 0.7350 Val: 2.1361 Test: 2.2711\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 044, Loss: 0.6498 Val: 2.1056 Test: 2.2422\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 045, Loss: 0.7264 Val: 2.0545 Test: 2.2239\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 046, Loss: 0.7836 Val: 1.9725 Test: 2.1875\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 047, Loss: 0.7988 Val: 1.9010 Test: 2.1489\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 048, Loss: 0.8195 Val: 1.8342 Test: 2.1157\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 049, Loss: 0.7982 Val: 1.8172 Test: 2.0907\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 050, Loss: 0.7500 Val: 1.8579 Test: 1.7184\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 051, Loss: 0.7124 Val: 1.8300 Test: 1.7076\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 052, Loss: 0.6916 Val: 1.7948 Test: 1.6479\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 053, Loss: 0.6881 Val: 1.7498 Test: 1.5503\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 054, Loss: 0.7220 Val: 1.6932 Test: 1.4621\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 055, Loss: 0.6664 Val: 1.6935 Test: 1.4469\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 056, Loss: 0.6445 Val: 1.7582 Test: 1.4873\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 057, Loss: 0.7456 Val: 1.8024 Test: 1.5205\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 058, Loss: 0.8199 Val: 1.7829 Test: 1.5140\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 059, Loss: 0.7013 Val: 1.7595 Test: 1.5553\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 060, Loss: 0.6540 Val: 1.7399 Test: 1.5697\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 061, Loss: 0.6578 Val: 1.7156 Test: 1.5337\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 062, Loss: 0.6154 Val: 1.6936 Test: 1.6723\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 063, Loss: 0.7259 Val: 1.7530 Test: 1.7309\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 064, Loss: 0.8203 Val: 1.6895 Test: 1.6808\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 065, Loss: 0.6802 Val: 1.6687 Test: 1.5983\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 066, Loss: 0.5907 Val: 1.6758 Test: 1.4721\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 067, Loss: 0.7032 Val: 1.6482 Test: 1.4361\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 068, Loss: 0.7068 Val: 1.6667 Test: 1.4294\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 069, Loss: 0.6682 Val: 1.6497 Test: 1.3895\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 070, Loss: 0.6848 Val: 1.6616 Test: 1.3251\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 071, Loss: 0.7087 Val: 1.6890 Test: 1.3096\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 072, Loss: 0.5989 Val: 1.7125 Test: 1.2960\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 073, Loss: 0.7133 Val: 1.7164 Test: 1.3038\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 074, Loss: 0.7902 Val: 1.7126 Test: 1.4317\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 075, Loss: 0.6354 Val: 1.6750 Test: 1.4838\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 076, Loss: 0.6738 Val: 1.6624 Test: 1.4642\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 077, Loss: 0.7336 Val: 1.6813 Test: 1.4100\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 078, Loss: 0.6302 Val: 1.6991 Test: 1.3951\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 079, Loss: 0.6515 Val: 1.6711 Test: 1.3691\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 080, Loss: 0.7038 Val: 1.6120 Test: 1.3397\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 081, Loss: 0.6663 Val: 1.5556 Test: 1.3520\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 082, Loss: 0.6245 Val: 1.5933 Test: 1.3418\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 083, Loss: 0.6639 Val: 1.6408 Test: 1.3342\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 084, Loss: 0.7229 Val: 1.7106 Test: 1.3339\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 085, Loss: 0.5530 Val: 1.7533 Test: 1.3653\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 086, Loss: 0.6009 Val: 1.7167 Test: 1.3963\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 087, Loss: 0.6892 Val: 1.6886 Test: 1.3958\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 088, Loss: 0.8077 Val: 1.6616 Test: 1.3533\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 089, Loss: 0.5700 Val: 1.6712 Test: 1.2956\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 090, Loss: 0.6810 Val: 1.6885 Test: 1.2819\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 091, Loss: 0.5690 Val: 1.6981 Test: 1.2966\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 092, Loss: 0.5877 Val: 1.7021 Test: 1.3123\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 093, Loss: 0.5771 Val: 1.7498 Test: 1.2838\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 094, Loss: 0.5666 Val: 1.8277 Test: 1.2832\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 095, Loss: 0.6687 Val: 1.8382 Test: 1.3226\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 096, Loss: 0.6879 Val: 1.8086 Test: 1.3077\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 097, Loss: 0.7123 Val: 1.6880 Test: 1.2649\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 098, Loss: 0.5278 Val: 1.6686 Test: 1.3508\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 099, Loss: 0.8140 Val: 1.7209 Test: 1.3245\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 100, Loss: 0.5914 Val: 1.7475 Test: 1.3181\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 101, Loss: 0.5305 Val: 1.7301 Test: 1.3629\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 102, Loss: 0.5902 Val: 1.6976 Test: 1.4497\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 103, Loss: 0.6078 Val: 1.7321 Test: 1.4222\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 104, Loss: 0.6409 Val: 1.6521 Test: 1.3585\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 105, Loss: 0.5403 Val: 1.6334 Test: 1.3604\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 106, Loss: 0.6672 Val: 1.6501 Test: 1.3302\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 107, Loss: 0.6962 Val: 1.5622 Test: 1.2474\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 108, Loss: 0.7024 Val: 1.5846 Test: 1.3206\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 109, Loss: 0.6037 Val: 1.5962 Test: 1.3558\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 110, Loss: 0.6051 Val: 1.6734 Test: 1.3915\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 111, Loss: 0.6350 Val: 1.6618 Test: 1.4206\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 112, Loss: 0.5611 Val: 1.5215 Test: 1.4427\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 113, Loss: 0.6327 Val: 1.1477 Test: 1.3962\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 114, Loss: 0.5928 Val: 1.1235 Test: 1.4283\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 115, Loss: 0.7255 Val: 1.5191 Test: 1.4876\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 116, Loss: 0.6032 Val: 1.6133 Test: 1.5122\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 117, Loss: 0.5613 Val: 1.7225 Test: 1.4929\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 118, Loss: 0.6842 Val: 1.6636 Test: 1.4786\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 119, Loss: 0.5114 Val: 1.3288 Test: 1.4000\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 120, Loss: 0.5988 Val: 1.3596 Test: 1.4405\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 121, Loss: 0.6023 Val: 1.2259 Test: 1.5643\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 122, Loss: 0.5852 Val: 1.9574 Test: 1.7873\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 123, Loss: 0.7031 Val: 2.0074 Test: 1.4500\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 124, Loss: 0.7198 Val: 1.7523 Test: 1.3897\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 125, Loss: 0.7104 Val: 1.7788 Test: 1.2882\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 126, Loss: 0.7761 Val: 1.6995 Test: 1.2911\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 127, Loss: 0.7033 Val: 1.6429 Test: 1.3510\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 128, Loss: 0.6312 Val: 1.6126 Test: 1.3618\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 129, Loss: 0.5529 Val: 1.6480 Test: 1.4101\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 130, Loss: 0.5921 Val: 1.6910 Test: 1.4533\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 131, Loss: 0.4994 Val: 1.7440 Test: 1.4992\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 132, Loss: 0.5668 Val: 1.6694 Test: 1.5207\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 133, Loss: 0.4681 Val: 1.3631 Test: 1.5227\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 134, Loss: 0.4796 Val: 1.3391 Test: 1.5171\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 135, Loss: 0.6023 Val: 1.3180 Test: 1.4436\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 136, Loss: 0.5083 Val: 1.6824 Test: 1.2745\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 137, Loss: 0.5594 Val: 1.6537 Test: 1.2050\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 138, Loss: 0.5919 Val: 1.6462 Test: 1.1631\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 139, Loss: 0.6260 Val: 1.6647 Test: 1.2004\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 140, Loss: 0.6004 Val: 1.6796 Test: 1.2586\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 141, Loss: 0.5360 Val: 1.7166 Test: 1.2968\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 142, Loss: 0.5098 Val: 1.3695 Test: 1.4603\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 143, Loss: 0.5142 Val: 1.3869 Test: 1.5925\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 144, Loss: 0.4467 Val: 1.4057 Test: 1.5785\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 145, Loss: 0.4221 Val: 1.3989 Test: 1.5614\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 146, Loss: 0.5527 Val: 1.3711 Test: 1.5353\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 147, Loss: 0.4557 Val: 1.3650 Test: 1.5288\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 148, Loss: 0.4924 Val: 1.3855 Test: 1.5389\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 149, Loss: 0.5071 Val: 1.4167 Test: 1.5614\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 150, Loss: 0.4776 Val: 1.4481 Test: 1.5746\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 151, Loss: 0.5831 Val: 1.4168 Test: 1.4526\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 152, Loss: 0.4970 Val: 1.3705 Test: 1.3643\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 153, Loss: 0.5458 Val: 1.2608 Test: 1.3112\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 154, Loss: 0.6697 Val: 1.6268 Test: 1.2810\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 155, Loss: 0.5680 Val: 1.5781 Test: 1.2539\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 156, Loss: 0.5826 Val: 1.5383 Test: 1.2290\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 157, Loss: 0.5275 Val: 1.6062 Test: 1.2903\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 158, Loss: 0.6127 Val: 1.7512 Test: 1.3147\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 159, Loss: 0.6498 Val: 1.6632 Test: 1.3664\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 160, Loss: 0.7769 Val: 1.6426 Test: 1.3519\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 161, Loss: 0.7255 Val: 1.5289 Test: 1.1867\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 162, Loss: 0.8128 Val: 1.4963 Test: 1.1427\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 163, Loss: 0.7209 Val: 1.4994 Test: 1.2033\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 164, Loss: 0.6060 Val: 1.5581 Test: 1.2914\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 165, Loss: 0.4317 Val: 1.1051 Test: 1.3137\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 166, Loss: 0.4830 Val: 1.0687 Test: 1.3574\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 167, Loss: 0.5104 Val: 1.1111 Test: 1.3714\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 168, Loss: 0.6204 Val: 1.1678 Test: 1.3896\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 169, Loss: 0.4257 Val: 1.2391 Test: 1.3743\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 170, Loss: 0.5159 Val: 1.2639 Test: 1.4511\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 171, Loss: 0.4682 Val: 1.2972 Test: 1.5309\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 172, Loss: 0.7071 Val: 1.2594 Test: 1.7695\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 173, Loss: 0.7687 Val: 1.4198 Test: 1.5116\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 174, Loss: 0.6249 Val: 1.3542 Test: 1.4185\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 175, Loss: 0.4956 Val: 1.2990 Test: 1.3546\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 176, Loss: 0.5318 Val: 1.2237 Test: 1.3656\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 177, Loss: 0.5958 Val: 1.2436 Test: 1.3565\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 178, Loss: 0.8130 Val: 1.2365 Test: 1.3687\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 179, Loss: 0.5486 Val: 1.2384 Test: 1.6527\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 180, Loss: 0.6687 Val: 1.1800 Test: 1.4351\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 181, Loss: 0.5672 Val: 1.3776 Test: 1.4495\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 182, Loss: 0.6450 Val: 1.3921 Test: 1.4323\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 183, Loss: 0.6047 Val: 1.4113 Test: 1.3753\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 184, Loss: 0.6206 Val: 1.4854 Test: 1.5504\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 185, Loss: 0.7846 Val: 1.5046 Test: 1.4990\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 186, Loss: 0.7494 Val: 1.4246 Test: 1.4695\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 187, Loss: 0.8737 Val: 1.3057 Test: 1.4132\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 188, Loss: 0.5278 Val: 1.2964 Test: 1.3805\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 189, Loss: 0.5203 Val: 1.1800 Test: 1.4466\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 190, Loss: 0.5916 Val: 1.1486 Test: 1.7234\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 191, Loss: 0.5140 Val: 1.1483 Test: 1.7394\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 192, Loss: 0.5504 Val: 1.2763 Test: 1.7337\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 193, Loss: 0.5558 Val: 1.4153 Test: 1.7302\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 194, Loss: 0.5480 Val: 1.4523 Test: 1.4166\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 195, Loss: 0.5314 Val: 1.4549 Test: 1.4274\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 196, Loss: 0.5512 Val: 1.4348 Test: 1.4501\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 197, Loss: 0.7432 Val: 1.5400 Test: 1.6291\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 198, Loss: 0.5678 Val: 1.3944 Test: 1.4702\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 199, Loss: 0.6326 Val: 1.3213 Test: 1.4549\n",
      "phgdh\n",
      "\n",
      "torch.Size([1831, 115])\n",
      "{'name': 'phgdh', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe04f3940>, 'pre_filter': None, '_indices': [28, 61, 66, 63, 46, 39, 49, 64, 4, 32, 9, 33, 5, 55, 60, 31, 22, 34, 7, 53, 27, 29, 45, 13, 20, 24, 38, 56, 10, 17, 6, 37, 36, 59, 54, 48, 26, 51, 43, 57, 52, 21, 11, 3, 15, 25, 0, 41, 16, 35, 58, 2, 40, 19, 12, 65, 47, 30, 62, 14, 42, 1, 18, 8, 50, 44, 23], 'data': Data(x=[1831, 115], edge_index=[2, 3954], edge_attr=[3954, 7], y=[67, 1], smiles=[67]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   30,   54,   85,  109,  133,  162,  191,  219,  247,  270,  300,\n",
      "         330,  356,  382,  412,  437,  461,  488,  516,  548,  574,  602,  625,\n",
      "         651,  677,  707,  733,  763,  790,  814,  841,  869,  895,  922,  952,\n",
      "         978, 1007, 1030, 1054, 1081, 1112, 1137, 1167, 1193, 1220, 1247, 1275,\n",
      "        1304, 1334, 1360, 1385, 1410, 1437, 1469, 1497, 1523, 1550, 1582, 1611,\n",
      "        1636, 1665, 1695, 1720, 1746, 1774, 1804, 1831]), 'edge_index': tensor([   0,   66,  118,  184,  236,  288,  352,  416,  478,  538,  588,  652,\n",
      "         716,  772,  828,  892,  946,  998, 1056, 1116, 1184, 1240, 1300, 1350,\n",
      "        1406, 1462, 1528, 1584, 1648, 1706, 1758, 1816, 1878, 1934, 1992, 2058,\n",
      "        2114, 2178, 2228, 2280, 2338, 2404, 2458, 2522, 2578, 2636, 2694, 2754,\n",
      "        2818, 2882, 2938, 2992, 3046, 3104, 3172, 3234, 3290, 3348, 3418, 3480,\n",
      "        3534, 3598, 3662, 3716, 3772, 3832, 3896, 3954]), 'edge_attr': tensor([   0,   66,  118,  184,  236,  288,  352,  416,  478,  538,  588,  652,\n",
      "         716,  772,  828,  892,  946,  998, 1056, 1116, 1184, 1240, 1300, 1350,\n",
      "        1406, 1462, 1528, 1584, 1648, 1706, 1758, 1816, 1878, 1934, 1992, 2058,\n",
      "        2114, 2178, 2228, 2280, 2338, 2404, 2458, 2522, 2578, 2636, 2694, 2754,\n",
      "        2818, 2882, 2938, 2992, 3046, 3104, 3172, 3234, 3290, 3348, 3418, 3480,\n",
      "        3534, 3598, 3662, 3716, 3772, 3832, 3896, 3954]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67])}), '_data_list': None}\n",
      "minv: 4.0\n",
      "maxv: 8.699999809265137\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 000, Loss: 1.0640 Val: 2.6055 Test: 2.6872\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 001, Loss: 0.9997 Val: 2.5605 Test: 2.6559\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 002, Loss: 1.0070 Val: 2.5030 Test: 2.5980\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 003, Loss: 1.0209 Val: 2.4387 Test: 2.5388\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 004, Loss: 0.9583 Val: 2.3997 Test: 2.4936\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 005, Loss: 0.9755 Val: 2.3323 Test: 2.4493\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 006, Loss: 0.9544 Val: 2.2688 Test: 2.3970\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 007, Loss: 0.9465 Val: 2.2167 Test: 2.3555\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 008, Loss: 0.9065 Val: 2.2028 Test: 2.3180\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 009, Loss: 0.9207 Val: 2.1788 Test: 2.2908\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 010, Loss: 0.9604 Val: 2.1235 Test: 2.2494\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 011, Loss: 0.8510 Val: 2.0778 Test: 2.2091\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 012, Loss: 0.8891 Val: 2.0347 Test: 2.1763\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 013, Loss: 0.9505 Val: 1.9649 Test: 2.1243\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 014, Loss: 0.9256 Val: 1.8852 Test: 2.0785\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 015, Loss: 0.8490 Val: 1.8208 Test: 2.0382\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 016, Loss: 0.8421 Val: 1.7692 Test: 2.0000\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 017, Loss: 0.8896 Val: 1.7340 Test: 1.9655\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 018, Loss: 0.8543 Val: 1.7147 Test: 1.9152\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 019, Loss: 0.8667 Val: 1.8567 Test: 1.8978\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 020, Loss: 0.8854 Val: 1.8865 Test: 1.9123\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 021, Loss: 0.9825 Val: 1.7213 Test: 1.8278\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 022, Loss: 0.9243 Val: 1.5341 Test: 1.7477\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 023, Loss: 0.8932 Val: 1.4697 Test: 1.6865\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 024, Loss: 0.8789 Val: 1.4200 Test: 1.6340\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 025, Loss: 0.8469 Val: 1.3918 Test: 1.6028\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 026, Loss: 0.8775 Val: 1.3684 Test: 1.5656\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 027, Loss: 0.8274 Val: 1.3241 Test: 1.5215\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 028, Loss: 0.7879 Val: 1.2884 Test: 1.4723\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 029, Loss: 0.8305 Val: 1.2611 Test: 1.4362\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 030, Loss: 0.8417 Val: 1.2409 Test: 1.4163\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 031, Loss: 0.8368 Val: 1.2113 Test: 1.4074\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 032, Loss: 0.8374 Val: 1.1884 Test: 1.4366\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 033, Loss: 0.8712 Val: 1.1654 Test: 1.4332\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 034, Loss: 0.8913 Val: 1.1281 Test: 1.3872\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 035, Loss: 0.8876 Val: 1.1030 Test: 1.3803\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 036, Loss: 0.8301 Val: 1.0868 Test: 1.4057\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 037, Loss: 0.8321 Val: 1.0750 Test: 1.4221\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 038, Loss: 0.7972 Val: 1.0616 Test: 1.3953\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 039, Loss: 0.8112 Val: 1.0462 Test: 1.3529\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 040, Loss: 0.8128 Val: 1.0382 Test: 1.2960\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 041, Loss: 0.7926 Val: 1.0299 Test: 1.2549\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 042, Loss: 0.8287 Val: 1.0301 Test: 1.2512\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 043, Loss: 0.9403 Val: 1.1382 Test: 1.2721\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 044, Loss: 0.9097 Val: 1.0233 Test: 1.2485\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 045, Loss: 0.8238 Val: 0.9812 Test: 1.2175\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 046, Loss: 0.8597 Val: 0.9914 Test: 1.1725\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 047, Loss: 0.8052 Val: 0.9716 Test: 1.1269\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 048, Loss: 0.7719 Val: 0.9551 Test: 1.0852\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 049, Loss: 0.8292 Val: 0.9891 Test: 1.1146\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 050, Loss: 0.7949 Val: 0.8617 Test: 1.1253\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 051, Loss: 0.8094 Val: 0.8587 Test: 1.1194\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 052, Loss: 0.7511 Val: 0.8772 Test: 1.0672\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 053, Loss: 0.7633 Val: 0.8800 Test: 1.0279\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 054, Loss: 0.7998 Val: 0.8538 Test: 0.9936\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 055, Loss: 0.8336 Val: 0.8387 Test: 0.9720\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 056, Loss: 0.8537 Val: 0.8365 Test: 0.9669\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 057, Loss: 0.8059 Val: 0.8469 Test: 0.9618\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 058, Loss: 0.7979 Val: 0.8611 Test: 0.9567\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 059, Loss: 0.7388 Val: 0.8750 Test: 0.9525\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 060, Loss: 0.7165 Val: 0.9033 Test: 0.9468\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 061, Loss: 0.7298 Val: 0.9202 Test: 0.9396\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 062, Loss: 0.7754 Val: 0.9025 Test: 0.9162\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 063, Loss: 0.7895 Val: 0.8845 Test: 0.8985\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 064, Loss: 0.8640 Val: 0.8493 Test: 0.8824\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 065, Loss: 0.8482 Val: 0.7799 Test: 0.8549\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 066, Loss: 0.7477 Val: 0.6563 Test: 0.8083\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 067, Loss: 0.7015 Val: 0.6249 Test: 0.7683\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 068, Loss: 0.7616 Val: 0.5976 Test: 0.7260\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 069, Loss: 0.7180 Val: 0.6250 Test: 0.8045\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 070, Loss: 0.7677 Val: 0.6560 Test: 0.8130\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 071, Loss: 0.7978 Val: 0.8061 Test: 0.7736\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 072, Loss: 0.6903 Val: 0.8421 Test: 0.8103\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 073, Loss: 0.7817 Val: 0.8561 Test: 0.8576\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 074, Loss: 0.7787 Val: 0.7872 Test: 0.8885\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 075, Loss: 0.7929 Val: 0.9416 Test: 0.8792\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 076, Loss: 0.8287 Val: 0.9198 Test: 0.8543\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 077, Loss: 0.8133 Val: 0.8825 Test: 0.8228\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 078, Loss: 0.8024 Val: 0.8623 Test: 0.7907\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 079, Loss: 0.7155 Val: 0.8866 Test: 0.7849\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 080, Loss: 0.7477 Val: 0.8949 Test: 0.7732\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 081, Loss: 0.7229 Val: 0.8813 Test: 0.7613\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 082, Loss: 0.7512 Val: 0.8723 Test: 0.7529\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 083, Loss: 0.7172 Val: 0.8508 Test: 0.7383\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 084, Loss: 0.7369 Val: 0.7960 Test: 0.7114\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 085, Loss: 0.7793 Val: 0.7773 Test: 0.6956\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 086, Loss: 0.7420 Val: 0.8210 Test: 0.7054\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 087, Loss: 0.7497 Val: 0.7924 Test: 0.7022\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 088, Loss: 0.7166 Val: 0.7636 Test: 0.6927\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 089, Loss: 0.7574 Val: 0.7781 Test: 0.6843\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 090, Loss: 0.7168 Val: 0.8787 Test: 0.6988\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 091, Loss: 0.7675 Val: 0.9495 Test: 0.7137\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 092, Loss: 0.7677 Val: 0.9695 Test: 0.7320\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 093, Loss: 0.7520 Val: 0.8825 Test: 0.7103\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 094, Loss: 0.7546 Val: 0.8344 Test: 0.6883\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 095, Loss: 0.7443 Val: 0.7964 Test: 0.6807\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 096, Loss: 0.7683 Val: 0.7450 Test: 0.6700\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 097, Loss: 0.7603 Val: 0.6684 Test: 0.6426\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 098, Loss: 0.7502 Val: 0.7192 Test: 0.6588\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 099, Loss: 0.7339 Val: 0.8270 Test: 0.6919\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 100, Loss: 0.7980 Val: 0.6629 Test: 0.6452\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 101, Loss: 0.8045 Val: 0.6044 Test: 0.6619\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 102, Loss: 0.7703 Val: 0.7303 Test: 0.7258\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 103, Loss: 0.7073 Val: 0.7577 Test: 0.6882\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 104, Loss: 0.7611 Val: 0.5940 Test: 0.6961\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 105, Loss: 0.7609 Val: 0.7040 Test: 0.8628\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 106, Loss: 0.7949 Val: 1.0557 Test: 0.7084\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 107, Loss: 0.8062 Val: 0.9035 Test: 0.6555\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 108, Loss: 0.8464 Val: 0.8807 Test: 0.6786\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 109, Loss: 0.8356 Val: 0.8527 Test: 0.6465\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 110, Loss: 0.7799 Val: 0.8264 Test: 0.6708\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 111, Loss: 0.8004 Val: 0.8208 Test: 0.6992\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 112, Loss: 0.8154 Val: 0.8349 Test: 0.7369\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 113, Loss: 0.7475 Val: 0.8327 Test: 0.7213\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 114, Loss: 0.6979 Val: 0.8280 Test: 0.6620\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 115, Loss: 0.7587 Val: 0.8486 Test: 0.5918\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 116, Loss: 0.7384 Val: 0.8858 Test: 0.6117\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 117, Loss: 0.7689 Val: 0.9346 Test: 0.6149\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 118, Loss: 0.7650 Val: 0.8560 Test: 0.6043\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 119, Loss: 0.7034 Val: 0.8251 Test: 0.5843\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 120, Loss: 0.7364 Val: 0.8136 Test: 0.5874\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 121, Loss: 0.8219 Val: 0.7833 Test: 0.5789\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 122, Loss: 0.7214 Val: 0.7835 Test: 0.5761\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 123, Loss: 0.8072 Val: 0.7753 Test: 0.5585\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 124, Loss: 0.7236 Val: 0.7647 Test: 0.5469\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 125, Loss: 0.8376 Val: 0.7992 Test: 0.6728\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 126, Loss: 0.7307 Val: 0.8524 Test: 0.6913\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 127, Loss: 0.7851 Val: 0.8524 Test: 0.6391\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 128, Loss: 0.7404 Val: 0.8621 Test: 0.5749\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 129, Loss: 0.7902 Val: 0.8741 Test: 0.5834\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 130, Loss: 0.7709 Val: 0.8845 Test: 0.5971\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 131, Loss: 0.7335 Val: 0.8323 Test: 0.5904\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 132, Loss: 0.7750 Val: 0.8569 Test: 0.5758\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 133, Loss: 0.7431 Val: 0.8624 Test: 0.7283\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 134, Loss: 0.7759 Val: 0.8764 Test: 0.6762\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 135, Loss: 0.7546 Val: 0.9744 Test: 0.5858\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 136, Loss: 0.7542 Val: 1.0297 Test: 0.6301\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 137, Loss: 0.7718 Val: 1.0170 Test: 0.6542\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 138, Loss: 0.7502 Val: 0.9924 Test: 0.6866\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 139, Loss: 0.8257 Val: 0.9533 Test: 0.7190\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 140, Loss: 0.7059 Val: 0.9306 Test: 0.6807\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 141, Loss: 0.8025 Val: 0.9434 Test: 0.6484\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 142, Loss: 0.7776 Val: 0.9316 Test: 0.6222\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 143, Loss: 0.7269 Val: 0.9194 Test: 0.6079\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 144, Loss: 0.7337 Val: 0.9058 Test: 0.6005\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 145, Loss: 0.7028 Val: 0.8911 Test: 0.5939\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 146, Loss: 0.7807 Val: 0.8872 Test: 0.6022\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 147, Loss: 0.8054 Val: 0.8856 Test: 0.6222\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 148, Loss: 0.7145 Val: 0.8775 Test: 0.6394\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 149, Loss: 0.7195 Val: 0.8733 Test: 0.6470\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 150, Loss: 0.8190 Val: 0.8635 Test: 0.6466\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 151, Loss: 0.7208 Val: 0.9047 Test: 0.6122\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 152, Loss: 0.6866 Val: 0.8766 Test: 0.6088\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 153, Loss: 0.7153 Val: 0.8499 Test: 0.6143\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 154, Loss: 0.7603 Val: 0.8588 Test: 0.6328\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 155, Loss: 0.7535 Val: 0.8733 Test: 0.6506\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 156, Loss: 0.7436 Val: 0.8759 Test: 0.6198\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 157, Loss: 0.7474 Val: 0.8613 Test: 0.5512\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 158, Loss: 0.6805 Val: 0.8284 Test: 0.5402\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 159, Loss: 0.7426 Val: 0.8022 Test: 0.5653\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 160, Loss: 0.7514 Val: 0.8026 Test: 0.5869\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 161, Loss: 0.6985 Val: 0.8366 Test: 0.5845\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 162, Loss: 0.7143 Val: 1.0306 Test: 0.5774\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 163, Loss: 0.7832 Val: 1.1422 Test: 0.8009\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 164, Loss: 0.7137 Val: 0.9261 Test: 0.7263\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 165, Loss: 0.6853 Val: 0.8808 Test: 0.5931\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 166, Loss: 0.7511 Val: 0.8298 Test: 0.6063\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 167, Loss: 0.7263 Val: 0.8517 Test: 0.5218\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 168, Loss: 0.6519 Val: 0.8862 Test: 0.5429\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 169, Loss: 0.6709 Val: 0.9756 Test: 0.6067\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 170, Loss: 0.7776 Val: 0.9752 Test: 0.6096\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 171, Loss: 0.8125 Val: 0.9424 Test: 0.5741\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 172, Loss: 0.7392 Val: 0.9347 Test: 0.5297\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 173, Loss: 0.7837 Val: 0.9724 Test: 0.5014\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 174, Loss: 0.7240 Val: 0.9920 Test: 0.5654\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 175, Loss: 0.7362 Val: 0.9705 Test: 0.6341\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 176, Loss: 0.7360 Val: 1.1397 Test: 0.6422\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 177, Loss: 0.7668 Val: 1.1682 Test: 0.7305\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 178, Loss: 0.7000 Val: 1.1295 Test: 0.7641\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 179, Loss: 0.7809 Val: 0.9850 Test: 0.7681\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 180, Loss: 0.7480 Val: 0.8215 Test: 0.7192\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 181, Loss: 0.7765 Val: 0.7709 Test: 0.6800\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 182, Loss: 0.7668 Val: 0.7887 Test: 0.6761\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 183, Loss: 0.7521 Val: 0.8046 Test: 0.6421\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 184, Loss: 0.7323 Val: 0.7170 Test: 0.5975\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 185, Loss: 0.6978 Val: 0.6980 Test: 0.5683\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 186, Loss: 0.6850 Val: 0.6795 Test: 0.5536\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 187, Loss: 0.7086 Val: 0.6683 Test: 0.6078\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 188, Loss: 0.6948 Val: 0.6827 Test: 0.6130\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 189, Loss: 0.6047 Val: 0.7311 Test: 0.6341\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 190, Loss: 0.9045 Val: 0.7272 Test: 0.6467\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 191, Loss: 0.8478 Val: 0.7140 Test: 0.6928\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 192, Loss: 0.7753 Val: 0.9428 Test: 0.6812\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 193, Loss: 0.7972 Val: 0.9943 Test: 0.6699\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 194, Loss: 0.7878 Val: 1.0088 Test: 0.7513\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 195, Loss: 0.8490 Val: 1.0407 Test: 0.7123\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 196, Loss: 0.8440 Val: 1.0434 Test: 0.6744\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 197, Loss: 0.7766 Val: 1.0263 Test: 0.6711\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 198, Loss: 0.7226 Val: 1.0222 Test: 0.6137\n",
      "16\n",
      "16\n",
      "9\n",
      "Epoch: 199, Loss: 0.7680 Val: 1.0082 Test: 0.5713\n",
      "rorg\n",
      "\n",
      "torch.Size([2778, 115])\n",
      "{'name': 'rorg', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe04f3b20>, 'pre_filter': None, '_indices': [49, 43, 54, 25, 38, 50, 0, 63, 11, 60, 42, 40, 28, 52, 55, 21, 59, 26, 15, 45, 17, 56, 20, 7, 48, 22, 16, 30, 4, 41, 5, 37, 65, 12, 13, 9, 27, 62, 57, 53, 66, 32, 8, 34, 33, 2, 19, 14, 23, 10, 58, 29, 3, 39, 36, 61, 35, 67, 31, 51, 46, 24, 64, 44, 1, 18, 6, 47], 'data': Data(x=[2778, 115], edge_index=[2, 5836], edge_attr=[5836, 7], y=[68, 1], smiles=[68]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   44,   88,  128,  170,  211,  252,  291,  331,  370,  410,  450,\n",
      "         496,  538,  577,  621,  662,  701,  742,  783,  827,  867,  912,  954,\n",
      "         995, 1035, 1077, 1117, 1160, 1198, 1238, 1278, 1320, 1363, 1404, 1444,\n",
      "        1483, 1526, 1567, 1609, 1648, 1686, 1726, 1767, 1806, 1846, 1884, 1923,\n",
      "        1963, 2003, 2044, 2082, 2122, 2166, 2210, 2252, 2295, 2339, 2380, 2420,\n",
      "        2461, 2496, 2537, 2578, 2616, 2657, 2699, 2738, 2778]), 'edge_index': tensor([   0,   92,  184,  268,  354,  438,  526,  606,  690,  772,  854,  938,\n",
      "        1038, 1128, 1208, 1304, 1392, 1476, 1562, 1650, 1746, 1830, 1926, 2014,\n",
      "        2100, 2184, 2272, 2356, 2446, 2528, 2612, 2696, 2784, 2874, 2960, 3044,\n",
      "        3124, 3214, 3300, 3388, 3472, 3550, 3634, 3720, 3802, 3888, 3966, 4048,\n",
      "        4132, 4216, 4302, 4380, 4462, 4556, 4650, 4738, 4828, 4922, 5008, 5090,\n",
      "        5174, 5248, 5334, 5420, 5500, 5584, 5672, 5754, 5836]), 'edge_attr': tensor([   0,   92,  184,  268,  354,  438,  526,  606,  690,  772,  854,  938,\n",
      "        1038, 1128, 1208, 1304, 1392, 1476, 1562, 1650, 1746, 1830, 1926, 2014,\n",
      "        2100, 2184, 2272, 2356, 2446, 2528, 2612, 2696, 2784, 2874, 2960, 3044,\n",
      "        3124, 3214, 3300, 3388, 3472, 3550, 3634, 3720, 3802, 3888, 3966, 4048,\n",
      "        4132, 4216, 4302, 4380, 4462, 4556, 4650, 4738, 4828, 4922, 5008, 5090,\n",
      "        5174, 5248, 5334, 5420, 5500, 5584, 5672, 5754, 5836]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68])}), '_data_list': None}\n",
      "minv: 5.559999942779541\n",
      "maxv: 8.369999885559082\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 000, Loss: 1.0309 Val: 1.6291 Test: 1.8745\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 001, Loss: 0.9877 Val: 1.5818 Test: 1.8385\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 002, Loss: 0.9625 Val: 1.5400 Test: 1.8073\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 003, Loss: 0.8843 Val: 1.4997 Test: 1.7673\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 004, Loss: 0.8630 Val: 1.4546 Test: 1.7127\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 005, Loss: 0.8639 Val: 1.4066 Test: 1.6676\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 006, Loss: 0.8289 Val: 1.3606 Test: 1.6187\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 007, Loss: 0.8568 Val: 1.3237 Test: 1.5843\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 008, Loss: 0.8405 Val: 1.2982 Test: 1.5552\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 009, Loss: 0.8191 Val: 1.2484 Test: 1.5051\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 010, Loss: 0.8289 Val: 1.2063 Test: 1.4587\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 011, Loss: 0.8375 Val: 1.1665 Test: 1.4218\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 012, Loss: 0.8828 Val: 1.1213 Test: 1.3844\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 013, Loss: 0.7776 Val: 1.1018 Test: 1.3558\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 014, Loss: 0.7837 Val: 1.0932 Test: 1.3320\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 015, Loss: 0.8238 Val: 1.0603 Test: 1.3388\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 016, Loss: 0.8332 Val: 1.0281 Test: 1.3038\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 017, Loss: 0.7502 Val: 1.0046 Test: 1.2754\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 018, Loss: 0.8123 Val: 0.9876 Test: 1.2720\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 019, Loss: 0.8327 Val: 0.9688 Test: 1.2738\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 020, Loss: 0.8871 Val: 0.9393 Test: 1.2100\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 021, Loss: 0.7613 Val: 0.8654 Test: 1.1389\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 022, Loss: 0.7797 Val: 0.8142 Test: 1.0744\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 023, Loss: 0.7910 Val: 0.7799 Test: 1.0417\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 024, Loss: 0.7863 Val: 0.7617 Test: 1.0147\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 025, Loss: 0.7634 Val: 0.7561 Test: 0.9841\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 026, Loss: 0.6935 Val: 0.7244 Test: 0.9817\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 027, Loss: 0.6658 Val: 0.6529 Test: 0.8981\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 028, Loss: 0.7038 Val: 0.6075 Test: 0.8524\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 029, Loss: 0.6042 Val: 0.5973 Test: 0.8540\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 030, Loss: 0.6656 Val: 0.5885 Test: 0.8369\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 031, Loss: 0.6808 Val: 0.5352 Test: 0.7381\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 032, Loss: 0.6478 Val: 0.5351 Test: 0.7203\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 033, Loss: 0.7120 Val: 0.5660 Test: 0.7099\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 034, Loss: 0.7526 Val: 0.4420 Test: 0.6194\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 035, Loss: 0.5899 Val: 0.4075 Test: 0.5598\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 036, Loss: 0.8091 Val: 0.4018 Test: 0.5241\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 037, Loss: 0.7114 Val: 0.3820 Test: 0.5156\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 038, Loss: 0.6935 Val: 0.3889 Test: 0.5218\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 039, Loss: 0.6847 Val: 0.4056 Test: 0.5387\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 040, Loss: 0.6226 Val: 0.4293 Test: 0.5273\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 041, Loss: 0.6287 Val: 0.4436 Test: 0.5138\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 042, Loss: 0.6504 Val: 0.4445 Test: 0.4800\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 043, Loss: 0.6923 Val: 0.4403 Test: 0.4822\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 044, Loss: 0.6837 Val: 0.4607 Test: 0.4753\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 045, Loss: 0.6048 Val: 0.4802 Test: 0.4357\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 046, Loss: 0.5880 Val: 0.5065 Test: 0.3906\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 047, Loss: 0.5928 Val: 0.5327 Test: 0.4037\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 048, Loss: 0.6382 Val: 0.5630 Test: 0.4062\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 049, Loss: 0.6070 Val: 0.5536 Test: 0.3426\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 050, Loss: 0.5584 Val: 0.5105 Test: 0.3036\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 051, Loss: 0.6753 Val: 0.5538 Test: 0.3485\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 052, Loss: 0.6821 Val: 0.5638 Test: 0.3718\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 053, Loss: 0.6172 Val: 0.5715 Test: 0.4224\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 054, Loss: 0.7184 Val: 0.5674 Test: 0.4945\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 055, Loss: 0.7042 Val: 0.5694 Test: 0.5045\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 056, Loss: 0.6853 Val: 0.5831 Test: 0.4591\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 057, Loss: 0.7240 Val: 0.5867 Test: 0.4399\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 058, Loss: 0.6894 Val: 0.5689 Test: 0.4643\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 059, Loss: 0.5614 Val: 0.5327 Test: 0.4586\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 060, Loss: 0.6120 Val: 0.5206 Test: 0.3800\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 061, Loss: 0.5991 Val: 0.5437 Test: 0.3816\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 062, Loss: 0.5846 Val: 0.5897 Test: 0.3924\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 063, Loss: 0.5602 Val: 0.5980 Test: 0.3814\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 064, Loss: 0.5315 Val: 0.5774 Test: 0.3618\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 065, Loss: 0.5548 Val: 0.5584 Test: 0.3964\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 066, Loss: 0.5051 Val: 0.6139 Test: 0.4392\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 067, Loss: 0.6290 Val: 0.6548 Test: 0.4958\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 068, Loss: 0.5885 Val: 0.6637 Test: 0.6424\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 069, Loss: 0.4675 Val: 0.6315 Test: 0.6463\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 070, Loss: 0.4349 Val: 0.6280 Test: 0.6409\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 071, Loss: 0.5042 Val: 0.6163 Test: 0.5803\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 072, Loss: 0.3837 Val: 0.6252 Test: 0.5687\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 073, Loss: 0.3833 Val: 0.7019 Test: 0.6149\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 074, Loss: 0.9036 Val: 0.7192 Test: 0.6096\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 075, Loss: 0.9055 Val: 0.6174 Test: 0.5025\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 076, Loss: 0.7996 Val: 0.5835 Test: 0.4335\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 077, Loss: 0.6479 Val: 0.5945 Test: 0.4066\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 078, Loss: 0.7159 Val: 0.5914 Test: 0.3679\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 079, Loss: 0.6789 Val: 0.5976 Test: 0.3524\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 080, Loss: 0.6344 Val: 0.6657 Test: 0.3472\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 081, Loss: 0.5767 Val: 0.6884 Test: 0.3489\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 082, Loss: 0.5333 Val: 0.6096 Test: 0.2991\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 083, Loss: 0.6355 Val: 0.6214 Test: 0.3421\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 084, Loss: 0.6419 Val: 0.7073 Test: 0.4397\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 085, Loss: 0.5963 Val: 0.6989 Test: 0.4393\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 086, Loss: 0.5920 Val: 0.6591 Test: 0.4130\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 087, Loss: 0.4850 Val: 0.6190 Test: 0.3646\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 088, Loss: 0.6013 Val: 0.6111 Test: 0.3580\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 089, Loss: 0.4830 Val: 0.6298 Test: 0.3931\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 090, Loss: 0.4786 Val: 0.6588 Test: 0.4382\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 091, Loss: 0.4477 Val: 0.6666 Test: 0.4575\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 092, Loss: 0.5024 Val: 0.6595 Test: 0.4615\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 093, Loss: 0.5624 Val: 0.6931 Test: 0.3919\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 094, Loss: 0.7350 Val: 0.6926 Test: 0.3136\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 095, Loss: 0.7774 Val: 0.7159 Test: 0.5167\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 096, Loss: 0.6001 Val: 0.6707 Test: 0.4631\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 097, Loss: 0.5829 Val: 0.6656 Test: 0.4212\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 098, Loss: 0.5608 Val: 0.6684 Test: 0.4107\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 099, Loss: 0.5660 Val: 0.6378 Test: 0.3875\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 100, Loss: 0.5937 Val: 0.6005 Test: 0.3579\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 101, Loss: 0.6497 Val: 0.6165 Test: 0.3480\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 102, Loss: 0.4987 Val: 0.6556 Test: 0.3957\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 103, Loss: 0.5863 Val: 0.6885 Test: 0.5103\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 104, Loss: 0.7924 Val: 0.6938 Test: 0.5742\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 105, Loss: 0.6408 Val: 0.6820 Test: 0.5300\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 106, Loss: 0.5549 Val: 0.6527 Test: 0.5174\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 107, Loss: 0.5423 Val: 0.6247 Test: 0.5028\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 108, Loss: 0.5422 Val: 0.6013 Test: 0.4671\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 109, Loss: 0.6093 Val: 0.5978 Test: 0.4060\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 110, Loss: 0.4904 Val: 0.6032 Test: 0.3856\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 111, Loss: 0.4029 Val: 0.6069 Test: 0.3815\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 112, Loss: 0.4866 Val: 0.6074 Test: 0.4080\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 113, Loss: 0.5573 Val: 0.5805 Test: 0.4278\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 114, Loss: 0.4005 Val: 0.6119 Test: 0.3669\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 115, Loss: 0.7285 Val: 0.6473 Test: 0.4796\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 116, Loss: 0.5136 Val: 0.6701 Test: 0.5055\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 117, Loss: 0.7437 Val: 0.6495 Test: 0.5538\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 118, Loss: 0.8391 Val: 0.6682 Test: 0.6128\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 119, Loss: 0.7180 Val: 0.6916 Test: 0.6805\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 120, Loss: 0.7285 Val: 0.6936 Test: 0.7004\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 121, Loss: 0.7417 Val: 0.6803 Test: 0.6712\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 122, Loss: 0.6917 Val: 0.6571 Test: 0.5926\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 123, Loss: 0.7212 Val: 0.6247 Test: 0.5240\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 124, Loss: 0.6149 Val: 0.6094 Test: 0.3982\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 125, Loss: 0.5952 Val: 0.6218 Test: 0.3562\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 126, Loss: 0.4827 Val: 0.6266 Test: 0.3521\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 127, Loss: 0.3905 Val: 0.6306 Test: 0.3644\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 128, Loss: 0.4145 Val: 0.6320 Test: 0.3688\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 129, Loss: 0.3867 Val: 0.6257 Test: 0.3167\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 130, Loss: 0.3587 Val: 0.6505 Test: 0.3238\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 131, Loss: 0.5550 Val: 0.6546 Test: 0.3341\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 132, Loss: 0.5942 Val: 0.6456 Test: 0.4556\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 133, Loss: 0.4546 Val: 0.6800 Test: 0.4855\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 134, Loss: 0.3690 Val: 0.6998 Test: 0.4883\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 135, Loss: 0.4285 Val: 0.6399 Test: 0.3126\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 136, Loss: 0.5334 Val: 0.6612 Test: 0.2884\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 137, Loss: 0.6364 Val: 0.6703 Test: 0.3015\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 138, Loss: 0.6797 Val: 0.6440 Test: 0.3779\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 139, Loss: 0.6649 Val: 0.6926 Test: 0.4470\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 140, Loss: 0.6681 Val: 0.7091 Test: 0.5262\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 141, Loss: 0.6939 Val: 0.7083 Test: 0.5244\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 142, Loss: 0.6718 Val: 0.7044 Test: 0.5451\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 143, Loss: 0.5928 Val: 0.6733 Test: 0.4658\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 144, Loss: 0.5590 Val: 0.6235 Test: 0.4521\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 145, Loss: 0.6214 Val: 0.6105 Test: 0.5522\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 146, Loss: 0.7811 Val: 0.6016 Test: 0.5566\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 147, Loss: 0.6856 Val: 0.5828 Test: 0.5598\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 148, Loss: 0.6763 Val: 0.5682 Test: 0.3840\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 149, Loss: 0.6808 Val: 0.5847 Test: 0.3895\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 150, Loss: 0.6558 Val: 0.6233 Test: 0.4020\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 151, Loss: 0.6723 Val: 0.6518 Test: 0.4153\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 152, Loss: 0.5628 Val: 0.6184 Test: 0.4423\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 153, Loss: 0.6644 Val: 0.6621 Test: 0.4496\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 154, Loss: 0.5595 Val: 0.6623 Test: 0.3780\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 155, Loss: 0.5805 Val: 0.6511 Test: 0.3752\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 156, Loss: 0.5243 Val: 0.6635 Test: 0.4669\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 157, Loss: 0.4927 Val: 0.6630 Test: 0.5176\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 158, Loss: 0.4730 Val: 0.6706 Test: 0.5376\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 159, Loss: 0.4608 Val: 0.6884 Test: 0.5605\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 160, Loss: 0.4359 Val: 0.6739 Test: 0.5437\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 161, Loss: 0.4373 Val: 0.6640 Test: 0.5401\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 162, Loss: 0.3761 Val: 0.6714 Test: 0.5367\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 163, Loss: 0.3726 Val: 0.6830 Test: 0.5012\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 164, Loss: 0.4380 Val: 0.6675 Test: 0.4838\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 165, Loss: 0.4203 Val: 0.7113 Test: 0.5748\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 166, Loss: 0.3536 Val: 0.7433 Test: 0.5963\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 167, Loss: 0.4352 Val: 0.7322 Test: 0.5837\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 168, Loss: 0.4804 Val: 0.7031 Test: 0.4034\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 169, Loss: 0.5609 Val: 0.6912 Test: 0.3955\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 170, Loss: 0.3981 Val: 0.6793 Test: 0.4630\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 171, Loss: 0.4225 Val: 0.6829 Test: 0.5698\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 172, Loss: 0.6113 Val: 0.7618 Test: 0.6173\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 173, Loss: 0.6462 Val: 0.7229 Test: 0.5072\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 174, Loss: 0.7938 Val: 0.7318 Test: 0.4047\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 175, Loss: 0.7661 Val: 0.7107 Test: 0.3883\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 176, Loss: 0.6954 Val: 0.6814 Test: 0.4405\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 177, Loss: 0.6560 Val: 0.7009 Test: 0.5391\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 178, Loss: 0.5566 Val: 0.7114 Test: 0.5659\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 179, Loss: 0.5866 Val: 0.7176 Test: 0.5787\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 180, Loss: 0.5887 Val: 0.7250 Test: 0.5860\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 181, Loss: 0.5883 Val: 0.6803 Test: 0.5494\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 182, Loss: 0.5141 Val: 0.6695 Test: 0.4546\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 183, Loss: 0.5475 Val: 0.6860 Test: 0.4234\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 184, Loss: 0.6026 Val: 0.6957 Test: 0.4673\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 185, Loss: 0.5737 Val: 0.7282 Test: 0.5703\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 186, Loss: 0.5681 Val: 0.7086 Test: 0.5979\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 187, Loss: 0.4772 Val: 0.6824 Test: 0.6058\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 188, Loss: 0.5045 Val: 0.6747 Test: 0.6032\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 189, Loss: 0.3658 Val: 0.7016 Test: 0.5822\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 190, Loss: 0.4933 Val: 0.7273 Test: 0.5470\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 191, Loss: 0.4296 Val: 0.7155 Test: 0.5349\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 192, Loss: 0.4177 Val: 0.7093 Test: 0.5232\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 193, Loss: 0.5527 Val: 0.7294 Test: 0.5311\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 194, Loss: 0.3586 Val: 0.7002 Test: 0.5137\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 195, Loss: 0.4350 Val: 0.6905 Test: 0.5163\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 196, Loss: 0.4021 Val: 0.6793 Test: 0.5013\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 197, Loss: 0.4334 Val: 0.6275 Test: 0.5236\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 198, Loss: 0.6463 Val: 0.6477 Test: 0.5514\n",
      "16\n",
      "16\n",
      "10\n",
      "Epoch: 199, Loss: 0.6109 Val: 0.6650 Test: 0.5947\n",
      "ido1\n",
      "\n",
      "torch.Size([1632, 115])\n",
      "{'name': 'ido1', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe0531c40>, 'pre_filter': None, '_indices': [8, 12, 57, 9, 22, 70, 30, 74, 38, 44, 76, 49, 20, 16, 15, 5, 58, 43, 27, 4, 10, 61, 45, 73, 75, 13, 36, 3, 66, 52, 72, 14, 0, 6, 46, 42, 68, 71, 32, 55, 53, 31, 25, 63, 39, 7, 54, 37, 26, 11, 28, 24, 77, 1, 64, 62, 21, 34, 59, 29, 33, 40, 17, 35, 47, 41, 51, 19, 48, 18, 50, 2, 67, 23, 65, 56, 60, 69], 'data': Data(x=[1632, 115], edge_index=[2, 3664], edge_attr=[3664, 7], y=[78, 1], smiles=[78]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   22,   45,   68,   83,  105,  130,  158,  188,  199,  211,  230,\n",
      "         251,  271,  295,  317,  347,  370,  391,  404,  418,  436,  456,  469,\n",
      "         490,  520,  543,  566,  585,  598,  611,  629,  646,  675,  699,  712,\n",
      "         733,  763,  793,  816,  835,  847,  870,  889,  902,  923,  953,  975,\n",
      "         996, 1017, 1031, 1054, 1077, 1094, 1114, 1135, 1156, 1187, 1209, 1232,\n",
      "        1253, 1270, 1291, 1314, 1334, 1355, 1376, 1399, 1420, 1434, 1447, 1469,\n",
      "        1495, 1525, 1546, 1568, 1588, 1609, 1632]), 'edge_index': tensor([   0,   50,  102,  154,  188,  238,  294,  356,  424,  448,  474,  516,\n",
      "         562,  608,  662,  712,  780,  832,  880,  908,  938,  976, 1020, 1050,\n",
      "        1098, 1166, 1218, 1270, 1312, 1340, 1368, 1406, 1442, 1508, 1562, 1592,\n",
      "        1640, 1708, 1776, 1828, 1870, 1898, 1950, 1992, 2020, 2068, 2136, 2186,\n",
      "        2232, 2280, 2310, 2362, 2414, 2452, 2498, 2546, 2594, 2664, 2714, 2766,\n",
      "        2812, 2850, 2898, 2950, 2994, 3042, 3086, 3138, 3186, 3216, 3244, 3294,\n",
      "        3352, 3420, 3468, 3518, 3564, 3612, 3664]), 'edge_attr': tensor([   0,   50,  102,  154,  188,  238,  294,  356,  424,  448,  474,  516,\n",
      "         562,  608,  662,  712,  780,  832,  880,  908,  938,  976, 1020, 1050,\n",
      "        1098, 1166, 1218, 1270, 1312, 1340, 1368, 1406, 1442, 1508, 1562, 1592,\n",
      "        1640, 1708, 1776, 1828, 1870, 1898, 1950, 1992, 2020, 2068, 2136, 2186,\n",
      "        2232, 2280, 2310, 2362, 2414, 2452, 2498, 2546, 2594, 2664, 2714, 2766,\n",
      "        2812, 2850, 2898, 2950, 2994, 3042, 3086, 3138, 3186, 3216, 3244, 3294,\n",
      "        3352, 3420, 3468, 3518, 3564, 3612, 3664]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78])}), '_data_list': None}\n",
      "minv: 4.329999923706055\n",
      "maxv: 7.679999828338623\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 000, Loss: 1.0024 Val: 2.0770 Test: 2.0835\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 001, Loss: 1.0400 Val: 2.0485 Test: 2.0481\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 002, Loss: 1.0082 Val: 2.0219 Test: 2.0233\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 003, Loss: 0.9602 Val: 2.0077 Test: 2.0149\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 004, Loss: 0.9793 Val: 1.9766 Test: 1.9829\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 005, Loss: 0.9662 Val: 1.9539 Test: 1.9645\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 006, Loss: 0.9357 Val: 1.9312 Test: 1.9433\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 007, Loss: 0.9869 Val: 1.9081 Test: 1.9163\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 008, Loss: 0.9175 Val: 1.8796 Test: 1.8866\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 009, Loss: 0.9673 Val: 1.8538 Test: 1.8626\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 010, Loss: 0.9140 Val: 1.8386 Test: 1.8375\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 011, Loss: 0.9109 Val: 1.7995 Test: 1.8163\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 012, Loss: 0.9388 Val: 1.7817 Test: 1.7839\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 013, Loss: 0.9974 Val: 1.7504 Test: 1.7462\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 014, Loss: 0.9459 Val: 1.7274 Test: 1.7366\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 015, Loss: 0.9348 Val: 1.7231 Test: 1.7176\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 016, Loss: 0.8693 Val: 1.7122 Test: 1.7057\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 017, Loss: 0.8767 Val: 1.6875 Test: 1.6849\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 018, Loss: 0.9144 Val: 1.6531 Test: 1.6488\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 019, Loss: 0.8706 Val: 1.6141 Test: 1.6249\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 020, Loss: 0.8864 Val: 1.5920 Test: 1.5896\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 021, Loss: 0.9498 Val: 1.5700 Test: 1.5714\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 022, Loss: 0.9366 Val: 1.5497 Test: 1.5585\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 023, Loss: 0.9054 Val: 1.5281 Test: 1.5351\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 024, Loss: 0.8993 Val: 1.5045 Test: 1.5061\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 025, Loss: 0.8958 Val: 1.4957 Test: 1.4725\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 026, Loss: 0.9219 Val: 1.4632 Test: 1.4209\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 027, Loss: 0.9594 Val: 1.4361 Test: 1.4260\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 028, Loss: 0.9216 Val: 1.4207 Test: 1.4208\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 029, Loss: 0.9774 Val: 1.3843 Test: 1.3781\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 030, Loss: 0.9067 Val: 1.3334 Test: 1.3434\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 031, Loss: 0.9507 Val: 1.2950 Test: 1.2724\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 032, Loss: 0.8822 Val: 1.2789 Test: 1.2535\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 033, Loss: 0.8735 Val: 1.2715 Test: 1.2393\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 034, Loss: 0.8973 Val: 1.2596 Test: 1.1950\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 035, Loss: 0.8271 Val: 1.2308 Test: 1.1445\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 036, Loss: 0.8709 Val: 1.2111 Test: 1.1596\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 037, Loss: 0.8345 Val: 1.1963 Test: 1.1504\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 038, Loss: 0.7277 Val: 1.2144 Test: 1.1495\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 039, Loss: 0.7775 Val: 1.2338 Test: 1.1503\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 040, Loss: 0.7662 Val: 1.2306 Test: 1.1026\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 041, Loss: 0.8765 Val: 1.1769 Test: 1.0550\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 042, Loss: 0.8296 Val: 1.1496 Test: 1.0241\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 043, Loss: 0.7502 Val: 1.1386 Test: 1.0178\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 044, Loss: 0.8400 Val: 1.1288 Test: 1.0034\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 045, Loss: 0.8685 Val: 1.1351 Test: 1.0049\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 046, Loss: 0.9079 Val: 1.1714 Test: 1.0189\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 047, Loss: 0.8313 Val: 1.1723 Test: 1.0709\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 048, Loss: 0.8618 Val: 1.1903 Test: 1.0648\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 049, Loss: 0.8845 Val: 1.1356 Test: 0.9977\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 050, Loss: 0.9266 Val: 1.1041 Test: 0.9779\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 051, Loss: 0.7984 Val: 1.0823 Test: 0.9503\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 052, Loss: 0.8014 Val: 1.0864 Test: 0.9203\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 053, Loss: 0.8482 Val: 1.0891 Test: 0.9184\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 054, Loss: 0.7955 Val: 1.0792 Test: 0.9101\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 055, Loss: 0.7917 Val: 1.1250 Test: 0.9312\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 056, Loss: 0.8441 Val: 1.1347 Test: 0.9407\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 057, Loss: 0.8071 Val: 1.1435 Test: 0.9367\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 058, Loss: 0.8847 Val: 1.1506 Test: 0.9435\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 059, Loss: 0.8691 Val: 1.1446 Test: 0.9298\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 060, Loss: 0.7923 Val: 1.1431 Test: 0.9472\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 061, Loss: 0.7299 Val: 1.1406 Test: 0.9341\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 062, Loss: 0.7325 Val: 1.1289 Test: 0.8873\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 063, Loss: 0.7436 Val: 1.1251 Test: 0.8601\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 064, Loss: 0.7543 Val: 1.1054 Test: 0.8445\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 065, Loss: 0.7486 Val: 1.0967 Test: 0.8449\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 066, Loss: 0.8729 Val: 1.0865 Test: 0.8492\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 067, Loss: 0.7456 Val: 1.0835 Test: 0.8383\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 068, Loss: 0.7853 Val: 1.0759 Test: 0.8188\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 069, Loss: 0.7610 Val: 1.0406 Test: 0.8150\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 070, Loss: 0.8600 Val: 1.0742 Test: 0.8559\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 071, Loss: 0.9370 Val: 1.0386 Test: 0.8606\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 072, Loss: 0.9073 Val: 0.9858 Test: 0.8600\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 073, Loss: 0.8866 Val: 1.0130 Test: 0.8743\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 074, Loss: 0.8352 Val: 1.0866 Test: 0.8865\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 075, Loss: 0.7856 Val: 1.1084 Test: 0.8749\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 076, Loss: 0.8479 Val: 1.1136 Test: 0.8758\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 077, Loss: 0.8365 Val: 1.1147 Test: 0.8744\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 078, Loss: 0.8375 Val: 1.1221 Test: 0.8560\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 079, Loss: 0.8678 Val: 1.0246 Test: 0.8647\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 080, Loss: 0.8349 Val: 0.9804 Test: 0.8780\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 081, Loss: 0.8322 Val: 1.0236 Test: 0.8983\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 082, Loss: 0.8074 Val: 0.9919 Test: 0.8811\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 083, Loss: 0.7696 Val: 0.9340 Test: 0.8307\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 084, Loss: 0.8276 Val: 0.8916 Test: 0.7930\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 085, Loss: 0.7451 Val: 0.8838 Test: 0.8124\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 086, Loss: 0.7927 Val: 0.9462 Test: 0.8386\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 087, Loss: 0.7428 Val: 0.9580 Test: 0.7837\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 088, Loss: 0.7099 Val: 0.9767 Test: 0.8715\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 089, Loss: 0.7730 Val: 1.0270 Test: 0.8812\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 090, Loss: 0.7102 Val: 1.0217 Test: 0.8938\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 091, Loss: 0.8340 Val: 1.0068 Test: 0.8621\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 092, Loss: 0.7679 Val: 1.0491 Test: 0.8775\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 093, Loss: 0.8151 Val: 1.0408 Test: 0.8899\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 094, Loss: 0.8001 Val: 1.0178 Test: 0.9066\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 095, Loss: 0.8164 Val: 1.0032 Test: 0.9247\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 096, Loss: 0.7524 Val: 1.0260 Test: 0.8972\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 097, Loss: 0.8105 Val: 1.0931 Test: 0.9109\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 098, Loss: 0.8057 Val: 1.1512 Test: 0.9819\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 099, Loss: 0.7198 Val: 1.1413 Test: 0.9743\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 100, Loss: 0.7530 Val: 1.0947 Test: 0.9486\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 101, Loss: 0.8390 Val: 1.0441 Test: 0.9250\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 102, Loss: 0.8571 Val: 1.0387 Test: 0.9248\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 103, Loss: 0.7964 Val: 1.0817 Test: 0.8821\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 104, Loss: 0.8630 Val: 1.0427 Test: 0.9051\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 105, Loss: 0.8383 Val: 1.0141 Test: 0.8830\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 106, Loss: 0.7991 Val: 1.0349 Test: 0.8880\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 107, Loss: 0.8512 Val: 1.0345 Test: 0.8817\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 108, Loss: 0.8054 Val: 1.0392 Test: 0.8723\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 109, Loss: 0.8697 Val: 1.0233 Test: 0.8623\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 110, Loss: 0.8148 Val: 1.0239 Test: 0.8584\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 111, Loss: 0.8377 Val: 1.0164 Test: 0.8544\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 112, Loss: 0.7045 Val: 1.0145 Test: 0.8538\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 113, Loss: 0.8708 Val: 1.0068 Test: 0.8612\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 114, Loss: 0.7731 Val: 1.0613 Test: 0.8685\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 115, Loss: 0.8110 Val: 1.0485 Test: 0.8685\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 116, Loss: 0.8067 Val: 1.0465 Test: 0.8491\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 117, Loss: 0.8624 Val: 1.0473 Test: 0.8706\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 118, Loss: 0.7696 Val: 1.0361 Test: 0.8793\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 119, Loss: 0.7404 Val: 1.0374 Test: 0.8944\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 120, Loss: 0.7502 Val: 1.0508 Test: 0.9074\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 121, Loss: 0.8503 Val: 1.0666 Test: 0.9151\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 122, Loss: 0.8219 Val: 1.0527 Test: 0.9148\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 123, Loss: 0.7084 Val: 1.0350 Test: 0.8922\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 124, Loss: 0.8602 Val: 1.0316 Test: 0.8946\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 125, Loss: 0.7642 Val: 1.0390 Test: 0.8836\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 126, Loss: 0.7398 Val: 1.0318 Test: 0.8340\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 127, Loss: 0.8966 Val: 1.0249 Test: 0.8939\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 128, Loss: 0.8075 Val: 1.0267 Test: 0.8958\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 129, Loss: 0.8956 Val: 0.9425 Test: 0.9031\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 130, Loss: 0.7763 Val: 0.9212 Test: 0.9034\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 131, Loss: 0.7774 Val: 0.9137 Test: 0.8942\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 132, Loss: 0.8002 Val: 0.9623 Test: 0.8764\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 133, Loss: 0.8920 Val: 0.9837 Test: 0.8538\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 134, Loss: 0.8196 Val: 0.9803 Test: 0.8262\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 135, Loss: 0.8775 Val: 0.9309 Test: 0.8038\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 136, Loss: 0.8869 Val: 0.8846 Test: 0.7954\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 137, Loss: 0.7803 Val: 0.8662 Test: 0.7521\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 138, Loss: 0.7552 Val: 0.8973 Test: 0.7535\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 139, Loss: 0.7636 Val: 0.9108 Test: 0.7706\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 140, Loss: 0.8407 Val: 0.9342 Test: 0.7913\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 141, Loss: 0.7757 Val: 0.9435 Test: 0.7981\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 142, Loss: 0.7892 Val: 0.9491 Test: 0.8213\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 143, Loss: 0.8179 Val: 0.9781 Test: 0.8271\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 144, Loss: 0.8199 Val: 1.0085 Test: 0.8847\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 145, Loss: 0.7418 Val: 1.0188 Test: 1.0134\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 146, Loss: 0.8188 Val: 1.0208 Test: 1.0343\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 147, Loss: 0.8582 Val: 1.0244 Test: 0.9668\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 148, Loss: 0.8343 Val: 1.0476 Test: 0.8281\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 149, Loss: 0.8711 Val: 1.0174 Test: 0.8337\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 150, Loss: 0.9115 Val: 0.9017 Test: 0.9092\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 151, Loss: 0.8183 Val: 0.8734 Test: 0.8982\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 152, Loss: 0.8381 Val: 0.8308 Test: 0.9294\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 153, Loss: 0.8723 Val: 0.8180 Test: 0.8785\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 154, Loss: 0.7837 Val: 0.8273 Test: 0.8667\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 155, Loss: 0.7964 Val: 0.9456 Test: 0.8900\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 156, Loss: 0.8623 Val: 0.9870 Test: 0.9296\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 157, Loss: 0.8455 Val: 1.0010 Test: 0.9539\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 158, Loss: 0.7105 Val: 1.0006 Test: 0.9663\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 159, Loss: 0.7281 Val: 0.9918 Test: 0.9792\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 160, Loss: 0.7000 Val: 1.0417 Test: 0.9484\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 161, Loss: 0.7343 Val: 1.0518 Test: 0.9596\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 162, Loss: 0.8270 Val: 1.0558 Test: 0.9775\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 163, Loss: 0.7446 Val: 1.0446 Test: 0.9585\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 164, Loss: 0.8390 Val: 1.0222 Test: 0.9297\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 165, Loss: 0.8635 Val: 0.9846 Test: 0.9515\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 166, Loss: 0.9335 Val: 0.9631 Test: 0.9723\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 167, Loss: 0.8886 Val: 1.0087 Test: 0.9465\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 168, Loss: 0.8852 Val: 0.9934 Test: 0.9285\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 169, Loss: 0.8055 Val: 0.9705 Test: 0.9249\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 170, Loss: 0.8183 Val: 0.9821 Test: 0.9364\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 171, Loss: 0.7802 Val: 1.0114 Test: 0.9463\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 172, Loss: 0.8596 Val: 1.0315 Test: 0.9520\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 173, Loss: 0.8767 Val: 1.0647 Test: 0.9409\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 174, Loss: 0.7301 Val: 1.1068 Test: 0.9578\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 175, Loss: 0.8457 Val: 1.1022 Test: 1.0389\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 176, Loss: 0.9053 Val: 1.0923 Test: 1.0883\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 177, Loss: 0.9836 Val: 1.0689 Test: 1.0860\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 178, Loss: 0.8434 Val: 1.0254 Test: 1.0838\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 179, Loss: 0.9349 Val: 0.9807 Test: 1.0405\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 180, Loss: 0.8992 Val: 0.9466 Test: 0.9216\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 181, Loss: 0.8661 Val: 0.9508 Test: 0.8520\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 182, Loss: 0.8906 Val: 0.9561 Test: 0.8275\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 183, Loss: 0.8021 Val: 0.8339 Test: 0.8395\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 184, Loss: 0.8452 Val: 0.8405 Test: 0.8589\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 185, Loss: 0.8066 Val: 0.9629 Test: 0.8944\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 186, Loss: 0.8001 Val: 0.9685 Test: 0.9154\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 187, Loss: 0.7964 Val: 0.9610 Test: 0.9329\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 188, Loss: 0.7523 Val: 0.9285 Test: 0.9773\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 189, Loss: 0.7081 Val: 0.9062 Test: 1.0325\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 190, Loss: 0.8873 Val: 0.8959 Test: 1.0384\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 191, Loss: 0.8200 Val: 0.8575 Test: 1.0430\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 192, Loss: 0.8225 Val: 0.8452 Test: 1.0325\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 193, Loss: 0.8629 Val: 0.8619 Test: 0.9787\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 194, Loss: 0.7859 Val: 0.9759 Test: 0.9651\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 195, Loss: 0.8112 Val: 0.9913 Test: 0.9360\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 196, Loss: 0.7850 Val: 1.0006 Test: 0.9057\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 197, Loss: 0.7504 Val: 0.9320 Test: 0.9013\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 198, Loss: 0.8125 Val: 0.9230 Test: 0.8944\n",
      "16\n",
      "16\n",
      "16\n",
      "Epoch: 199, Loss: 0.8113 Val: 0.9132 Test: 0.8960\n",
      "klk5\n",
      "\n",
      "torch.Size([2118, 115])\n",
      "{'name': 'klk5', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe0538220>, 'pre_filter': None, '_indices': [48, 69, 15, 46, 52, 75, 1, 23, 37, 42, 27, 54, 11, 33, 22, 68, 65, 71, 29, 17, 36, 57, 56, 39, 76, 4, 10, 32, 40, 18, 41, 55, 12, 7, 77, 63, 30, 5, 62, 28, 26, 21, 34, 45, 60, 16, 38, 2, 6, 20, 43, 47, 64, 53, 72, 35, 70, 0, 24, 19, 44, 59, 3, 78, 9, 8, 74, 49, 31, 66, 58, 61, 25, 14, 50, 67, 13, 51, 73], 'data': Data(x=[2118, 115], edge_index=[2, 4484], edge_attr=[4484, 7], y=[79, 1], smiles=[79]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   30,   49,   77,   99,  128,  158,  177,  200,  230,  260,  281,\n",
      "         310,  333,  362,  392,  423,  450,  481,  511,  541,  571,  594,  618,\n",
      "         640,  670,  700,  720,  751,  782,  812,  833,  863,  894,  915,  937,\n",
      "         964,  986, 1007, 1035, 1068, 1085, 1118, 1140, 1162, 1192, 1223, 1245,\n",
      "        1267, 1298, 1329, 1362, 1392, 1425, 1445, 1470, 1492, 1513, 1535, 1564,\n",
      "        1594, 1621, 1644, 1669, 1699, 1716, 1746, 1777, 1806, 1839, 1861, 1891,\n",
      "        1914, 1945, 1975, 2006, 2027, 2060, 2088, 2118]), 'edge_index': tensor([   0,   62,  102,  162,  210,  272,  336,  376,  424,  488,  552,  596,\n",
      "         658,  704,  768,  832,  900,  956, 1020, 1084, 1148, 1212, 1258, 1308,\n",
      "        1356, 1422, 1484, 1528, 1594, 1660, 1724, 1768, 1832, 1898, 1942, 1986,\n",
      "        2040, 2086, 2130, 2192, 2262, 2296, 2366, 2412, 2458, 2522, 2588, 2636,\n",
      "        2682, 2750, 2818, 2888, 2952, 3022, 3064, 3114, 3162, 3208, 3254, 3316,\n",
      "        3380, 3434, 3480, 3530, 3596, 3630, 3694, 3760, 3822, 3892, 3938, 4002,\n",
      "        4048, 4114, 4178, 4246, 4290, 4360, 4418, 4484]), 'edge_attr': tensor([   0,   62,  102,  162,  210,  272,  336,  376,  424,  488,  552,  596,\n",
      "         658,  704,  768,  832,  900,  956, 1020, 1084, 1148, 1212, 1258, 1308,\n",
      "        1356, 1422, 1484, 1528, 1594, 1660, 1724, 1768, 1832, 1898, 1942, 1986,\n",
      "        2040, 2086, 2130, 2192, 2262, 2296, 2366, 2412, 2458, 2522, 2588, 2636,\n",
      "        2682, 2750, 2818, 2888, 2952, 3022, 3064, 3114, 3162, 3208, 3254, 3316,\n",
      "        3380, 3434, 3480, 3530, 3596, 3630, 3694, 3760, 3822, 3892, 3938, 4002,\n",
      "        4048, 4114, 4178, 4246, 4290, 4360, 4418, 4484]), 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79]), 'smiles': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79])}), '_data_list': None}\n",
      "minv: 4.300000190734863\n",
      "maxv: 9.300000190734863\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 000, Loss: 1.0078 Val: 1.7322 Test: 2.4406\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 001, Loss: 0.9915 Val: 1.6857 Test: 2.4013\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 002, Loss: 0.8878 Val: 1.6427 Test: 2.3435\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 003, Loss: 0.9505 Val: 1.6190 Test: 2.3054\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 004, Loss: 0.9053 Val: 1.5885 Test: 2.2772\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 005, Loss: 0.8062 Val: 1.5553 Test: 2.2395\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 006, Loss: 0.8579 Val: 1.5118 Test: 2.1967\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 007, Loss: 0.8711 Val: 1.4781 Test: 2.1572\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 008, Loss: 0.8933 Val: 1.4415 Test: 2.1130\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 009, Loss: 0.8264 Val: 1.4201 Test: 2.0684\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 010, Loss: 0.8913 Val: 1.3870 Test: 2.0357\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 011, Loss: 0.9569 Val: 1.3670 Test: 1.9897\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 012, Loss: 0.8223 Val: 1.3063 Test: 1.9342\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 013, Loss: 0.8889 Val: 1.2765 Test: 1.8897\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 014, Loss: 0.8912 Val: 1.2390 Test: 1.8265\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 015, Loss: 0.7356 Val: 1.2345 Test: 1.8055\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 016, Loss: 0.7599 Val: 1.2243 Test: 1.7670\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 017, Loss: 0.7557 Val: 1.2175 Test: 1.7189\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 018, Loss: 0.8091 Val: 1.2891 Test: 1.6843\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 019, Loss: 0.8500 Val: 1.3095 Test: 1.6583\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 020, Loss: 0.8598 Val: 1.1913 Test: 1.6174\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 021, Loss: 0.8195 Val: 1.1203 Test: 1.6172\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 022, Loss: 0.7292 Val: 1.0926 Test: 1.5367\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 023, Loss: 0.7875 Val: 1.0628 Test: 1.5244\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 024, Loss: 0.7696 Val: 1.0509 Test: 1.5093\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 025, Loss: 0.8307 Val: 1.0461 Test: 1.4638\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 026, Loss: 0.7711 Val: 1.0210 Test: 1.4270\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 027, Loss: 0.6039 Val: 0.9623 Test: 1.4016\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 028, Loss: 0.6544 Val: 0.9338 Test: 1.3543\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 029, Loss: 0.5591 Val: 0.8893 Test: 1.3019\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 030, Loss: 0.7788 Val: 0.9406 Test: 1.2281\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 031, Loss: 0.6864 Val: 0.9525 Test: 1.2763\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 032, Loss: 0.7123 Val: 0.7513 Test: 1.0968\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 033, Loss: 0.6537 Val: 0.7575 Test: 1.0972\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 034, Loss: 0.7367 Val: 0.7788 Test: 1.0050\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 035, Loss: 0.7427 Val: 0.8338 Test: 0.9943\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 036, Loss: 0.7235 Val: 0.7473 Test: 0.9385\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 037, Loss: 0.7885 Val: 0.7393 Test: 0.9290\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 038, Loss: 0.7236 Val: 0.9341 Test: 0.9177\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 039, Loss: 0.6534 Val: 0.7235 Test: 0.8987\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 040, Loss: 0.6488 Val: 0.7041 Test: 0.8835\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 041, Loss: 0.6770 Val: 0.6868 Test: 0.8902\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 042, Loss: 0.6786 Val: 0.6846 Test: 0.8405\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 043, Loss: 0.6321 Val: 0.6899 Test: 0.8631\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 044, Loss: 0.7505 Val: 0.6755 Test: 0.8218\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 045, Loss: 0.6516 Val: 0.6707 Test: 0.8545\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 046, Loss: 0.6724 Val: 0.6332 Test: 0.8364\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 047, Loss: 0.6050 Val: 0.8399 Test: 0.8360\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 048, Loss: 0.8099 Val: 0.8681 Test: 0.8713\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 049, Loss: 0.7409 Val: 0.5898 Test: 0.8818\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 050, Loss: 0.6204 Val: 0.6197 Test: 0.8924\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 051, Loss: 0.5439 Val: 0.6082 Test: 0.8712\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 052, Loss: 0.5928 Val: 0.6401 Test: 0.8564\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 053, Loss: 0.7818 Val: 0.6920 Test: 0.8598\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 054, Loss: 0.6177 Val: 0.5999 Test: 0.8409\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 055, Loss: 0.6544 Val: 0.5799 Test: 0.8087\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 056, Loss: 0.5811 Val: 0.6724 Test: 0.7535\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 057, Loss: 0.6181 Val: 0.6630 Test: 0.7414\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 058, Loss: 0.6066 Val: 0.6171 Test: 0.7123\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 059, Loss: 0.6399 Val: 0.5962 Test: 0.7299\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 060, Loss: 0.6124 Val: 0.5955 Test: 0.7630\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 061, Loss: 0.5356 Val: 0.7271 Test: 0.7704\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 062, Loss: 0.4758 Val: 0.8738 Test: 0.7722\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 063, Loss: 0.6493 Val: 0.9718 Test: 0.7515\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 064, Loss: 0.6195 Val: 0.7957 Test: 0.7433\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 065, Loss: 0.7611 Val: 0.5733 Test: 0.7650\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 066, Loss: 0.7603 Val: 0.5684 Test: 0.7620\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 067, Loss: 0.6213 Val: 0.5684 Test: 0.7563\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 068, Loss: 0.6963 Val: 0.6036 Test: 0.8080\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 069, Loss: 0.6231 Val: 0.7728 Test: 0.7198\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 070, Loss: 0.5703 Val: 0.6769 Test: 0.7025\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 071, Loss: 0.5779 Val: 0.6594 Test: 0.6946\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 072, Loss: 0.5570 Val: 0.6722 Test: 0.6688\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 073, Loss: 0.6097 Val: 0.6533 Test: 0.6741\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 074, Loss: 0.6429 Val: 0.6062 Test: 0.7677\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 075, Loss: 0.5785 Val: 0.7182 Test: 0.7818\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 076, Loss: 0.3983 Val: 0.8262 Test: 0.8000\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 077, Loss: 0.5339 Val: 0.8973 Test: 0.8061\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 078, Loss: 0.4932 Val: 0.9256 Test: 0.8038\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 079, Loss: 0.5033 Val: 0.8897 Test: 0.8702\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 080, Loss: 0.6172 Val: 0.8441 Test: 0.7364\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 081, Loss: 0.6317 Val: 1.1005 Test: 0.8239\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 082, Loss: 0.7354 Val: 1.3157 Test: 1.0952\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 083, Loss: 0.9080 Val: 0.8638 Test: 1.0169\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 084, Loss: 0.7955 Val: 0.6719 Test: 0.9810\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 085, Loss: 0.7515 Val: 0.6178 Test: 0.9852\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 086, Loss: 0.7356 Val: 0.6027 Test: 0.9951\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 087, Loss: 0.6008 Val: 0.5919 Test: 1.0103\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 088, Loss: 0.6386 Val: 0.7800 Test: 0.8588\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 089, Loss: 0.6661 Val: 0.8875 Test: 0.7612\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 090, Loss: 0.7336 Val: 0.6650 Test: 0.7678\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 091, Loss: 0.7956 Val: 0.6692 Test: 0.8005\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 092, Loss: 0.6742 Val: 0.6949 Test: 0.8146\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 093, Loss: 0.6006 Val: 0.6753 Test: 0.8369\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 094, Loss: 0.8241 Val: 0.6695 Test: 0.8469\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 095, Loss: 0.5689 Val: 0.6808 Test: 0.8660\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 096, Loss: 0.6344 Val: 0.6314 Test: 0.8556\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 097, Loss: 0.6350 Val: 0.7554 Test: 0.8481\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 098, Loss: 0.6947 Val: 0.7307 Test: 0.9040\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 099, Loss: 0.6532 Val: 0.8092 Test: 0.9372\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 100, Loss: 0.7637 Val: 0.8169 Test: 0.9025\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 101, Loss: 0.6619 Val: 0.9250 Test: 0.8681\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 102, Loss: 0.5239 Val: 1.0412 Test: 0.8714\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 103, Loss: 0.5900 Val: 1.0522 Test: 0.8524\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 104, Loss: 0.5391 Val: 0.8506 Test: 0.8371\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 105, Loss: 0.4859 Val: 0.6943 Test: 0.8022\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 106, Loss: 0.5598 Val: 0.6800 Test: 0.7867\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 107, Loss: 0.5404 Val: 0.6895 Test: 0.8382\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 108, Loss: 0.6039 Val: 0.8838 Test: 0.8381\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 109, Loss: 0.7576 Val: 1.1317 Test: 0.7641\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 110, Loss: 0.7707 Val: 0.9994 Test: 0.7797\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 111, Loss: 0.6419 Val: 0.7426 Test: 0.8217\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 112, Loss: 0.6348 Val: 0.6120 Test: 0.9758\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 113, Loss: 0.8188 Val: 0.5624 Test: 1.0318\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 114, Loss: 0.7405 Val: 0.9265 Test: 0.9210\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 115, Loss: 0.6210 Val: 1.1343 Test: 0.7929\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 116, Loss: 0.6691 Val: 0.8769 Test: 0.7718\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 117, Loss: 0.7588 Val: 0.7859 Test: 0.7773\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 118, Loss: 0.5253 Val: 0.8598 Test: 0.8176\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 119, Loss: 0.6042 Val: 0.8362 Test: 0.8284\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 120, Loss: 0.6149 Val: 0.8615 Test: 0.8170\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 121, Loss: 0.7020 Val: 0.8699 Test: 0.9653\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 122, Loss: 0.7854 Val: 0.9001 Test: 0.7487\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 123, Loss: 0.8393 Val: 0.9799 Test: 0.8820\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 124, Loss: 0.6087 Val: 0.9879 Test: 0.8970\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 125, Loss: 0.7107 Val: 0.8387 Test: 0.8745\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 126, Loss: 0.6150 Val: 0.8272 Test: 0.8604\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 127, Loss: 0.5260 Val: 0.8134 Test: 0.8273\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 128, Loss: 0.6220 Val: 0.7919 Test: 0.8041\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 129, Loss: 0.6797 Val: 0.7655 Test: 0.7967\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 130, Loss: 0.6526 Val: 0.8775 Test: 0.7848\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 131, Loss: 0.5009 Val: 0.7342 Test: 0.7520\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 132, Loss: 0.6640 Val: 0.8713 Test: 0.7651\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 133, Loss: 0.7037 Val: 0.9660 Test: 0.7562\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 134, Loss: 0.5941 Val: 0.9565 Test: 0.7890\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 135, Loss: 0.7069 Val: 0.9351 Test: 0.7826\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 136, Loss: 0.6547 Val: 0.9260 Test: 0.7763\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 137, Loss: 0.7269 Val: 0.8700 Test: 0.7591\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 138, Loss: 0.5506 Val: 0.8120 Test: 0.7721\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 139, Loss: 0.5616 Val: 0.8329 Test: 0.7749\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 140, Loss: 0.5221 Val: 0.8032 Test: 0.7306\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 141, Loss: 0.6036 Val: 0.9706 Test: 0.7936\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 142, Loss: 0.5967 Val: 0.7483 Test: 0.7033\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 143, Loss: 0.4860 Val: 0.7150 Test: 0.7181\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 144, Loss: 0.6922 Val: 0.6908 Test: 0.7402\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 145, Loss: 0.5264 Val: 0.7335 Test: 0.7740\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 146, Loss: 0.6275 Val: 0.7000 Test: 0.7962\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 147, Loss: 0.5750 Val: 0.6691 Test: 0.7905\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 148, Loss: 0.5839 Val: 0.6738 Test: 0.7850\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 149, Loss: 0.4627 Val: 0.6948 Test: 0.7848\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 150, Loss: 0.4997 Val: 0.7110 Test: 0.7596\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 151, Loss: 0.5974 Val: 0.7101 Test: 0.7342\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 152, Loss: 0.5106 Val: 0.6933 Test: 0.7227\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 153, Loss: 0.4846 Val: 0.6405 Test: 0.7498\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 154, Loss: 0.5016 Val: 0.6250 Test: 0.7811\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 155, Loss: 0.6561 Val: 0.6468 Test: 0.7771\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 156, Loss: 0.6634 Val: 0.6451 Test: 0.8090\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 157, Loss: 0.4157 Val: 0.6047 Test: 0.7961\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 158, Loss: 0.5313 Val: 0.5982 Test: 0.8154\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 159, Loss: 0.6379 Val: 0.5855 Test: 0.7899\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 160, Loss: 0.6329 Val: 0.6357 Test: 0.7618\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 161, Loss: 0.5518 Val: 0.6608 Test: 0.8025\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 162, Loss: 0.5844 Val: 0.6559 Test: 0.7509\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 163, Loss: 0.5897 Val: 0.6379 Test: 0.7384\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 164, Loss: 0.5501 Val: 0.6303 Test: 0.7321\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 165, Loss: 0.5667 Val: 0.6212 Test: 0.7332\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 166, Loss: 0.5940 Val: 0.6143 Test: 0.7441\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 167, Loss: 0.4713 Val: 0.6172 Test: 0.7466\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 168, Loss: 0.5105 Val: 0.6516 Test: 0.7332\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 169, Loss: 0.5637 Val: 0.6734 Test: 0.7109\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 170, Loss: 0.4417 Val: 0.6605 Test: 0.7037\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 171, Loss: 0.6084 Val: 0.6709 Test: 0.7159\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 172, Loss: 0.5063 Val: 0.6307 Test: 0.7302\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 173, Loss: 0.6416 Val: 0.6150 Test: 0.7688\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 174, Loss: 0.6015 Val: 0.6083 Test: 0.7933\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 175, Loss: 0.5482 Val: 0.6007 Test: 0.7695\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 176, Loss: 0.5586 Val: 0.6327 Test: 0.7537\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 177, Loss: 0.6093 Val: 0.6077 Test: 0.7921\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 178, Loss: 0.5523 Val: 0.6261 Test: 0.8194\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 179, Loss: 0.6683 Val: 0.6232 Test: 0.7283\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 180, Loss: 0.6455 Val: 0.5840 Test: 0.7251\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 181, Loss: 0.5306 Val: 0.5817 Test: 0.8155\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 182, Loss: 0.5924 Val: 0.5804 Test: 0.8405\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 183, Loss: 0.5325 Val: 0.5879 Test: 0.8607\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 184, Loss: 0.7383 Val: 0.6125 Test: 0.8566\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 185, Loss: 0.7517 Val: 0.6260 Test: 0.8332\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 186, Loss: 0.5454 Val: 0.6322 Test: 0.8360\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 187, Loss: 0.5393 Val: 0.6001 Test: 0.8486\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 188, Loss: 0.5932 Val: 0.5958 Test: 0.8387\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 189, Loss: 0.5355 Val: 0.5735 Test: 0.8228\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 190, Loss: 0.5599 Val: 0.6458 Test: 0.8027\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 191, Loss: 0.5233 Val: 0.6640 Test: 0.7193\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 192, Loss: 0.6976 Val: 0.6351 Test: 0.7283\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 193, Loss: 0.5384 Val: 0.5846 Test: 0.7608\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 194, Loss: 0.5636 Val: 0.5527 Test: 0.7713\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 195, Loss: 0.5902 Val: 0.5611 Test: 0.7702\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 196, Loss: 0.5053 Val: 0.5793 Test: 0.7770\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 197, Loss: 0.4843 Val: 0.6108 Test: 0.7683\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 198, Loss: 0.4768 Val: 0.6305 Test: 0.7498\n",
      "16\n",
      "16\n",
      "16\n",
      "1\n",
      "Epoch: 199, Loss: 0.5837 Val: 0.6129 Test: 0.8322\n",
      "notum\n",
      "\n",
      "torch.Size([2479, 115])\n",
      "{'name': 'notum', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe03770a0>, 'pre_filter': None, '_indices': [64, 102, 125, 49, 74, 17, 24, 9, 20, 62, 73, 117, 33, 1, 113, 108, 21, 76, 68, 16, 61, 70, 84, 89, 54, 96, 81, 119, 10, 32, 86, 22, 60, 95, 28, 107, 4, 109, 43, 39, 85, 110, 93, 118, 115, 92, 101, 116, 99, 72, 2, 25, 0, 120, 6, 67, 87, 3, 77, 105, 112, 30, 90, 78, 45, 27, 29, 55, 114, 14, 26, 44, 52, 71, 41, 47, 48, 126, 111, 57, 121, 38, 100, 15, 65, 122, 23, 11, 56, 18, 97, 82, 79, 50, 42, 31, 37, 5, 19, 51, 103, 13, 94, 58, 124, 66, 36, 98, 123, 53, 12, 8, 35, 34, 88, 104, 83, 75, 46, 7, 127, 59, 80, 91, 63, 40, 106, 69], 'data': Data(x=[2479, 115], edge_index=[2, 5338], edge_attr=[5338, 7], y=[128, 1], smiles=[128]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   22,   47,   62,   79,   96,  113,  132,  153,  171,  185,  198,\n",
      "         217,  235,  254,  269,  284,  308,  323,  339,  357,  373,  397,  420,\n",
      "         435,  444,  461,  477,  487,  506,  524,  539,  558,  577,  602,  619,\n",
      "         637,  655,  677,  700,  721,  738,  757,  773,  789,  808,  832,  849,\n",
      "         864,  883,  906,  927,  946,  966,  981, 1006, 1027, 1046, 1064, 1087,\n",
      "        1106, 1120, 1138, 1154, 1171, 1191, 1209, 1227, 1242, 1264, 1285, 1310,\n",
      "        1334, 1361, 1386, 1402, 1428, 1454, 1471, 1487, 1503, 1522, 1543, 1563,\n",
      "        1587, 1609, 1633, 1658, 1679, 1702, 1725, 1749, 1777, 1793, 1812, 1830,\n",
      "        1847, 1864, 1884, 1903, 1921, 1940, 1966, 1985, 2010, 2032, 2057, 2073,\n",
      "        2086, 2106, 2129, 2144, 2165, 2188, 2212, 2238, 2256, 2275, 2284, 2301,\n",
      "        2318, 2338, 2354, 2370, 2384, 2408, 2436, 2462, 2479]), 'edge_index': tensor([   0,   48,  102,  134,  170,  206,  242,  284,  328,  366,  396,  422,\n",
      "         462,  500,  540,  572,  604,  656,  688,  722,  760,  794,  846,  896,\n",
      "         928,  946,  982, 1016, 1036, 1076, 1114, 1146, 1186, 1226, 1280, 1316,\n",
      "        1354, 1392, 1440, 1492, 1538, 1574, 1614, 1648, 1682, 1722, 1776, 1812,\n",
      "        1844, 1884, 1934, 1980, 2020, 2064, 2096, 2150, 2194, 2234, 2272, 2322,\n",
      "        2362, 2392, 2432, 2466, 2502, 2544, 2582, 2620, 2652, 2700, 2746, 2802,\n",
      "        2856, 2918, 2974, 3008, 3068, 3126, 3162, 3196, 3230, 3270, 3314, 3358,\n",
      "        3412, 3460, 3514, 3570, 3616, 3668, 3718, 3770, 3832, 3866, 3908, 3946,\n",
      "        3982, 4018, 4060, 4100, 4138, 4178, 4234, 4274, 4328, 4376, 4430, 4464,\n",
      "        4492, 4534, 4586, 4618, 4664, 4712, 4764, 4822, 4860, 4900, 4918, 4954,\n",
      "        4990, 5032, 5066, 5100, 5130, 5184, 5246, 5302, 5338]), 'edge_attr': tensor([   0,   48,  102,  134,  170,  206,  242,  284,  328,  366,  396,  422,\n",
      "         462,  500,  540,  572,  604,  656,  688,  722,  760,  794,  846,  896,\n",
      "         928,  946,  982, 1016, 1036, 1076, 1114, 1146, 1186, 1226, 1280, 1316,\n",
      "        1354, 1392, 1440, 1492, 1538, 1574, 1614, 1648, 1682, 1722, 1776, 1812,\n",
      "        1844, 1884, 1934, 1980, 2020, 2064, 2096, 2150, 2194, 2234, 2272, 2322,\n",
      "        2362, 2392, 2432, 2466, 2502, 2544, 2582, 2620, 2652, 2700, 2746, 2802,\n",
      "        2856, 2918, 2974, 3008, 3068, 3126, 3162, 3196, 3230, 3270, 3314, 3358,\n",
      "        3412, 3460, 3514, 3570, 3616, 3668, 3718, 3770, 3832, 3866, 3908, 3946,\n",
      "        3982, 4018, 4060, 4100, 4138, 4178, 4234, 4274, 4328, 4376, 4430, 4464,\n",
      "        4492, 4534, 4586, 4618, 4664, 4712, 4764, 4822, 4860, 4900, 4918, 4954,\n",
      "        4990, 5032, 5066, 5100, 5130, 5184, 5246, 5302, 5338]), 'y': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128]), 'smiles': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128])}), '_data_list': None}\n",
      "minv: 4.0\n",
      "maxv: 9.0\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 000, Loss: 0.9716 Val: 3.1593 Test: 2.9793\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 001, Loss: 0.9044 Val: 3.0856 Test: 2.9134\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 002, Loss: 0.9116 Val: 3.0288 Test: 2.8498\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 003, Loss: 0.9239 Val: 2.9597 Test: 2.7730\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 004, Loss: 0.8883 Val: 2.8917 Test: 2.7124\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 005, Loss: 0.8371 Val: 2.8591 Test: 2.6604\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 006, Loss: 0.9108 Val: 2.7912 Test: 2.5932\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 007, Loss: 0.8978 Val: 2.6964 Test: 2.5125\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 008, Loss: 0.8419 Val: 2.6424 Test: 2.4315\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 009, Loss: 0.8217 Val: 2.6265 Test: 2.3980\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 010, Loss: 0.7734 Val: 2.4978 Test: 2.2881\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 011, Loss: 0.7648 Val: 2.4116 Test: 2.1961\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 012, Loss: 0.7557 Val: 2.3964 Test: 2.1829\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 013, Loss: 0.8424 Val: 2.2573 Test: 2.0447\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 014, Loss: 0.7752 Val: 2.1777 Test: 1.9157\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 015, Loss: 0.7856 Val: 2.2483 Test: 2.0084\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 016, Loss: 0.8783 Val: 2.2460 Test: 1.9944\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 017, Loss: 0.7587 Val: 2.0697 Test: 1.8157\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 018, Loss: 0.8295 Val: 1.9000 Test: 1.6583\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 019, Loss: 0.7753 Val: 1.9267 Test: 1.6981\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 020, Loss: 0.8159 Val: 1.8042 Test: 1.5848\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 021, Loss: 0.7733 Val: 1.6592 Test: 1.4445\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 022, Loss: 0.7403 Val: 1.5247 Test: 1.2931\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 023, Loss: 0.7522 Val: 1.4532 Test: 1.1825\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 024, Loss: 0.7442 Val: 1.4291 Test: 1.1750\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 025, Loss: 0.7512 Val: 1.3307 Test: 1.1037\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 026, Loss: 0.7014 Val: 1.2326 Test: 1.0146\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 027, Loss: 0.6374 Val: 1.1547 Test: 0.9626\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 028, Loss: 0.7023 Val: 1.0697 Test: 0.8672\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 029, Loss: 0.6945 Val: 1.0253 Test: 0.7843\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 030, Loss: 0.6774 Val: 0.9678 Test: 0.6921\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 031, Loss: 0.6375 Val: 0.9301 Test: 0.6771\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 032, Loss: 0.6448 Val: 0.9003 Test: 0.6486\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 033, Loss: 0.6350 Val: 0.9158 Test: 0.6849\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 034, Loss: 0.7290 Val: 0.9539 Test: 0.6883\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 035, Loss: 0.6400 Val: 0.9879 Test: 0.6265\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 036, Loss: 0.6007 Val: 1.0047 Test: 0.6232\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 037, Loss: 0.6297 Val: 0.9736 Test: 0.5973\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 038, Loss: 0.7269 Val: 0.9639 Test: 0.5973\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 039, Loss: 0.6336 Val: 0.9066 Test: 0.6033\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 040, Loss: 0.5761 Val: 0.9638 Test: 0.5732\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 041, Loss: 0.6642 Val: 0.9729 Test: 0.6553\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 042, Loss: 0.6591 Val: 0.9652 Test: 0.6187\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 043, Loss: 0.6410 Val: 1.0009 Test: 0.7011\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 044, Loss: 0.6700 Val: 0.8661 Test: 0.7230\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 045, Loss: 0.7621 Val: 0.8309 Test: 0.9755\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 046, Loss: 0.7231 Val: 0.9121 Test: 0.7223\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 047, Loss: 0.7029 Val: 0.8822 Test: 0.7786\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 048, Loss: 0.6388 Val: 0.8294 Test: 0.7052\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 049, Loss: 0.6735 Val: 0.8563 Test: 0.6624\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 050, Loss: 0.6649 Val: 0.8273 Test: 0.6907\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 051, Loss: 0.6131 Val: 0.8529 Test: 0.7424\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 052, Loss: 0.6330 Val: 0.8463 Test: 0.6355\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 053, Loss: 0.6070 Val: 0.8403 Test: 0.6584\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 054, Loss: 0.6435 Val: 0.8978 Test: 0.6984\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 055, Loss: 0.6804 Val: 0.8357 Test: 0.6585\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 056, Loss: 0.6548 Val: 0.8267 Test: 0.6120\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 057, Loss: 0.7459 Val: 0.8672 Test: 0.6066\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 058, Loss: 0.5763 Val: 0.8767 Test: 0.5789\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 059, Loss: 0.6663 Val: 0.9019 Test: 0.5749\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 060, Loss: 0.6517 Val: 0.9538 Test: 0.5917\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 061, Loss: 0.6832 Val: 0.9766 Test: 0.6277\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 062, Loss: 0.6448 Val: 0.9447 Test: 0.6388\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 063, Loss: 0.6210 Val: 0.8687 Test: 0.6258\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 064, Loss: 0.6159 Val: 0.8413 Test: 0.6160\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 065, Loss: 0.5675 Val: 0.8222 Test: 0.5871\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 066, Loss: 0.5657 Val: 0.7878 Test: 0.5744\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 067, Loss: 0.5807 Val: 0.7657 Test: 0.5689\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 068, Loss: 0.5805 Val: 0.8129 Test: 0.5914\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 069, Loss: 0.6165 Val: 0.8148 Test: 0.6008\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 070, Loss: 0.5674 Val: 0.7775 Test: 0.5924\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 071, Loss: 0.5965 Val: 0.7892 Test: 0.5601\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 072, Loss: 0.5355 Val: 0.9693 Test: 0.5895\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 073, Loss: 0.5807 Val: 0.9257 Test: 0.6210\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 074, Loss: 0.6089 Val: 0.7341 Test: 0.5638\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 075, Loss: 0.5852 Val: 0.7787 Test: 0.5978\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 076, Loss: 0.5398 Val: 0.8432 Test: 0.5834\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 077, Loss: 0.5924 Val: 0.8746 Test: 0.5609\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 078, Loss: 0.5876 Val: 0.8601 Test: 0.5874\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 079, Loss: 0.6244 Val: 0.8457 Test: 0.6013\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 080, Loss: 0.5155 Val: 0.8347 Test: 0.6161\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 081, Loss: 0.5635 Val: 0.8258 Test: 0.5775\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 082, Loss: 0.5852 Val: 0.9965 Test: 0.6491\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 083, Loss: 0.5855 Val: 1.0455 Test: 0.6471\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 084, Loss: 0.6049 Val: 0.9900 Test: 0.5651\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 085, Loss: 0.5066 Val: 0.9255 Test: 0.6042\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 086, Loss: 0.6042 Val: 0.9807 Test: 0.6378\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 087, Loss: 0.6349 Val: 1.0517 Test: 0.6103\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 088, Loss: 0.6826 Val: 0.8903 Test: 0.6061\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 089, Loss: 0.7163 Val: 0.7955 Test: 0.6596\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 090, Loss: 0.7283 Val: 0.6925 Test: 0.9739\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 091, Loss: 0.8404 Val: 0.7054 Test: 0.9475\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 092, Loss: 0.7909 Val: 0.7267 Test: 0.6785\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 093, Loss: 0.7224 Val: 0.7688 Test: 0.6647\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 094, Loss: 0.7332 Val: 0.7930 Test: 0.6787\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 095, Loss: 0.6315 Val: 0.7892 Test: 0.6951\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 096, Loss: 0.5770 Val: 0.7781 Test: 0.6991\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 097, Loss: 0.5134 Val: 0.8068 Test: 0.6777\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 098, Loss: 0.6512 Val: 0.7939 Test: 0.6737\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 099, Loss: 0.6276 Val: 0.7261 Test: 0.6448\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 100, Loss: 0.5285 Val: 0.7207 Test: 0.6135\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 101, Loss: 0.5551 Val: 0.7847 Test: 0.6120\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 102, Loss: 0.6079 Val: 0.8440 Test: 0.5944\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 103, Loss: 0.6811 Val: 0.7720 Test: 0.5732\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 104, Loss: 0.7002 Val: 0.7433 Test: 0.5999\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 105, Loss: 0.6676 Val: 0.7520 Test: 0.6058\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 106, Loss: 0.5980 Val: 0.8403 Test: 0.6432\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 107, Loss: 0.5616 Val: 0.8492 Test: 0.6442\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 108, Loss: 0.6164 Val: 0.7892 Test: 0.6419\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 109, Loss: 0.6232 Val: 0.8036 Test: 0.6663\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 110, Loss: 0.5772 Val: 0.8437 Test: 0.6852\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 111, Loss: 0.5885 Val: 0.8583 Test: 0.7179\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 112, Loss: 0.6179 Val: 0.8698 Test: 0.6758\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 113, Loss: 0.6831 Val: 0.8733 Test: 0.6555\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 114, Loss: 0.6008 Val: 0.8518 Test: 0.5832\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 115, Loss: 0.6140 Val: 0.7569 Test: 1.2348\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 116, Loss: 0.8579 Val: 0.7569 Test: 0.5326\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 117, Loss: 0.6551 Val: 0.7955 Test: 0.5905\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 118, Loss: 0.7666 Val: 0.8221 Test: 0.6283\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 119, Loss: 0.6576 Val: 0.8423 Test: 0.6083\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 120, Loss: 0.7071 Val: 0.8209 Test: 0.5677\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 121, Loss: 0.6529 Val: 0.8880 Test: 0.6267\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 122, Loss: 0.6297 Val: 1.2090 Test: 0.7969\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 123, Loss: 0.6953 Val: 1.0924 Test: 0.8066\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 124, Loss: 0.6387 Val: 0.9150 Test: 0.5958\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 125, Loss: 0.5952 Val: 0.7555 Test: 0.5723\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 126, Loss: 0.6413 Val: 0.7440 Test: 0.6325\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 127, Loss: 0.6373 Val: 0.8113 Test: 0.6700\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 128, Loss: 0.5889 Val: 0.8876 Test: 0.6892\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 129, Loss: 0.5182 Val: 0.9174 Test: 0.6482\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 130, Loss: 0.6434 Val: 0.9743 Test: 0.6162\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 131, Loss: 0.6373 Val: 0.7537 Test: 0.5529\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 132, Loss: 0.6311 Val: 0.7757 Test: 0.5676\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 133, Loss: 0.6067 Val: 0.8443 Test: 0.5955\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 134, Loss: 0.5836 Val: 0.8527 Test: 0.5870\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 135, Loss: 0.5314 Val: 0.8587 Test: 0.5943\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 136, Loss: 0.5668 Val: 0.8794 Test: 0.6128\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 137, Loss: 0.5614 Val: 0.9229 Test: 0.6188\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 138, Loss: 0.5650 Val: 0.9216 Test: 0.6501\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 139, Loss: 0.5583 Val: 0.8697 Test: 0.6496\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 140, Loss: 0.6276 Val: 0.8088 Test: 0.6011\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 141, Loss: 0.5801 Val: 0.8122 Test: 0.5872\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 142, Loss: 0.4881 Val: 0.8360 Test: 0.6156\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 143, Loss: 0.6305 Val: 0.8499 Test: 0.6346\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 144, Loss: 0.5472 Val: 0.8833 Test: 0.6211\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 145, Loss: 0.5557 Val: 0.9154 Test: 0.6508\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 146, Loss: 0.4967 Val: 0.9111 Test: 0.6701\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 147, Loss: 0.5671 Val: 0.8977 Test: 0.6678\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 148, Loss: 0.4779 Val: 0.8983 Test: 0.6792\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 149, Loss: 0.5155 Val: 0.9618 Test: 0.6666\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 150, Loss: 0.4847 Val: 0.9412 Test: 0.6510\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 151, Loss: 0.5298 Val: 0.8514 Test: 0.7056\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 152, Loss: 0.6459 Val: 0.8431 Test: 0.6877\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 153, Loss: 0.5448 Val: 0.8120 Test: 0.6233\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 154, Loss: 0.5465 Val: 0.7894 Test: 0.5756\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 155, Loss: 0.4875 Val: 0.8521 Test: 0.5763\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 156, Loss: 0.4955 Val: 0.8737 Test: 0.5328\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 157, Loss: 0.5170 Val: 0.8594 Test: 0.5102\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 158, Loss: 0.4947 Val: 0.8473 Test: 0.5124\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 159, Loss: 0.5209 Val: 0.8742 Test: 0.5831\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 160, Loss: 0.5425 Val: 0.9118 Test: 0.6699\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 161, Loss: 0.5290 Val: 0.9262 Test: 0.6648\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 162, Loss: 0.5381 Val: 0.8983 Test: 0.6393\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 163, Loss: 0.5287 Val: 0.8774 Test: 0.6099\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 164, Loss: 0.4946 Val: 0.8754 Test: 0.5919\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 165, Loss: 0.5204 Val: 0.8917 Test: 0.5767\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 166, Loss: 0.4446 Val: 0.9189 Test: 0.5982\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 167, Loss: 0.6278 Val: 0.9228 Test: 0.6060\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 168, Loss: 0.5891 Val: 0.8988 Test: 0.5691\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 169, Loss: 0.6144 Val: 0.8785 Test: 0.5544\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 170, Loss: 0.5773 Val: 1.0524 Test: 0.5996\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 171, Loss: 0.5523 Val: 1.0882 Test: 0.5944\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 172, Loss: 0.5019 Val: 1.0602 Test: 0.6391\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 173, Loss: 0.5651 Val: 0.9731 Test: 0.5869\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 174, Loss: 0.6300 Val: 0.8905 Test: 0.6143\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 175, Loss: 0.6250 Val: 0.8775 Test: 0.6533\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 176, Loss: 0.6422 Val: 1.1296 Test: 0.6920\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 177, Loss: 0.6401 Val: 1.1745 Test: 0.6719\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 178, Loss: 0.5213 Val: 1.0522 Test: 0.6101\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 179, Loss: 0.5747 Val: 0.7827 Test: 0.6095\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 180, Loss: 0.5791 Val: 0.7891 Test: 0.6133\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 181, Loss: 0.5792 Val: 0.8328 Test: 0.6242\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 182, Loss: 0.5670 Val: 0.8483 Test: 0.6461\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 183, Loss: 0.5764 Val: 0.8717 Test: 0.6134\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 184, Loss: 0.6167 Val: 1.0923 Test: 0.5747\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 185, Loss: 0.6161 Val: 1.1354 Test: 0.6188\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 186, Loss: 0.6137 Val: 1.1095 Test: 0.6711\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 187, Loss: 0.6611 Val: 1.0590 Test: 0.7007\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 188, Loss: 0.6022 Val: 0.7307 Test: 0.7052\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 189, Loss: 0.6526 Val: 0.7425 Test: 0.7281\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 190, Loss: 0.6071 Val: 0.7687 Test: 0.6910\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 191, Loss: 0.6040 Val: 0.8076 Test: 0.6885\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 192, Loss: 0.6145 Val: 0.8237 Test: 0.6800\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 193, Loss: 0.5119 Val: 0.8278 Test: 0.6659\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 194, Loss: 0.5941 Val: 0.8429 Test: 0.6620\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 195, Loss: 0.5385 Val: 0.8684 Test: 0.6831\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 196, Loss: 0.5338 Val: 0.8781 Test: 0.6988\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 197, Loss: 0.5111 Val: 0.8083 Test: 0.6841\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 198, Loss: 0.5440 Val: 0.9189 Test: 0.6664\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "14\n",
      "Epoch: 199, Loss: 0.5015 Val: 0.9577 Test: 0.6644\n",
      "eaat3\n",
      "\n",
      "torch.Size([3180, 115])\n",
      "{'name': 'eaat3', 'root': 'data', 'transform': None, 'pre_transform': <dataset.GenAtomFeatures object at 0x7fcbe045a3a0>, 'pre_filter': None, '_indices': [12, 26, 36, 166, 22, 4, 128, 80, 63, 73, 15, 68, 60, 103, 32, 28, 134, 54, 109, 16, 3, 129, 125, 161, 159, 155, 71, 6, 82, 112, 145, 30, 48, 132, 91, 62, 25, 89, 157, 87, 105, 127, 144, 41, 40, 160, 51, 45, 55, 47, 29, 17, 95, 53, 56, 10, 163, 59, 1, 139, 156, 106, 43, 88, 50, 162, 46, 70, 7, 119, 108, 107, 79, 85, 81, 154, 35, 49, 149, 147, 84, 66, 61, 135, 57, 86, 44, 153, 97, 64, 42, 75, 136, 116, 52, 126, 24, 111, 152, 74, 148, 33, 99, 123, 9, 117, 90, 0, 78, 77, 146, 13, 133, 131, 137, 118, 92, 94, 158, 67, 142, 72, 100, 151, 14, 18, 102, 37, 38, 140, 5, 138, 104, 143, 8, 58, 120, 20, 150, 21, 31, 19, 93, 164, 11, 115, 124, 122, 167, 34, 130, 101, 69, 165, 23, 96, 76, 27, 2, 83, 114, 39, 121, 113, 65, 141, 110, 98], 'data': Data(x=[3180, 115], edge_index=[2, 6420], edge_attr=[6420, 7], y=[168, 1], smiles=[168]), 'slices': defaultdict(<class 'dict'>, {'x': tensor([   0,   14,   28,   38,   48,   64,   80,   93,  106,  115,  124,  142,\n",
      "         160,  170,  180,  197,  214,  228,  242,  256,  270,  286,  302,  319,\n",
      "         336,  351,  366,  391,  416,  432,  445,  461,  477,  493,  506,  522,\n",
      "         538,  554,  570,  586,  602,  615,  628,  658,  688,  718,  748,  779,\n",
      "         810,  840,  870,  902,  934,  967, 1000, 1030, 1060, 1073, 1086, 1100,\n",
      "        1114, 1130, 1146, 1156, 1166, 1175, 1184, 1197, 1210, 1224, 1238, 1248,\n",
      "        1258, 1272, 1286, 1302, 1318, 1331, 1344, 1360, 1376, 1394, 1412, 1429,\n",
      "        1446, 1461, 1476, 1492, 1508, 1524, 1540, 1556, 1572, 1588, 1604, 1629,\n",
      "        1654, 1671, 1688, 1718, 1748, 1779, 1810, 1840, 1870, 1903, 1936, 1966,\n",
      "        1996, 2028, 2058, 2088, 2120, 2133, 2146, 2160, 2174, 2184, 2194, 2210,\n",
      "        2219, 2235, 2244, 2254, 2264, 2278, 2292, 2305, 2318, 2336, 2354, 2371,\n",
      "        2388, 2401, 2417, 2430, 2446, 2460, 2474, 2490, 2506, 2522, 2538, 2554,\n",
      "        2570, 2586, 2602, 2618, 2634, 2649, 2664, 2681, 2698, 2729, 2760, 2790,\n",
      "        2820, 2845, 2870, 2900, 2930, 2963, 2996, 3028, 3060, 3090, 3120, 3150,\n",
      "        3180]), 'edge_index': tensor([   0,   28,   56,   74,   92,  124,  156,  180,  204,  220,  236,  272,\n",
      "         308,  326,  344,  378,  412,  440,  468,  496,  524,  556,  588,  622,\n",
      "         656,  686,  716,  768,  820,  852,  876,  908,  940,  972,  996, 1028,\n",
      "        1060, 1092, 1124, 1156, 1188, 1212, 1236, 1298, 1360, 1422, 1484, 1548,\n",
      "        1612, 1674, 1736, 1802, 1868, 1938, 2008, 2074, 2140, 2164, 2188, 2216,\n",
      "        2244, 2276, 2308, 2326, 2344, 2360, 2376, 2400, 2424, 2452, 2480, 2498,\n",
      "        2516, 2544, 2572, 2604, 2636, 2660, 2684, 2716, 2748, 2784, 2820, 2854,\n",
      "        2888, 2918, 2948, 2980, 3012, 3044, 3076, 3108, 3140, 3172, 3204, 3256,\n",
      "        3308, 3342, 3376, 3438, 3500, 3564, 3628, 3694, 3760, 3830, 3900, 3962,\n",
      "        4024, 4090, 4152, 4214, 4280, 4304, 4328, 4356, 4384, 4402, 4420, 4452,\n",
      "        4468, 4500, 4516, 4534, 4552, 4580, 4608, 4632, 4656, 4692, 4728, 4762,\n",
      "        4796, 4820, 4852, 4876, 4908, 4936, 4964, 4996, 5028, 5060, 5092, 5124,\n",
      "        5156, 5188, 5220, 5252, 5284, 5314, 5344, 5378, 5412, 5476, 5540, 5602,\n",
      "        5664, 5716, 5768, 5834, 5900, 5970, 6040, 6106, 6172, 6234, 6296, 6358,\n",
      "        6420]), 'edge_attr': tensor([   0,   28,   56,   74,   92,  124,  156,  180,  204,  220,  236,  272,\n",
      "         308,  326,  344,  378,  412,  440,  468,  496,  524,  556,  588,  622,\n",
      "         656,  686,  716,  768,  820,  852,  876,  908,  940,  972,  996, 1028,\n",
      "        1060, 1092, 1124, 1156, 1188, 1212, 1236, 1298, 1360, 1422, 1484, 1548,\n",
      "        1612, 1674, 1736, 1802, 1868, 1938, 2008, 2074, 2140, 2164, 2188, 2216,\n",
      "        2244, 2276, 2308, 2326, 2344, 2360, 2376, 2400, 2424, 2452, 2480, 2498,\n",
      "        2516, 2544, 2572, 2604, 2636, 2660, 2684, 2716, 2748, 2784, 2820, 2854,\n",
      "        2888, 2918, 2948, 2980, 3012, 3044, 3076, 3108, 3140, 3172, 3204, 3256,\n",
      "        3308, 3342, 3376, 3438, 3500, 3564, 3628, 3694, 3760, 3830, 3900, 3962,\n",
      "        4024, 4090, 4152, 4214, 4280, 4304, 4328, 4356, 4384, 4402, 4420, 4452,\n",
      "        4468, 4500, 4516, 4534, 4552, 4580, 4608, 4632, 4656, 4692, 4728, 4762,\n",
      "        4796, 4820, 4852, 4876, 4908, 4936, 4964, 4996, 5028, 5060, 5092, 5124,\n",
      "        5156, 5188, 5220, 5252, 5284, 5314, 5344, 5378, 5412, 5476, 5540, 5602,\n",
      "        5664, 5716, 5768, 5834, 5900, 5970, 6040, 6106, 6172, 6234, 6296, 6358,\n",
      "        6420]), 'y': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168]), 'smiles': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168])}), '_data_list': None}\n",
      "minv: 4.0\n",
      "maxv: 8.899999618530273\n",
      "emb_dim: 300\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 000, Loss: 0.9609 Val: 1.9418 Test: 2.2415\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 001, Loss: 0.9782 Val: 1.8479 Test: 2.1399\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 002, Loss: 0.9116 Val: 1.7590 Test: 2.0191\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 003, Loss: 0.9412 Val: 1.6378 Test: 1.9141\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 004, Loss: 0.9515 Val: 1.5398 Test: 1.7877\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 005, Loss: 0.8290 Val: 1.4533 Test: 1.6847\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 006, Loss: 0.8260 Val: 1.3604 Test: 1.5558\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 007, Loss: 0.7404 Val: 1.3299 Test: 1.5121\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 008, Loss: 0.7699 Val: 1.2933 Test: 1.4364\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 009, Loss: 0.7234 Val: 1.2625 Test: 1.4220\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 010, Loss: 0.7431 Val: 1.1977 Test: 1.3378\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 011, Loss: 0.7074 Val: 1.1266 Test: 1.2572\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 012, Loss: 0.7570 Val: 1.0538 Test: 1.1733\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 013, Loss: 0.7373 Val: 0.9906 Test: 1.1360\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 014, Loss: 0.7732 Val: 0.9593 Test: 1.0809\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 015, Loss: 0.7031 Val: 0.8961 Test: 1.0080\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 016, Loss: 0.7463 Val: 0.8576 Test: 0.9604\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 017, Loss: 0.7048 Val: 0.7952 Test: 0.9056\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 018, Loss: 0.7941 Val: 0.7390 Test: 0.9097\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 019, Loss: 0.7831 Val: 0.7182 Test: 0.9763\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 020, Loss: 0.7636 Val: 0.6804 Test: 0.9460\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 021, Loss: 0.7358 Val: 0.6238 Test: 0.8377\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 022, Loss: 0.8032 Val: 0.6988 Test: 0.7600\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 023, Loss: 0.8176 Val: 0.7238 Test: 0.7432\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 024, Loss: 0.7558 Val: 0.6628 Test: 0.7187\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 025, Loss: 0.7177 Val: 0.6315 Test: 0.7869\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 026, Loss: 0.7276 Val: 0.5422 Test: 0.7404\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 027, Loss: 0.7002 Val: 0.5133 Test: 0.7748\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 028, Loss: 0.7245 Val: 0.5119 Test: 0.7714\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 029, Loss: 0.7019 Val: 0.4977 Test: 0.7375\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 030, Loss: 0.7022 Val: 0.5661 Test: 0.6891\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 031, Loss: 0.8212 Val: 0.6275 Test: 0.7571\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 032, Loss: 0.7284 Val: 0.6557 Test: 0.7775\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 033, Loss: 0.6960 Val: 0.6017 Test: 0.8565\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 034, Loss: 0.7192 Val: 0.5976 Test: 0.8524\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 035, Loss: 0.7291 Val: 0.6178 Test: 0.9407\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 036, Loss: 0.7285 Val: 0.6467 Test: 0.9254\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 037, Loss: 0.8050 Val: 0.7689 Test: 0.9029\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 038, Loss: 0.7419 Val: 0.6282 Test: 0.8443\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 039, Loss: 0.7339 Val: 0.6489 Test: 0.8301\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 040, Loss: 0.6715 Val: 0.5724 Test: 0.8275\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 041, Loss: 0.7109 Val: 0.6174 Test: 0.9279\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 042, Loss: 0.7268 Val: 0.5543 Test: 0.8821\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 043, Loss: 0.7267 Val: 0.5517 Test: 0.7488\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 044, Loss: 0.7249 Val: 0.5462 Test: 0.7306\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 045, Loss: 0.7030 Val: 0.5789 Test: 0.7414\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 046, Loss: 0.7027 Val: 0.5165 Test: 0.7748\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 047, Loss: 0.6944 Val: 0.5021 Test: 0.8017\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 048, Loss: 0.6881 Val: 0.5371 Test: 0.7412\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 049, Loss: 0.6717 Val: 0.5289 Test: 0.7524\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 050, Loss: 0.6817 Val: 0.5047 Test: 0.7646\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 051, Loss: 0.6762 Val: 0.5242 Test: 0.7359\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 052, Loss: 0.6531 Val: 0.4634 Test: 0.7980\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 053, Loss: 0.6806 Val: 0.4266 Test: 0.8682\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 054, Loss: 0.6864 Val: 0.4311 Test: 0.8458\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 055, Loss: 0.6898 Val: 0.4998 Test: 0.7957\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 056, Loss: 0.7072 Val: 0.4728 Test: 0.8059\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 057, Loss: 0.7638 Val: 0.5090 Test: 0.8833\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 058, Loss: 0.7059 Val: 0.5040 Test: 0.8792\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 059, Loss: 0.6603 Val: 0.4918 Test: 0.8108\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 060, Loss: 0.6579 Val: 0.4968 Test: 0.7939\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 061, Loss: 0.6271 Val: 0.4954 Test: 0.7922\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 062, Loss: 0.6535 Val: 0.4741 Test: 0.8090\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 063, Loss: 0.6602 Val: 0.4822 Test: 0.8105\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 064, Loss: 0.6503 Val: 0.4797 Test: 0.8098\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 065, Loss: 0.6680 Val: 0.4812 Test: 0.8119\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 066, Loss: 0.6664 Val: 0.4852 Test: 0.8063\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 067, Loss: 0.6871 Val: 0.4900 Test: 0.8089\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 068, Loss: 0.6713 Val: 0.4776 Test: 0.8084\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 069, Loss: 0.6396 Val: 0.4791 Test: 0.7935\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 070, Loss: 0.6591 Val: 0.4605 Test: 0.8068\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 071, Loss: 0.6809 Val: 0.4588 Test: 0.8140\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 072, Loss: 0.6517 Val: 0.4618 Test: 0.8156\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 073, Loss: 0.6737 Val: 0.4974 Test: 0.7942\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 074, Loss: 0.6646 Val: 0.4445 Test: 0.8243\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 075, Loss: 0.6697 Val: 0.4459 Test: 0.8188\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 076, Loss: 0.6916 Val: 0.4756 Test: 0.7978\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 077, Loss: 0.6757 Val: 0.4661 Test: 0.7826\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 078, Loss: 0.6723 Val: 0.4774 Test: 0.7643\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 079, Loss: 0.6572 Val: 0.4676 Test: 0.7823\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 080, Loss: 0.6441 Val: 0.4561 Test: 0.7875\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 081, Loss: 0.6343 Val: 0.4500 Test: 0.7923\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 082, Loss: 0.6679 Val: 0.4935 Test: 0.8185\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 083, Loss: 0.7425 Val: 0.4934 Test: 0.8004\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 084, Loss: 0.6884 Val: 0.4714 Test: 0.8258\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 085, Loss: 0.6488 Val: 0.4817 Test: 0.8048\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 086, Loss: 0.6530 Val: 0.5070 Test: 0.7632\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 087, Loss: 0.6941 Val: 0.5124 Test: 0.7361\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 088, Loss: 0.7122 Val: 0.4728 Test: 0.8034\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 089, Loss: 0.6667 Val: 0.5056 Test: 0.8471\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 090, Loss: 0.7272 Val: 0.6602 Test: 0.7400\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 091, Loss: 0.7351 Val: 0.5492 Test: 0.8335\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 092, Loss: 0.6962 Val: 0.5798 Test: 0.7684\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 093, Loss: 0.8618 Val: 0.6242 Test: 0.6630\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 094, Loss: 0.7532 Val: 0.6218 Test: 0.6432\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 095, Loss: 0.7566 Val: 0.6045 Test: 0.7034\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 096, Loss: 0.7024 Val: 0.6057 Test: 0.7399\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 097, Loss: 0.6704 Val: 0.5568 Test: 0.7440\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 098, Loss: 0.7303 Val: 0.5189 Test: 0.7232\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 099, Loss: 0.7083 Val: 0.5314 Test: 0.7030\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 100, Loss: 0.6865 Val: 0.5411 Test: 0.6996\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 101, Loss: 0.7235 Val: 0.5517 Test: 0.6952\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 102, Loss: 0.7199 Val: 0.5675 Test: 0.6988\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 103, Loss: 0.6932 Val: 0.5870 Test: 0.7816\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 104, Loss: 0.6857 Val: 0.5267 Test: 0.7992\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 105, Loss: 0.7110 Val: 0.5213 Test: 0.8296\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 106, Loss: 0.6956 Val: 0.5312 Test: 0.8117\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 107, Loss: 0.6889 Val: 0.5015 Test: 0.7832\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 108, Loss: 0.6696 Val: 0.5015 Test: 0.7880\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 109, Loss: 0.6824 Val: 0.4976 Test: 0.7751\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 110, Loss: 0.6733 Val: 0.5133 Test: 0.8163\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 111, Loss: 0.6722 Val: 0.4834 Test: 0.7944\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 112, Loss: 0.6561 Val: 0.5084 Test: 0.7343\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 113, Loss: 0.6965 Val: 0.5494 Test: 0.8096\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 114, Loss: 0.7021 Val: 0.6065 Test: 0.8774\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 115, Loss: 0.7122 Val: 0.5788 Test: 0.9375\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 116, Loss: 0.7788 Val: 0.5482 Test: 0.9307\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 117, Loss: 0.7784 Val: 0.5491 Test: 0.8825\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 118, Loss: 0.7186 Val: 0.5786 Test: 0.8548\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 119, Loss: 0.7501 Val: 0.5481 Test: 0.8143\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 120, Loss: 0.7085 Val: 0.5309 Test: 0.8004\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 121, Loss: 0.7011 Val: 0.5507 Test: 0.8194\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 122, Loss: 0.6912 Val: 0.5708 Test: 0.8389\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 123, Loss: 0.6851 Val: 0.6096 Test: 0.8315\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 124, Loss: 0.7617 Val: 0.6390 Test: 0.7297\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 125, Loss: 0.7079 Val: 0.5819 Test: 0.6993\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 126, Loss: 0.7077 Val: 0.5452 Test: 0.6989\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 127, Loss: 0.7362 Val: 0.5562 Test: 0.7353\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 128, Loss: 0.6746 Val: 0.5758 Test: 0.8797\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 129, Loss: 0.7216 Val: 0.5604 Test: 0.7989\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 130, Loss: 0.7389 Val: 0.5913 Test: 0.8363\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 131, Loss: 0.6941 Val: 0.5815 Test: 0.7751\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 132, Loss: 0.6862 Val: 0.4934 Test: 0.7121\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 133, Loss: 0.7134 Val: 0.4798 Test: 0.7000\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 134, Loss: 0.6600 Val: 0.5541 Test: 0.7668\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 135, Loss: 0.7005 Val: 0.5524 Test: 0.6981\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 136, Loss: 0.7009 Val: 0.4968 Test: 0.7094\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 137, Loss: 0.6798 Val: 0.4532 Test: 0.7775\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 138, Loss: 0.7534 Val: 0.5751 Test: 0.8972\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 139, Loss: 0.7173 Val: 0.5629 Test: 0.6719\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 140, Loss: 0.7214 Val: 0.7193 Test: 0.9459\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 141, Loss: 0.7763 Val: 0.6305 Test: 0.9113\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 142, Loss: 0.7439 Val: 0.6136 Test: 0.7233\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 143, Loss: 0.7479 Val: 0.5383 Test: 0.7222\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 144, Loss: 0.7333 Val: 0.5734 Test: 0.9297\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 145, Loss: 0.7204 Val: 0.5838 Test: 0.9146\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 146, Loss: 0.7499 Val: 0.5204 Test: 0.8408\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 147, Loss: 0.7081 Val: 0.5487 Test: 0.7718\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 148, Loss: 0.7402 Val: 0.5959 Test: 0.7805\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 149, Loss: 0.6606 Val: 0.6710 Test: 0.7631\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 150, Loss: 0.8162 Val: 0.7989 Test: 0.9645\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 151, Loss: 0.7310 Val: 0.5678 Test: 0.7466\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 152, Loss: 0.7556 Val: 0.6243 Test: 0.7198\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 153, Loss: 0.7214 Val: 0.6591 Test: 0.7620\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 154, Loss: 0.7266 Val: 0.6400 Test: 0.7848\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 155, Loss: 0.6905 Val: 0.6168 Test: 0.7969\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 156, Loss: 0.7318 Val: 0.6077 Test: 0.7876\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 157, Loss: 0.7445 Val: 0.6139 Test: 0.7724\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 158, Loss: 0.6929 Val: 0.6169 Test: 0.8029\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 159, Loss: 0.7558 Val: 0.5832 Test: 0.7784\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 160, Loss: 0.6728 Val: 0.5764 Test: 0.8181\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 161, Loss: 0.7364 Val: 0.5140 Test: 0.7508\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 162, Loss: 0.6833 Val: 0.4840 Test: 0.7565\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 163, Loss: 0.6938 Val: 0.4585 Test: 0.7325\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 164, Loss: 0.6808 Val: 0.4742 Test: 0.7508\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 165, Loss: 0.7153 Val: 0.4585 Test: 0.7624\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 166, Loss: 0.6634 Val: 0.4554 Test: 0.7579\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 167, Loss: 0.6665 Val: 0.5155 Test: 0.7474\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 168, Loss: 0.6880 Val: 0.5295 Test: 0.7440\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 169, Loss: 0.6735 Val: 0.5321 Test: 0.7533\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 170, Loss: 0.6660 Val: 0.5488 Test: 0.7706\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 171, Loss: 0.7078 Val: 0.5507 Test: 0.7684\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 172, Loss: 0.6740 Val: 0.5466 Test: 0.7939\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 173, Loss: 0.7068 Val: 0.5556 Test: 0.7758\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 174, Loss: 0.7530 Val: 0.5035 Test: 0.7252\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 175, Loss: 0.6698 Val: 0.5078 Test: 0.7211\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 176, Loss: 0.6720 Val: 0.4824 Test: 0.7212\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 177, Loss: 0.6694 Val: 0.4745 Test: 0.7236\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 178, Loss: 0.6592 Val: 0.4748 Test: 0.7450\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 179, Loss: 0.6882 Val: 0.4685 Test: 0.7407\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 180, Loss: 0.6752 Val: 0.4924 Test: 0.7232\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 181, Loss: 0.6512 Val: 0.5547 Test: 0.6590\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 182, Loss: 0.6687 Val: 0.5814 Test: 0.7046\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 183, Loss: 0.6399 Val: 0.6023 Test: 0.7930\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 184, Loss: 0.6898 Val: 0.5891 Test: 0.7905\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 185, Loss: 0.7015 Val: 0.6375 Test: 0.7589\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 186, Loss: 0.7493 Val: 0.6239 Test: 0.7678\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 187, Loss: 0.6837 Val: 0.6197 Test: 0.7827\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 188, Loss: 0.6286 Val: 0.6059 Test: 0.8007\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 189, Loss: 0.6758 Val: 0.5960 Test: 0.8438\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 190, Loss: 0.6755 Val: 0.5721 Test: 0.8408\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 191, Loss: 0.6680 Val: 0.5440 Test: 0.8167\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 192, Loss: 0.6948 Val: 0.5243 Test: 0.8050\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 193, Loss: 0.6968 Val: 0.5102 Test: 0.7887\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 194, Loss: 0.6593 Val: 0.4993 Test: 0.7599\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 195, Loss: 0.6877 Val: 0.4817 Test: 0.7478\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 196, Loss: 0.6537 Val: 0.4813 Test: 0.7201\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 197, Loss: 0.6570 Val: 0.4906 Test: 0.7184\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 198, Loss: 0.7065 Val: 0.5034 Test: 0.7280\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "6\n",
      "Epoch: 199, Loss: 0.6495 Val: 0.5101 Test: 0.7282\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import (InMemoryDataset, Data, download_url,\n",
    "                                  extract_gz)\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "\n",
    "from model import GNN,GNN_graphpred\n",
    "from dataset import LSSInhibitor,GenAtomFeatures,GenAttentiveFeatures\n",
    "from loss import ada_batch_all_triplet_loss\n",
    "\n",
    "device = 'cuda:7'\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "lr = 0.0001\n",
    "decay = 1e-3\n",
    "num_layer = 5\n",
    "emb_dim = 300\n",
    "dropout_ratio = 0\n",
    "JK = 'last'\n",
    "dataset = 'EAAT3'\n",
    "output_model_file = ''\n",
    "gnn_type = 'gin'\n",
    "seed = 0\n",
    "num_workers = 8\n",
    "mode = 'ada_batch_all_triplet_loss'   #ada_batch_all_triplet_loss,ada_batch_hard_triplet_loss,triplet_loss\n",
    "feature_type = 'custom'  #random,onehot,custom,pseudo\n",
    "graph_pooling = 'set2set2' #mean,last,sum,set2set,atention\n",
    "\n",
    "\n",
    "def train():\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        '''\n",
    "        data.x = data.x.to(torch.float32)\n",
    "        data.edge_index=data.edge_index.to(torch.long)\n",
    "        data.edge_attr=data.edge_attr.to(torch.float32)\n",
    "        data.batch=data.batch.to(torch.long)\n",
    "        '''\n",
    "        #out = model(float(data.x), data.edge_index.to(torch.long), data.edge_attr.to(torch.float32), data.batch.to(torch.long))\n",
    "        emb,pre = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        #loss = F.mse_loss(out, data.y)\n",
    "        loss = ada_batch_all_triplet_loss(embeddings=emb, labels=data.y, prediction=pre, device=device, minv=minv, maxv=maxv, weight=0.5, cliff=0.5,squared=False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    mse = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        emb,pre = model(data.x.to(torch.float32), data.edge_index, data.edge_attr, data.batch)\n",
    "        pre = pre*(maxv-minv)+minv\n",
    "        mse.append(F.mse_loss(pre, data.y, reduction='none').cpu())\n",
    "        #print('mse:',mse)\n",
    "    return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "\n",
    "\n",
    "for dataset_name in LSSInhibitor.names.keys():\n",
    "    \n",
    "    print(dataset_name + \"\\n\")\n",
    "    \n",
    "    path = './data'\n",
    "    \n",
    "    # use the attentiveFP node and edge features during the mol-2-graph transoformation\n",
    "    dataset = LSSInhibitor(path, name=dataset_name, pre_transform=GenAtomFeatures('custom')).shuffle()\n",
    "    #dataset = LSSInhibitor(path, name=dataset_name, pre_transform=GenAttentiveFeatures()).shuffle()\n",
    "    #dataset = MoleculeNet(path, name='FreeSolv', pre_transform=GenFeatures()).shuffle()\n",
    "    print(dataset.data.x.shape)\n",
    "    print(dataset.__dict__)\n",
    "    \n",
    "    minv = 1e12\n",
    "    maxv = -1e12\n",
    "    for i in dataset:\n",
    "        if i.y<minv:\n",
    "            minv = i.y.item()\n",
    "        if i.y>maxv:\n",
    "            maxv = i.y.item()\n",
    "    print('minv:',minv) \n",
    "    print('maxv:',maxv)\n",
    "    \n",
    "    #batch_size = 8\n",
    "    \n",
    "    # train, valid, test splitting\n",
    "    N = len(dataset) // 5\n",
    "    val_dataset = dataset[:N]\n",
    "    test_dataset = dataset[N:2 * N]\n",
    "    train_dataset = dataset[2 * N:]\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = GNN_graphpred(num_layer, emb_dim, num_tasks = 1, JK = JK, drop_ratio =dropout_ratio,feature_type = feature_type,graph_pooling = graph_pooling, gnn_type = gnn_type).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
    "                                 weight_decay=decay)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(val_loader)\n",
    "        test_rmse = test(test_loader)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "              f'Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9733a6-a2e3-4d4d-9b86-52a6907902a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuic] *",
   "language": "python",
   "name": "conda-env-cuic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
