{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f201db6-8941-41b0-a906-0d73acc1bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import AttentiveFP\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from rdkit import Chem\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd \n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '/home/shenwanxiang/Research/bidd-clsar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466f9073-7a71-4298-9856-a9f6d5341c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "torch.cuda.set_device(gpuid)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49971289-a82d-4ae6-be61-0d3befe1da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clsar2.dataset import LSSNS, HSSMS  # dataset\n",
    "from clsar2.feature import GenNodeEdgeFeatures115, GenNodeEdgeFeatures39  # feature\n",
    "from clsar2.model.model import ACANet_GCN, ACANet_GIN, ACANet_GAT, ACANet_PNA, get_deg  # model\n",
    "from clsar2.model.pooling import SubstructurePool\n",
    "from clsar2.model.loss import ACALoss\n",
    "from clsar2.model.saver import SaveBestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3124d0-28da-4ebf-ba21-77cfa712e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = HSSMS\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "fp_dim = 1024\n",
    "\n",
    "pre_transform = GenNodeEdgeFeatures39(nBits = fp_dim, radius = 2) #nBits = fp_dim, radius = 2\n",
    "dataset_name = 'chembl4203_ki'\n",
    "\n",
    "edge_dim = pre_transform.edge_dim\n",
    "in_channels = pre_transform.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5488573e-54b8-4e46-a026-4d933930100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] \n",
    "\n",
    "path = './tmpignore/%s_%s_%s' % (pre_transform.in_channels, pre_transform.fp_type, fp_dim)\n",
    "dataset = Dataset(path, name=dataset_name, pre_transform=pre_transform)\n",
    "\n",
    "split = pd.Series(dataset.data.split)\n",
    "train_idx = split[split == 'train'].index.tolist()\n",
    "test_idx = split[split == 'test'].index.tolist()\n",
    "\n",
    "train_dataset = dataset.index_select(train_idx)\n",
    "test_dataset = dataset.index_select(test_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "train_y = dataset.data.y[train_idx]\n",
    "test_y = dataset.data.y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53d9da5-2386-49a2-89d7-a19623b8e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = get_deg(train_dataset)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed218506-98b8-42fc-9912-d5db03632e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/anaconda3/envs/cuda11.3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "pub_args = {'in_channels': in_channels, \n",
    "            'out_channels': 1, \n",
    "            'fp_dim':fp_dim, \n",
    "             'edge_dim': edge_dim, \n",
    "             'convs_layers': [128, 64, 1], \n",
    "             'dense_layers':[512, 128, 32],\n",
    "             'pooling_layer': SubstructurePool('mean'),\n",
    "             'dropout_p': 0.0, 'batch_norms': torch.nn.BatchNorm1d} # ,'batch_norms': \n",
    "\n",
    "model = ACANet_PNA(**pub_args, deg=deg, aggregators=['mean', 'min', 'max', 'sum'], \n",
    "                     scalers=['identity', 'amplification', 'attenuation']).to(device)\n",
    "\n",
    "aca_loss = ACALoss(alpha = 1., cliff_lower = 1., cliff_upper=1., dev_mode=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=10**-5) #\n",
    "\n",
    "saver = SaveBestModel(data_transformer = train_dataset.smiles_to_data, \n",
    "                      save_dir = './', save_name = '%s' % dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad84def-93b2-4183-9092-40b366c6ffd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4a1174-df42-4f95-aa29-e76f1c4b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x, out, embed = model(data.x.float(), data.edge_index,\n",
    "                           data.edge_attr, data.batch, data.fp)\n",
    "        #loss = F.mse_loss(out, data.y)\n",
    "        loss = aca_loss(data.y, out, embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model):\n",
    "    mse = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        x, out, embed = model.eval()(data.x.float(), data.edge_index,\n",
    "                           data.edge_attr, data.batch, data.fp)\n",
    "        mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "    return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(test_loader, model):\n",
    "    #embeds = []\n",
    "    preds = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        E = model.eval()\n",
    "        x, predictions, embeddings = E(data.x.float(), data.edge_index,\n",
    "                                        data.edge_attr, data.batch, data.fp)\n",
    "        #embeds.append(embeddings)\n",
    "        preds.append(predictions)\n",
    "    #embeddings = torch.concat(embeds, axis=0).cpu().numpy()\n",
    "    predictions = torch.concat(preds, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96406ca4-8fda-4cc9-8147-24e5ff3356f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.0427 Test: 6.6792\n",
      "Epoch: 002, Loss: 2.7986 Test: 6.6565\n",
      "Epoch: 003, Loss: 2.7879 Test: 6.6296\n",
      "Epoch: 004, Loss: 2.7837 Test: 6.5925\n",
      "Epoch: 005, Loss: 2.7665 Test: 6.5436\n",
      "Epoch: 006, Loss: 2.7557 Test: 6.4551\n",
      "Epoch: 007, Loss: 2.7312 Test: 6.2488\n",
      "Epoch: 008, Loss: 2.6782 Test: 5.6272\n",
      "Epoch: 009, Loss: 2.4554 Test: 2.0181\n",
      "Epoch: 010, Loss: 1.9614 Test: 1.7971\n",
      "Epoch: 011, Loss: 1.8488 Test: 1.7106\n",
      "Epoch: 012, Loss: 1.7580 Test: 1.5362\n",
      "Epoch: 013, Loss: 1.6539 Test: 1.2782\n",
      "Epoch: 014, Loss: 1.6005 Test: 1.3410\n",
      "Epoch: 015, Loss: 1.5405 Test: 1.1885\n",
      "Epoch: 016, Loss: 1.5034 Test: 1.1750\n",
      "Epoch: 017, Loss: 1.4720 Test: 1.1908\n",
      "Epoch: 018, Loss: 1.4454 Test: 1.1224\n",
      "Epoch: 019, Loss: 1.4228 Test: 1.1271\n",
      "Epoch: 020, Loss: 1.4157 Test: 1.0925\n",
      "Epoch: 021, Loss: 1.3944 Test: 1.1271\n",
      "Epoch: 022, Loss: 1.3859 Test: 1.0862\n",
      "Epoch: 023, Loss: 1.3611 Test: 1.0225\n",
      "Epoch: 024, Loss: 1.3691 Test: 1.0113\n",
      "Epoch: 025, Loss: 1.3586 Test: 1.0928\n",
      "Epoch: 026, Loss: 1.3638 Test: 1.1593\n",
      "Epoch: 027, Loss: 1.3538 Test: 0.9776\n",
      "Epoch: 028, Loss: 1.3449 Test: 1.0234\n",
      "Epoch: 029, Loss: 1.3283 Test: 1.1726\n",
      "Epoch: 030, Loss: 1.3374 Test: 0.9676\n",
      "Epoch: 031, Loss: 1.3137 Test: 0.9755\n",
      "Epoch: 032, Loss: 1.3184 Test: 0.9551\n",
      "Epoch: 033, Loss: 1.3096 Test: 0.9508\n",
      "Epoch: 034, Loss: 1.3208 Test: 0.9768\n",
      "Epoch: 035, Loss: 1.3146 Test: 0.9627\n",
      "Epoch: 036, Loss: 1.2957 Test: 0.9797\n",
      "Epoch: 037, Loss: 1.2970 Test: 1.0049\n",
      "Epoch: 038, Loss: 1.2913 Test: 0.9293\n",
      "Epoch: 039, Loss: 1.3048 Test: 0.9581\n",
      "Epoch: 040, Loss: 1.2806 Test: 0.9400\n",
      "Epoch: 041, Loss: 1.2815 Test: 0.9294\n",
      "Epoch: 042, Loss: 1.2805 Test: 0.9531\n",
      "Epoch: 043, Loss: 1.2806 Test: 0.9624\n",
      "Epoch: 044, Loss: 1.2792 Test: 0.9154\n",
      "Epoch: 045, Loss: 1.2768 Test: 0.9295\n",
      "Epoch: 046, Loss: 1.2779 Test: 0.9135\n",
      "Epoch: 047, Loss: 1.2656 Test: 0.9180\n",
      "Epoch: 048, Loss: 1.2691 Test: 0.9362\n",
      "Epoch: 049, Loss: 1.2679 Test: 0.9553\n",
      "Epoch: 050, Loss: 1.2600 Test: 0.9213\n",
      "Epoch: 051, Loss: 1.2658 Test: 0.9365\n",
      "Epoch: 052, Loss: 1.2653 Test: 0.9616\n",
      "Epoch: 053, Loss: 1.2606 Test: 0.9637\n",
      "Epoch: 054, Loss: 1.2610 Test: 0.9110\n",
      "Epoch: 055, Loss: 1.2445 Test: 0.9089\n",
      "Epoch: 056, Loss: 1.2393 Test: 0.9150\n",
      "Epoch: 057, Loss: 1.2431 Test: 0.9250\n",
      "Epoch: 058, Loss: 1.2503 Test: 0.9646\n",
      "Epoch: 059, Loss: 1.2436 Test: 0.9048\n",
      "Epoch: 060, Loss: 1.2394 Test: 0.9073\n",
      "Epoch: 061, Loss: 1.2222 Test: 0.9518\n",
      "Epoch: 062, Loss: 1.2496 Test: 0.9246\n",
      "Epoch: 063, Loss: 1.2407 Test: 0.9076\n",
      "Epoch: 064, Loss: 1.2348 Test: 0.9317\n",
      "Epoch: 065, Loss: 1.2417 Test: 0.9046\n",
      "Epoch: 066, Loss: 1.2299 Test: 0.9050\n",
      "Epoch: 067, Loss: 1.2347 Test: 0.9310\n",
      "Epoch: 068, Loss: 1.2357 Test: 0.9003\n",
      "Epoch: 069, Loss: 1.2187 Test: 0.9017\n",
      "Epoch: 070, Loss: 1.2237 Test: 0.9025\n",
      "Epoch: 071, Loss: 1.2211 Test: 0.9049\n",
      "Epoch: 072, Loss: 1.2144 Test: 0.9097\n",
      "Epoch: 073, Loss: 1.2026 Test: 0.9160\n",
      "Epoch: 074, Loss: 1.2159 Test: 0.9037\n",
      "Epoch: 075, Loss: 1.2081 Test: 0.9247\n",
      "Epoch: 076, Loss: 1.2068 Test: 0.9160\n",
      "Epoch: 077, Loss: 1.2097 Test: 0.9034\n",
      "Epoch: 078, Loss: 1.2032 Test: 0.9049\n",
      "Epoch: 079, Loss: 1.2141 Test: 0.9087\n",
      "Epoch: 080, Loss: 1.2027 Test: 0.9080\n",
      "Epoch: 081, Loss: 1.1974 Test: 0.9118\n",
      "Epoch: 082, Loss: 1.2059 Test: 0.9005\n",
      "Epoch: 083, Loss: 1.1884 Test: 0.9040\n",
      "Epoch: 084, Loss: 1.1866 Test: 0.9004\n",
      "Epoch: 085, Loss: 1.1802 Test: 0.9147\n",
      "Epoch: 086, Loss: 1.2023 Test: 0.9113\n",
      "Epoch: 087, Loss: 1.1914 Test: 0.9235\n",
      "Epoch: 088, Loss: 1.2011 Test: 0.9036\n",
      "Epoch: 089, Loss: 1.1934 Test: 0.9376\n",
      "Epoch: 090, Loss: 1.1954 Test: 0.8990\n",
      "Epoch: 091, Loss: 1.1831 Test: 0.9078\n",
      "Epoch: 092, Loss: 1.1743 Test: 0.9120\n",
      "Epoch: 093, Loss: 1.1846 Test: 0.9079\n",
      "Epoch: 094, Loss: 1.1823 Test: 0.9172\n",
      "Epoch: 095, Loss: 1.1745 Test: 0.9149\n",
      "Epoch: 096, Loss: 1.1814 Test: 0.9087\n",
      "Epoch: 097, Loss: 1.1629 Test: 0.9351\n",
      "Epoch: 098, Loss: 1.1791 Test: 0.9146\n",
      "Epoch: 099, Loss: 1.1582 Test: 0.9139\n",
      "Epoch: 100, Loss: 1.1708 Test: 0.9285\n",
      "Epoch: 101, Loss: 1.1623 Test: 0.9253\n",
      "Epoch: 102, Loss: 1.1602 Test: 0.9174\n",
      "Epoch: 103, Loss: 1.1761 Test: 0.9613\n",
      "Epoch: 104, Loss: 1.1719 Test: 0.9186\n",
      "Epoch: 105, Loss: 1.1680 Test: 0.9227\n",
      "Epoch: 106, Loss: 1.1538 Test: 0.9254\n",
      "Epoch: 107, Loss: 1.1698 Test: 0.9234\n",
      "Epoch: 108, Loss: 1.1582 Test: 0.9179\n",
      "Epoch: 109, Loss: 1.1585 Test: 0.9240\n",
      "Epoch: 110, Loss: 1.1424 Test: 0.9212\n",
      "Epoch: 111, Loss: 1.1448 Test: 0.9457\n",
      "Epoch: 112, Loss: 1.1502 Test: 0.9275\n",
      "Epoch: 113, Loss: 1.1455 Test: 0.9222\n",
      "Epoch: 114, Loss: 1.1473 Test: 0.9332\n",
      "Epoch: 115, Loss: 1.1422 Test: 0.9277\n",
      "Epoch: 116, Loss: 1.1349 Test: 0.9322\n",
      "Epoch: 117, Loss: 1.1395 Test: 0.9269\n",
      "Epoch: 118, Loss: 1.1451 Test: 0.9294\n",
      "Epoch: 119, Loss: 1.1382 Test: 0.9259\n",
      "Epoch: 120, Loss: 1.1507 Test: 0.9276\n",
      "Epoch: 121, Loss: 1.1440 Test: 0.9325\n",
      "Epoch: 122, Loss: 1.1403 Test: 0.9325\n",
      "Epoch: 123, Loss: 1.1408 Test: 0.9298\n",
      "Epoch: 124, Loss: 1.1450 Test: 0.9319\n",
      "Epoch: 125, Loss: 1.1298 Test: 0.9296\n",
      "Epoch: 126, Loss: 1.1304 Test: 0.9334\n",
      "Epoch: 127, Loss: 1.1234 Test: 0.9301\n",
      "Epoch: 128, Loss: 1.1275 Test: 0.9570\n",
      "Epoch: 129, Loss: 1.1296 Test: 0.9395\n",
      "Epoch: 130, Loss: 1.1258 Test: 0.9404\n",
      "Epoch: 131, Loss: 1.1311 Test: 0.9307\n",
      "Epoch: 132, Loss: 1.1386 Test: 0.9405\n",
      "Epoch: 133, Loss: 1.1273 Test: 0.9364\n",
      "Epoch: 134, Loss: 1.1179 Test: 0.9432\n",
      "Epoch: 135, Loss: 1.1231 Test: 0.9369\n",
      "Epoch: 136, Loss: 1.1162 Test: 0.9374\n",
      "Epoch: 137, Loss: 1.1228 Test: 0.9463\n",
      "Epoch: 138, Loss: 1.1100 Test: 0.9506\n",
      "Epoch: 139, Loss: 1.1136 Test: 0.9415\n",
      "Epoch: 140, Loss: 1.1096 Test: 0.9481\n",
      "Epoch: 141, Loss: 1.1141 Test: 0.9506\n",
      "Epoch: 142, Loss: 1.1123 Test: 0.9447\n",
      "Epoch: 143, Loss: 1.1149 Test: 0.9442\n",
      "Epoch: 144, Loss: 1.1058 Test: 0.9418\n",
      "Epoch: 145, Loss: 1.1259 Test: 0.9406\n",
      "Epoch: 146, Loss: 1.1156 Test: 0.9463\n",
      "Epoch: 147, Loss: 1.1123 Test: 0.9383\n",
      "Epoch: 148, Loss: 1.1157 Test: 0.9510\n",
      "Epoch: 149, Loss: 1.1049 Test: 0.9413\n",
      "Epoch: 150, Loss: 1.1033 Test: 0.9459\n",
      "Epoch: 151, Loss: 1.0973 Test: 0.9460\n",
      "Epoch: 152, Loss: 1.0974 Test: 0.9555\n",
      "Epoch: 153, Loss: 1.1196 Test: 0.9396\n",
      "Epoch: 154, Loss: 1.1038 Test: 0.9412\n",
      "Epoch: 155, Loss: 1.0990 Test: 0.9580\n",
      "Epoch: 156, Loss: 1.0930 Test: 0.9456\n",
      "Epoch: 157, Loss: 1.1099 Test: 0.9567\n",
      "Epoch: 158, Loss: 1.1009 Test: 0.9503\n",
      "Epoch: 159, Loss: 1.0977 Test: 0.9522\n",
      "Epoch: 160, Loss: 1.0921 Test: 0.9567\n",
      "Epoch: 161, Loss: 1.0827 Test: 0.9664\n",
      "Epoch: 162, Loss: 1.0974 Test: 0.9627\n",
      "Epoch: 163, Loss: 1.0947 Test: 0.9638\n",
      "Epoch: 164, Loss: 1.0999 Test: 0.9583\n",
      "Epoch: 165, Loss: 1.0970 Test: 0.9588\n",
      "Epoch: 166, Loss: 1.0919 Test: 0.9615\n",
      "Epoch: 167, Loss: 1.1002 Test: 0.9604\n",
      "Epoch: 168, Loss: 1.0912 Test: 0.9531\n",
      "Epoch: 169, Loss: 1.0952 Test: 0.9550\n",
      "Epoch: 170, Loss: 1.0858 Test: 0.9492\n",
      "Epoch: 171, Loss: 1.1032 Test: 0.9606\n",
      "Epoch: 172, Loss: 1.0939 Test: 0.9618\n",
      "Epoch: 173, Loss: 1.0938 Test: 0.9539\n",
      "Epoch: 174, Loss: 1.0869 Test: 0.9588\n",
      "Epoch: 175, Loss: 1.0940 Test: 0.9621\n",
      "Epoch: 176, Loss: 1.0900 Test: 0.9691\n",
      "Epoch: 177, Loss: 1.0831 Test: 0.9566\n",
      "Epoch: 178, Loss: 1.0842 Test: 0.9814\n",
      "Epoch: 179, Loss: 1.0855 Test: 0.9656\n",
      "Epoch: 180, Loss: 1.0926 Test: 0.9763\n",
      "Epoch: 181, Loss: 1.0878 Test: 0.9628\n",
      "Epoch: 182, Loss: 1.0780 Test: 0.9663\n",
      "Epoch: 183, Loss: 1.0887 Test: 0.9607\n",
      "Epoch: 184, Loss: 1.0820 Test: 0.9631\n",
      "Epoch: 185, Loss: 1.0729 Test: 0.9589\n",
      "Epoch: 186, Loss: 1.0808 Test: 0.9558\n",
      "Epoch: 187, Loss: 1.0777 Test: 0.9672\n",
      "Epoch: 188, Loss: 1.0916 Test: 0.9779\n",
      "Epoch: 189, Loss: 1.0930 Test: 0.9727\n",
      "Epoch: 190, Loss: 1.0612 Test: 0.9599\n",
      "Epoch: 191, Loss: 1.0736 Test: 0.9651\n",
      "Epoch: 192, Loss: 1.0944 Test: 0.9725\n",
      "Epoch: 193, Loss: 1.0754 Test: 0.9733\n",
      "Epoch: 194, Loss: 1.0904 Test: 0.9675\n",
      "Epoch: 195, Loss: 1.0751 Test: 0.9783\n",
      "Epoch: 196, Loss: 1.0648 Test: 0.9743\n",
      "Epoch: 197, Loss: 1.0710 Test: 0.9753\n",
      "Epoch: 198, Loss: 1.0764 Test: 0.9697\n",
      "Epoch: 199, Loss: 1.0874 Test: 0.9778\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for epoch in range(1, epochs):\n",
    "    train_rmse = train(train_loader)\n",
    "    #val_rmse = test(val_loader)\n",
    "    saver(train_rmse, epoch, model, optimizer)\n",
    "    \n",
    "    test_rmse = test(test_loader, model)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Test: {test_rmse:.4f}')\n",
    "    history.append({'Epoch': epoch, 'train_rmse': train_rmse,'test_rmse': test_rmse})\n",
    "\n",
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f1c7e-e006-4781-993c-952cdbda76eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056d47d9-2674-4db2-8c35-0284a2222690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(saver.inMemorySave['model_state_dict'])\n",
    "test_pred = predict(test_loader, model)\n",
    "y_true = test_y.cpu().numpy().reshape(-1, ) \n",
    "y_pred = test_pred.cpu().numpy().reshape(-1, )    \n",
    "cliff_flag = dataset.data.cliff[test_idx].cpu().numpy()\n",
    "dfp = pd.DataFrame([y_true, y_pred, cliff_flag]).T\n",
    "dfp.columns = ['y_true', 'y_pred', 'cliff_flag']\n",
    "\n",
    "## calculate the overall rmse and cliff rmse of the test set\n",
    "overall_rmse = np.sqrt(np.mean(np.square(dfp.y_true - dfp.y_pred)))\n",
    "g = dfp.groupby('cliff_flag')\n",
    "noncliff_rmse, cliff_rmse = g.apply(lambda x:np.sqrt(np.mean(np.square(x.y_true - x.y_pred)))).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a12d4a-bfaf-4b7a-a485-8930c5556c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9598818644677843, 0.9553347495588231)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_rmse, cliff_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3fd24a-022e-4dce-a034-6844409fe58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959881603717804"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce1405b4-2b7f-4114-ba00-20af52f91676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.test_rmse.plot() #iloc[20:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebd904e-5483-403b-b25b-88739b889a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_channels': 39,\n",
       " 'out_channels': 1,\n",
       " 'edge_dim': 10,\n",
       " 'fp_dim': 1024,\n",
       " 'convs_layers': [128, 64, 1],\n",
       " 'dropout_p': 0.0,\n",
       " 'batch_norms': ModuleList(\n",
       "   (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       " ),\n",
       " 'pooling_layer': <clsar2.model.pooling.SubstructurePool at 0x7fe517049400>,\n",
       " 'dense_layers': [512, 128, 32],\n",
       " 'deg': tensor([   0, 1819, 7704, 4669,   95]),\n",
       " 'aggregators': ['mean', 'min', 'max', 'sum'],\n",
       " 'scalers': ['identity', 'amplification', 'attenuation']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99ec89a-5fdd-4509-9f4b-8cf31d4a5a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9598818644677843, 0.9553347495588231)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_rmse, cliff_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "028df5f9-9120-466c-8ccf-3c5a6c283eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.879794\t1.001433"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
