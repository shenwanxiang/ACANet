{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988ab225-f6f0-44b6-b24d-7d1374847765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "%matplotlib inline\n",
    "#A100 80GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f5cf08-0f69-4dc4-830d-ba271bd8ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "torch.cuda.set_device(gpuid)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a32cb66-5320-421b-9259-880db0f5aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/shenwanxiang/Research/bidd-clsar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde33631-dc5e-419f-b970-26e4b10535fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clsar.dataset import LSSNS, HSSMS\n",
    "from clsar.feature import Gen39AtomFeatures\n",
    "from clsar.model.model import ACANet_PNA, get_deg, _fix_reproducibility # model\n",
    "from clsar.model.loss import ACALoss, get_best_cliff\n",
    "_fix_reproducibility(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca7d324-4af1-4477-99c4-4b0a96157fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, aca_loss):\n",
    "\n",
    "    total_examples = 0\n",
    "    total_loss =  0    \n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0  \n",
    "    \n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index, \n",
    "                                        data.edge_attr, data.batch)\n",
    "        \n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                            fingerprints = data.fp)\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs        \n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs        \n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "    \n",
    "    train_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    return train_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_loader, model, aca_loss):\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0\n",
    "\n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "    \n",
    "    mse = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index,\n",
    "                                        data.edge_attr, data.batch)\n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                           fingerprints = data.fp)\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs\n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "\n",
    "        mse.append(F.mse_loss(predictions, data.y, reduction='none').cpu())\n",
    "\n",
    "    test_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    test_rmse = float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    \n",
    "    return test_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets, test_rmse\n",
    "\n",
    "\n",
    "\n",
    "def Test_performance(alpha=1.0):\n",
    "    \n",
    "    model = ACANet_PNA(**pub_args, deg=deg).to(device)  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=10**-5)\n",
    "    aca_loss = ACALoss(alpha=alpha, \n",
    "                        cliff_lower = 1., \n",
    "                        cliff_upper = 1.,\n",
    "                        squared = False,\n",
    "                        similarity_gate = True,\n",
    "                        similarity_neg = 0.8,\n",
    "                        similarity_pos = 0.2,\n",
    "                        dev_mode = True,)\n",
    "    \n",
    "    history = []\n",
    "    #ls_his = []\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, tsm_loss, reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets = train(train_loader, model, optimizer, aca_loss)\n",
    "\n",
    "        _, _, _, _, _, _, train_n_hv_triplets, train_rmse = test(train_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, val_n_hv_triplets, val_rmse = test(val_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, test_n_hv_triplets, test_rmse = test(test_loader, model, aca_loss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f} tsm_loss: {tsm_loss:.4f} reg_loss: {reg_loss:.4f} '\n",
    "              f'N_Y: {n_label_triplets:03d} N_S: {n_structure_triplets:03d} N: {n_triplets:03d} N_HV: {n_hv_triplets:03d} '\n",
    "              f'Val: {val_rmse:.4f} Test: {test_rmse:.4f}')\n",
    "    \n",
    "        history.append({'Epoch':epoch, 'train_loss':train_loss, 'train_triplet_loss':tsm_loss,\n",
    "                        'train_reg_loss':reg_loss, 'val_rmse':val_rmse, \n",
    "                        'test_rmse':test_rmse, 'train_rmse':train_rmse,\n",
    "                        \n",
    "                        'n_label_triplets': n_label_triplets, \n",
    "                        'n_structure_triplets':n_structure_triplets,\n",
    "                        'n_triplets':n_triplets,\n",
    "                        'n_hv_triplets':n_hv_triplets,\n",
    "                        \n",
    "\n",
    "                        'train_n_hv_triplets':train_n_hv_triplets,\n",
    "                        'val_n_hv_triplets':val_n_hv_triplets,\n",
    "                        'test_n_hv_triplets':test_n_hv_triplets,\n",
    "                       \n",
    "                       })\n",
    "        #ls_his.append({'Epoch':epoch, 'mae_loss':float(mae_loss), 'triplet_loss':float(triplet_loss)})\n",
    "    dfh = pd.DataFrame(history)\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aa1d63b-0567-406f-9f48-291959b18978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'CHEMBL3979_EC50'\n",
    "Dataset =  HSSMS #LSSNS \n",
    "epochs = 800\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "\n",
    "pre_transform = Gen39AtomFeatures()\n",
    "in_channels = pre_transform.in_channels\n",
    "path = './data/'\n",
    "\n",
    "## model HPs\n",
    "pub_args = {'in_channels':pre_transform.in_channels, \n",
    "            'edge_dim':pre_transform.edge_dim,\n",
    "            'convs_layers': [64, 128, 256, 512],   \n",
    "            'dense_layers': [256, 128, 32], \n",
    "            'out_channels':1, \n",
    "            'aggregators': ['mean', 'min', 'max', 'sum','std'],\n",
    "            'scalers':['identity', 'amplification', 'attenuation'] ,\n",
    "            'dropout_p': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7b5051-3408-450a-9901-79b1042719d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f6da88-ccba-4731-a70b-f53c5c006652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 37.6279 tsm_loss: 31.3185 reg_loss: 6.3093 N_Y: 405148 N_S: 1345 N: 723 N_HV: 2090 Val: 6.6624 Test: 6.6574\n",
      "Epoch: 002, Loss: 5.7550 tsm_loss: 0.0000 reg_loss: 5.7550 N_Y: 403115 N_S: 1728 N: 871 N_HV: 000 Val: 6.6591 Test: 6.6541\n",
      "Epoch: 003, Loss: 5.1914 tsm_loss: 0.0910 reg_loss: 5.1004 N_Y: 404914 N_S: 1451 N: 809 N_HV: 021 Val: 6.6528 Test: 6.6478\n",
      "Epoch: 004, Loss: 4.2507 tsm_loss: 0.0859 reg_loss: 4.1649 N_Y: 405940 N_S: 1777 N: 1001 N_HV: 085 Val: 6.6404 Test: 6.6357\n",
      "Epoch: 005, Loss: 2.9336 tsm_loss: 0.0863 reg_loss: 2.8472 N_Y: 404316 N_S: 1261 N: 689 N_HV: 005 Val: 6.5882 Test: 6.5852\n",
      "Epoch: 006, Loss: 1.4796 tsm_loss: 0.0000 reg_loss: 1.4796 N_Y: 405238 N_S: 1233 N: 603 N_HV: 000 Val: 6.3250 Test: 6.3300\n",
      "Epoch: 007, Loss: 1.2747 tsm_loss: 0.0000 reg_loss: 1.2747 N_Y: 405693 N_S: 1638 N: 871 N_HV: 000 Val: 5.6635 Test: 5.6828\n",
      "Epoch: 008, Loss: 1.3072 tsm_loss: 0.0000 reg_loss: 1.3072 N_Y: 402855 N_S: 1793 N: 973 N_HV: 000 Val: 4.8576 Test: 4.8965\n",
      "Epoch: 009, Loss: 1.8877 tsm_loss: 0.8374 reg_loss: 1.0503 N_Y: 406004 N_S: 1435 N: 746 N_HV: 192 Val: 4.0279 Test: 4.0888\n",
      "Epoch: 010, Loss: 1.0524 tsm_loss: 0.0000 reg_loss: 1.0524 N_Y: 405806 N_S: 1559 N: 878 N_HV: 000 Val: 3.0824 Test: 3.1740\n",
      "Epoch: 011, Loss: 1.0143 tsm_loss: 0.0000 reg_loss: 1.0143 N_Y: 405062 N_S: 1411 N: 723 N_HV: 000 Val: 2.0780 Test: 2.1917\n",
      "Epoch: 012, Loss: 0.9651 tsm_loss: 0.0000 reg_loss: 0.9651 N_Y: 406463 N_S: 1541 N: 850 N_HV: 000 Val: 1.4345 Test: 1.5371\n",
      "Epoch: 013, Loss: 0.9321 tsm_loss: 0.0107 reg_loss: 0.9214 N_Y: 405298 N_S: 1323 N: 679 N_HV: 021 Val: 1.2514 Test: 1.3474\n",
      "Epoch: 014, Loss: 0.8709 tsm_loss: 0.0000 reg_loss: 0.8709 N_Y: 406703 N_S: 1283 N: 663 N_HV: 000 Val: 1.1631 Test: 1.2585\n",
      "Epoch: 015, Loss: 0.8436 tsm_loss: 0.0000 reg_loss: 0.8436 N_Y: 406643 N_S: 1196 N: 602 N_HV: 000 Val: 1.0028 Test: 1.0726\n",
      "Epoch: 016, Loss: 0.7997 tsm_loss: 0.0000 reg_loss: 0.7997 N_Y: 406424 N_S: 1345 N: 706 N_HV: 000 Val: 0.9408 Test: 0.9763\n",
      "Epoch: 017, Loss: 0.7784 tsm_loss: 0.0000 reg_loss: 0.7784 N_Y: 404845 N_S: 1234 N: 626 N_HV: 000 Val: 0.9131 Test: 0.9537\n",
      "Epoch: 018, Loss: 0.7453 tsm_loss: 0.0000 reg_loss: 0.7453 N_Y: 404885 N_S: 1087 N: 579 N_HV: 000 Val: 0.8996 Test: 0.9368\n",
      "Epoch: 019, Loss: 0.7243 tsm_loss: 0.0000 reg_loss: 0.7243 N_Y: 406782 N_S: 1476 N: 723 N_HV: 000 Val: 0.8818 Test: 0.8956\n",
      "Epoch: 020, Loss: 0.7067 tsm_loss: 0.0000 reg_loss: 0.7067 N_Y: 405298 N_S: 1502 N: 826 N_HV: 000 Val: 0.8694 Test: 0.8739\n",
      "Epoch: 021, Loss: 0.6923 tsm_loss: 0.0000 reg_loss: 0.6923 N_Y: 406244 N_S: 1370 N: 748 N_HV: 000 Val: 0.8576 Test: 0.8568\n",
      "Epoch: 022, Loss: 0.6699 tsm_loss: 0.0000 reg_loss: 0.6699 N_Y: 405928 N_S: 1692 N: 948 N_HV: 000 Val: 0.8487 Test: 0.8409\n",
      "Epoch: 023, Loss: 0.6631 tsm_loss: 0.0000 reg_loss: 0.6631 N_Y: 403800 N_S: 1408 N: 680 N_HV: 000 Val: 0.8433 Test: 0.8263\n",
      "Epoch: 024, Loss: 0.6399 tsm_loss: 0.0000 reg_loss: 0.6399 N_Y: 405587 N_S: 1451 N: 763 N_HV: 000 Val: 0.8300 Test: 0.8256\n",
      "Epoch: 025, Loss: 0.6293 tsm_loss: 0.0000 reg_loss: 0.6293 N_Y: 405119 N_S: 1312 N: 704 N_HV: 000 Val: 0.8240 Test: 0.8079\n",
      "Epoch: 026, Loss: 0.6153 tsm_loss: 0.0000 reg_loss: 0.6153 N_Y: 402455 N_S: 1473 N: 828 N_HV: 000 Val: 0.8210 Test: 0.7939\n",
      "Epoch: 027, Loss: 0.5967 tsm_loss: 0.0000 reg_loss: 0.5967 N_Y: 405387 N_S: 1344 N: 732 N_HV: 000 Val: 0.8234 Test: 0.8220\n",
      "Epoch: 028, Loss: 0.5902 tsm_loss: 0.0000 reg_loss: 0.5902 N_Y: 405515 N_S: 1354 N: 687 N_HV: 000 Val: 0.8126 Test: 0.7808\n",
      "Epoch: 029, Loss: 0.5859 tsm_loss: 0.0000 reg_loss: 0.5859 N_Y: 405913 N_S: 1435 N: 698 N_HV: 000 Val: 0.8023 Test: 0.7939\n",
      "Epoch: 030, Loss: 0.5661 tsm_loss: 0.0000 reg_loss: 0.5661 N_Y: 404583 N_S: 1354 N: 651 N_HV: 000 Val: 0.7934 Test: 0.7713\n",
      "Epoch: 031, Loss: 0.5513 tsm_loss: 0.0000 reg_loss: 0.5513 N_Y: 405951 N_S: 1387 N: 801 N_HV: 000 Val: 0.7863 Test: 0.7665\n",
      "Epoch: 032, Loss: 0.5429 tsm_loss: 0.0000 reg_loss: 0.5429 N_Y: 405273 N_S: 1324 N: 709 N_HV: 000 Val: 0.7826 Test: 0.7534\n",
      "Epoch: 033, Loss: 0.5338 tsm_loss: 0.0000 reg_loss: 0.5338 N_Y: 406406 N_S: 1398 N: 769 N_HV: 000 Val: 0.7859 Test: 0.7675\n",
      "Epoch: 034, Loss: 0.5563 tsm_loss: 0.0248 reg_loss: 0.5316 N_Y: 405212 N_S: 1711 N: 852 N_HV: 021 Val: 0.7818 Test: 0.7515\n",
      "Epoch: 035, Loss: 0.5504 tsm_loss: 0.0000 reg_loss: 0.5504 N_Y: 406095 N_S: 1351 N: 744 N_HV: 000 Val: 0.8011 Test: 0.8032\n",
      "Epoch: 036, Loss: 0.5632 tsm_loss: 0.0000 reg_loss: 0.5632 N_Y: 405707 N_S: 1691 N: 896 N_HV: 000 Val: 0.7741 Test: 0.7406\n",
      "Epoch: 037, Loss: 0.5232 tsm_loss: 0.0000 reg_loss: 0.5232 N_Y: 405158 N_S: 1397 N: 783 N_HV: 000 Val: 0.7739 Test: 0.7392\n",
      "Epoch: 038, Loss: 0.5142 tsm_loss: 0.0000 reg_loss: 0.5142 N_Y: 407471 N_S: 1537 N: 827 N_HV: 000 Val: 0.7682 Test: 0.7281\n",
      "Epoch: 039, Loss: 0.4894 tsm_loss: 0.0000 reg_loss: 0.4894 N_Y: 405393 N_S: 1301 N: 723 N_HV: 000 Val: 0.7658 Test: 0.7140\n",
      "Epoch: 040, Loss: 0.4692 tsm_loss: 0.0000 reg_loss: 0.4692 N_Y: 405484 N_S: 1453 N: 708 N_HV: 000 Val: 0.7675 Test: 0.7127\n",
      "Epoch: 041, Loss: 0.4634 tsm_loss: 0.0000 reg_loss: 0.4634 N_Y: 406454 N_S: 1265 N: 638 N_HV: 000 Val: 0.7573 Test: 0.7151\n",
      "Epoch: 042, Loss: 0.4569 tsm_loss: 0.0000 reg_loss: 0.4569 N_Y: 406354 N_S: 1242 N: 650 N_HV: 000 Val: 0.7545 Test: 0.7185\n",
      "Epoch: 043, Loss: 0.4431 tsm_loss: 0.0000 reg_loss: 0.4431 N_Y: 406257 N_S: 1528 N: 841 N_HV: 000 Val: 0.7528 Test: 0.7163\n",
      "Epoch: 044, Loss: 0.4346 tsm_loss: 0.0000 reg_loss: 0.4346 N_Y: 406492 N_S: 1310 N: 738 N_HV: 000 Val: 0.7489 Test: 0.6993\n",
      "Epoch: 045, Loss: 0.4273 tsm_loss: 0.0000 reg_loss: 0.4273 N_Y: 404990 N_S: 1617 N: 819 N_HV: 000 Val: 0.7526 Test: 0.7098\n",
      "Epoch: 046, Loss: 0.4208 tsm_loss: 0.0000 reg_loss: 0.4208 N_Y: 405941 N_S: 1575 N: 838 N_HV: 000 Val: 0.7438 Test: 0.6853\n",
      "Epoch: 047, Loss: 0.4301 tsm_loss: 0.0000 reg_loss: 0.4301 N_Y: 404105 N_S: 1664 N: 868 N_HV: 000 Val: 0.7688 Test: 0.7385\n",
      "Epoch: 048, Loss: 0.4588 tsm_loss: 0.0000 reg_loss: 0.4588 N_Y: 404943 N_S: 1375 N: 738 N_HV: 000 Val: 0.8040 Test: 0.7237\n",
      "Epoch: 049, Loss: 0.4200 tsm_loss: 0.0000 reg_loss: 0.4200 N_Y: 404347 N_S: 1451 N: 844 N_HV: 000 Val: 0.7865 Test: 0.7615\n",
      "Epoch: 050, Loss: 0.4534 tsm_loss: 0.0000 reg_loss: 0.4534 N_Y: 406211 N_S: 1453 N: 794 N_HV: 000 Val: 0.7636 Test: 0.6937\n",
      "Epoch: 051, Loss: 0.4055 tsm_loss: 0.0000 reg_loss: 0.4055 N_Y: 405691 N_S: 1341 N: 695 N_HV: 000 Val: 0.7520 Test: 0.7218\n",
      "Epoch: 052, Loss: 0.4142 tsm_loss: 0.0000 reg_loss: 0.4142 N_Y: 404377 N_S: 1558 N: 856 N_HV: 000 Val: 0.7677 Test: 0.6848\n",
      "Epoch: 053, Loss: 0.4300 tsm_loss: 0.0000 reg_loss: 0.4300 N_Y: 404100 N_S: 1609 N: 895 N_HV: 000 Val: 0.7609 Test: 0.7123\n",
      "Epoch: 054, Loss: 0.3962 tsm_loss: 0.0000 reg_loss: 0.3962 N_Y: 405580 N_S: 1614 N: 920 N_HV: 000 Val: 0.7759 Test: 0.6934\n",
      "Epoch: 055, Loss: 0.3995 tsm_loss: 0.0000 reg_loss: 0.3995 N_Y: 405678 N_S: 1377 N: 796 N_HV: 000 Val: 0.7526 Test: 0.6699\n",
      "Epoch: 056, Loss: 0.3902 tsm_loss: 0.0000 reg_loss: 0.3902 N_Y: 404904 N_S: 1561 N: 786 N_HV: 000 Val: 0.7474 Test: 0.6724\n",
      "Epoch: 057, Loss: 0.3825 tsm_loss: 0.0000 reg_loss: 0.3825 N_Y: 404917 N_S: 1650 N: 882 N_HV: 000 Val: 0.7504 Test: 0.7179\n",
      "Epoch: 058, Loss: 0.3795 tsm_loss: 0.0000 reg_loss: 0.3795 N_Y: 406097 N_S: 1369 N: 760 N_HV: 000 Val: 0.7379 Test: 0.6783\n",
      "Epoch: 059, Loss: 0.3775 tsm_loss: 0.0000 reg_loss: 0.3775 N_Y: 406429 N_S: 1461 N: 754 N_HV: 000 Val: 0.7575 Test: 0.6767\n",
      "Epoch: 060, Loss: 0.3885 tsm_loss: 0.0000 reg_loss: 0.3885 N_Y: 405173 N_S: 1532 N: 835 N_HV: 000 Val: 0.7616 Test: 0.6956\n",
      "Epoch: 061, Loss: 0.3686 tsm_loss: 0.0000 reg_loss: 0.3686 N_Y: 404684 N_S: 1296 N: 687 N_HV: 000 Val: 0.7447 Test: 0.6782\n",
      "Epoch: 062, Loss: 0.3588 tsm_loss: 0.0000 reg_loss: 0.3588 N_Y: 405611 N_S: 1441 N: 720 N_HV: 000 Val: 0.7377 Test: 0.6735\n",
      "Epoch: 063, Loss: 0.3549 tsm_loss: 0.0000 reg_loss: 0.3549 N_Y: 406048 N_S: 1826 N: 1059 N_HV: 000 Val: 0.7376 Test: 0.6635\n",
      "Epoch: 064, Loss: 0.3606 tsm_loss: 0.0000 reg_loss: 0.3606 N_Y: 406376 N_S: 1411 N: 708 N_HV: 000 Val: 0.7737 Test: 0.7288\n",
      "Epoch: 065, Loss: 0.3584 tsm_loss: 0.0000 reg_loss: 0.3584 N_Y: 404238 N_S: 1634 N: 830 N_HV: 000 Val: 0.7472 Test: 0.6996\n",
      "Epoch: 066, Loss: 0.4262 tsm_loss: 0.0504 reg_loss: 0.3758 N_Y: 405576 N_S: 1217 N: 685 N_HV: 021 Val: 0.8855 Test: 0.8177\n",
      "Epoch: 067, Loss: 0.4395 tsm_loss: 0.0000 reg_loss: 0.4395 N_Y: 406874 N_S: 1389 N: 780 N_HV: 000 Val: 0.8855 Test: 0.7845\n",
      "Epoch: 068, Loss: 0.4428 tsm_loss: 0.0000 reg_loss: 0.4428 N_Y: 407221 N_S: 1297 N: 661 N_HV: 000 Val: 0.8499 Test: 0.7661\n",
      "Epoch: 069, Loss: 0.4358 tsm_loss: 0.0000 reg_loss: 0.4358 N_Y: 404535 N_S: 1662 N: 896 N_HV: 000 Val: 0.7562 Test: 0.7060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# With AC-Awareness ($\\alpha = 1$)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mTest_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Without AC-Awareness ($\\alpha = 0$)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 119\u001b[0m, in \u001b[0;36mTest_performance\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#ls_his = []\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs):\n\u001b[0;32m--> 119\u001b[0m     train_loss, tsm_loss, reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maca_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     _, _, _, _, _, _, train_n_hv_triplets, train_rmse \u001b[38;5;241m=\u001b[39m test(train_loader, model, aca_loss)\n\u001b[1;32m    122\u001b[0m     _, _, _, _, _, _, val_n_hv_triplets, val_rmse \u001b[38;5;241m=\u001b[39m test(val_loader, model, aca_loss)\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, aca_loss)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m predictions, embeddings \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat(), data\u001b[38;5;241m.\u001b[39medge_index, \n\u001b[1;32m     19\u001b[0m                                 data\u001b[38;5;241m.\u001b[39medge_attr, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m---> 21\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m \u001b[43maca_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfingerprints\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs \u001b[38;5;241m=\u001b[39m loss_out\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Research/bidd-clsar/clsar/model/loss.py:94\u001b[0m, in \u001b[0;36mACALoss.forward\u001b[0;34m(self, labels, predictions, embeddings, fingerprints)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     76\u001b[0m             labels: Tensor,\n\u001b[1;32m     77\u001b[0m             predictions: Tensor,\n\u001b[1;32m     78\u001b[0m             embeddings: Tensor,\n\u001b[1;32m     79\u001b[0m             fingerprints: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Compute the ACA loss.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        If dev_mode=False: loss\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_aca_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfingerprints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfingerprints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcliff_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcliff_lower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcliff_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcliff_upper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_gate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_gate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdev_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdev_mode\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/bidd-clsar/clsar/model/loss.py:306\u001b[0m, in \u001b[0;36m_aca_loss\u001b[0;34m(labels, predictions, embeddings, fingerprints, alpha, cliff_lower, cliff_upper, similarity_gate, similarity_neg, similarity_pos, squared, p, dev_mode, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     loss \u001b[38;5;241m=\u001b[39m reg_loss \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m tsm_loss\n\u001b[1;32m    305\u001b[0m     pos_triplets \u001b[38;5;241m=\u001b[39m (triplet_loss_masked \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-16\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 306\u001b[0m     N_HV_ACTs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mpos_triplets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# 8. 返回结果\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev_mode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train, valid, test splitting\n",
    "res1 = []\n",
    "res2 = []\n",
    "for seed in [8]: #, 16, 24, 42, 64, 128, 256, 512, 1024, 2048\n",
    "    dataset = Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42)\n",
    "    N = len(dataset) // 5\n",
    "    val_dataset = dataset[:N]\n",
    "    test_dataset = dataset[N:2 * N]\n",
    "    train_dataset = dataset[2 * N:]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    deg = get_deg(train_dataset)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # With AC-Awareness ($\\alpha = 1$)\n",
    "    df1 = Test_performance(alpha=1.0)\n",
    "    df1['seed'] = seed\n",
    "    # Without AC-Awareness ($\\alpha = 0$)\n",
    "    df2 = Test_performance(alpha=0.0)\n",
    "    df2['seed'] = seed\n",
    "    res1.append(df1)\n",
    "    res2.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc7c98-8ac7-41d4-97d2-c3ea1e0d1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "Epoch: 001, Loss: 505.8423 tsm_loss: 499.5391 reg_loss: 6.3032 N_Y: 406098 N_S: 1754772 N: 406098 N_HV: 23162645 Val: 6.6238 Test: 6.5971\n",
    "Epoch: 002, Loss: 164.1380 tsm_loss: 158.4051 reg_loss: 5.7329 N_Y: 407711 N_S: 1754772 N: 407711 N_HV: 20964135 Val: 6.6180 Test: 6.5913\n",
    "Epoch: 003, Loss: 116.1526 tsm_loss: 111.1598 reg_loss: 4.9928 N_Y: 406124 N_S: 1754772 N: 406124 N_HV: 19399902 Val: 6.5892 Test: 6.5626\n",
    "Epoch: 004, Loss: 85.8233 tsm_loss: 81.7326 reg_loss: 4.0907 N_Y: 404934 N_S: 1754772 N: 404934 N_HV: 18177328 Val: 6.4449 Test: 6.4185\n",
    "Epoch: 005, Loss: 64.3978 tsm_loss: 61.3894 reg_loss: 3.0084 N_Y: 404929 N_S: 1754772 N: 404929 N_HV: 16892596 Val: 6.0339 Test: 6.0068\n",
    "Epoch: 006, Loss: 51.1996 tsm_loss: 49.4932 reg_loss: 1.7064 N_Y: 405037 N_S: 1754772 N: 405037 N_HV: 16161038 Val: 5.1840 Test: 5.1546\n",
    "Epoch: 007, Loss: 41.2007 tsm_loss: 40.2709 reg_loss: 0.9298 N_Y: 407263 N_S: 1754772 N: 407263 N_HV: 15222850 Val: 3.9104 Test: 3.8786\n",
    "Epoch: 008, Loss: 35.2102 tsm_loss: 34.1526 reg_loss: 1.0576 N_Y: 405573 N_S: 1754772 N: 405573 N_HV: 14585591 Val: 2.7372 Test: 2.7022\n",
    "Epoch: 009, Loss: 28.8879 tsm_loss: 27.8919 reg_loss: 0.9960 N_Y: 405859 N_S: 1754772 N: 405859 N_HV: 13614493 Val: 2.1299 Test: 2.0952\n",
    "Epoch: 010, Loss: 25.8078 tsm_loss: 24.9041 reg_loss: 0.9037 N_Y: 407184 N_S: 1754772 N: 407184 N_HV: 13922313 Val: 1.7592 Test: 1.7214\n",
    "Epoch: 011, Loss: 23.2485 tsm_loss: 22.3144 reg_loss: 0.9341 N_Y: 406054 N_S: 1754772 N: 406054 N_HV: 13378202 Val: 1.4483 Test: 1.4128\n",
    "Epoch: 012, Loss: 21.3133 tsm_loss: 20.3923 reg_loss: 0.9210 N_Y: 404748 N_S: 1754772 N: 404748 N_HV: 13387117 Val: 1.2014 Test: 1.1677\n",
    "Epoch: 013, Loss: 19.2430 tsm_loss: 18.3419 reg_loss: 0.9011 N_Y: 405713 N_S: 1754772 N: 405713 N_HV: 12809677 Val: 1.1517 Test: 1.1229\n",
    "Epoch: 014, Loss: 17.0707 tsm_loss: 16.1642 reg_loss: 0.9066 N_Y: 406226 N_S: 1754772 N: 406226 N_HV: 12633630 Val: 1.1729 Test: 1.1523\n",
    "Epoch: 015, Loss: 15.2888 tsm_loss: 14.3809 reg_loss: 0.9079 N_Y: 405731 N_S: 1754772 N: 405731 N_HV: 12061293 Val: 1.1521 Test: 1.1294\n",
    "Epoch: 016, Loss: 14.0254 tsm_loss: 13.1231 reg_loss: 0.9022 N_Y: 404400 N_S: 1754772 N: 404400 N_HV: 11842593 Val: 1.1371 Test: 1.1158\n",
    "Epoch: 017, Loss: 13.4109 tsm_loss: 12.5078 reg_loss: 0.9031 N_Y: 404543 N_S: 1754772 N: 404543 N_HV: 11810295 Val: 1.1364 Test: 1.1107\n",
    "Epoch: 018, Loss: 12.6436 tsm_loss: 11.7351 reg_loss: 0.9085 N_Y: 406329 N_S: 1754772 N: 406329 N_HV: 11543175 Val: 1.1534 Test: 1.1327\n",
    "Epoch: 019, Loss: 11.8808 tsm_loss: 10.9745 reg_loss: 0.9063 N_Y: 405813 N_S: 1754772 N: 405813 N_HV: 11353777 Val: 1.1434 Test: 1.1189\n",
    "Epoch: 020, Loss: 11.0344 tsm_loss: 10.1323 reg_loss: 0.9021 N_Y: 406899 N_S: 1754772 N: 406899 N_HV: 10946095 Val: 1.1407 Test: 1.1188\n",
    "Epoch: 021, Loss: 10.3187 tsm_loss: 9.4154 reg_loss: 0.9033 N_Y: 403563 N_S: 1754772 N: 403563 N_HV: 10568007 Val: 1.1482 Test: 1.1259\n",
    "Epoch: 022, Loss: 9.7869 tsm_loss: 8.8831 reg_loss: 0.9038 N_Y: 406004 N_S: 1754772 N: 406004 N_HV: 10596467 Val: 1.1231 Test: 1.0989\n",
    "Epoch: 023, Loss: 9.3746 tsm_loss: 8.4732 reg_loss: 0.9014 N_Y: 407431 N_S: 1754772 N: 407431 N_HV: 10632770 Val: 1.1309 Test: 1.1085\n",
    "Epoch: 024, Loss: 9.0668 tsm_loss: 8.1601 reg_loss: 0.9068 N_Y: 406086 N_S: 1754772 N: 406086 N_HV: 10323246 Val: 1.1329 Test: 1.1063\n",
    "Epoch: 025, Loss: 8.2675 tsm_loss: 7.3605 reg_loss: 0.9070 N_Y: 406918 N_S: 1754772 N: 406918 N_HV: 10002732 Val: 1.1291 Test: 1.1024\n",
    "Epoch: 026, Loss: 7.9625 tsm_loss: 7.0596 reg_loss: 0.9029 N_Y: 405222 N_S: 1754772 N: 405222 N_HV: 9910866 Val: 1.1358 Test: 1.1156\n",
    "Epoch: 027, Loss: 7.7207 tsm_loss: 6.8163 reg_loss: 0.9044 N_Y: 407275 N_S: 1754772 N: 407275 N_HV: 9627092 Val: 1.1381 Test: 1.1156\n",
    "Epoch: 028, Loss: 7.3465 tsm_loss: 6.4424 reg_loss: 0.9041 N_Y: 406111 N_S: 1754772 N: 406111 N_HV: 9591652 Val: 1.1387 Test: 1.1190\n",
    "Epoch: 029, Loss: 7.2410 tsm_loss: 6.3353 reg_loss: 0.9057 N_Y: 405556 N_S: 1754772 N: 405556 N_HV: 9888921 Val: 1.1268 Test: 1.1008\n",
    "Epoch: 030, Loss: 7.1685 tsm_loss: 6.2637 reg_loss: 0.9048 N_Y: 407425 N_S: 1754772 N: 407425 N_HV: 9382837 Val: 1.1183 Test: 1.0961\n",
    "Epoch: 031, Loss: 6.7557 tsm_loss: 5.8509 reg_loss: 0.9048 N_Y: 404700 N_S: 1754772 N: 404700 N_HV: 9148393 Val: 1.1357 Test: 1.1145\n",
    "Epoch: 032, Loss: 6.5621 tsm_loss: 5.6617 reg_loss: 0.9004 N_Y: 405274 N_S: 1754772 N: 405274 N_HV: 8934917 Val: 1.1313 Test: 1.1093\n",
    "Epoch: 033, Loss: 6.3590 tsm_loss: 5.4563 reg_loss: 0.9027 N_Y: 406289 N_S: 1754772 N: 406289 N_HV: 9076396 Val: 1.1398 Test: 1.1149\n",
    "Epoch: 034, Loss: 6.0815 tsm_loss: 5.1754 reg_loss: 0.9062 N_Y: 405553 N_S: 1754772 N: 405553 N_HV: 8810466 Val: 1.1280 Test: 1.1037\n",
    "Epoch: 035, Loss: 6.0966 tsm_loss: 5.1956 reg_loss: 0.9010 N_Y: 406233 N_S: 1754772 N: 406233 N_HV: 8982248 Val: 1.1454 Test: 1.1245\n",
    "Epoch: 036, Loss: 5.8351 tsm_loss: 4.9338 reg_loss: 0.9014 N_Y: 404242 N_S: 1754772 N: 404242 N_HV: 8504289 Val: 1.1284 Test: 1.1081\n",
    "Epoch: 037, Loss: 5.5254 tsm_loss: 4.6232 reg_loss: 0.9021 N_Y: 405728 N_S: 1754772 N: 405728 N_HV: 8503393 Val: 1.1233 Test: 1.1017\n",
    "Epoch: 038, Loss: 5.4919 tsm_loss: 4.5901 reg_loss: 0.9017 N_Y: 404674 N_S: 1754772 N: 404674 N_HV: 8560067 Val: 1.1220 Test: 1.0956\n",
    "Epoch: 039, Loss: 5.4316 tsm_loss: 4.5291 reg_loss: 0.9025 N_Y: 405837 N_S: 1754772 N: 405837 N_HV: 8387001 Val: 1.1150 Test: 1.0942\n",
    "Epoch: 040, Loss: 5.7299 tsm_loss: 4.8302 reg_loss: 0.8997 N_Y: 405549 N_S: 1754772 N: 405549 N_HV: 8875401 Val: 1.1238 Test: 1.0985\n",
    "Epoch: 041, Loss: 5.2737 tsm_loss: 4.3732 reg_loss: 0.9006 N_Y: 405118 N_S: 1754772 N: 405118 N_HV: 8189212 Val: 1.1351 Test: 1.1161\n",
    "Epoch: 042, Loss: 5.2150 tsm_loss: 4.3148 reg_loss: 0.9001 N_Y: 405764 N_S: 1754772 N: 405764 N_HV: 8563219 Val: 1.1400 Test: 1.1209\n",
    "Epoch: 043, Loss: 5.0776 tsm_loss: 4.1838 reg_loss: 0.8937 N_Y: 405348 N_S: 1754772 N: 405348 N_HV: 7983448 Val: 1.1168 Test: 1.0917\n",
    "Epoch: 044, Loss: 4.7990 tsm_loss: 3.8994 reg_loss: 0.8995 N_Y: 407445 N_S: 1754772 N: 407445 N_HV: 7964485 Val: 1.1129 Test: 1.0883\n",
    "Epoch: 045, Loss: 4.9075 tsm_loss: 4.0140 reg_loss: 0.8935 N_Y: 406010 N_S: 1754772 N: 406010 N_HV: 7723743 Val: 1.1200 Test: 1.0950\n",
    "Epoch: 046, Loss: 5.2888 tsm_loss: 4.3939 reg_loss: 0.8949 N_Y: 405659 N_S: 1754772 N: 405659 N_HV: 7869013 Val: 1.1254 Test: 1.1053\n",
    "Epoch: 047, Loss: 4.7228 tsm_loss: 3.8304 reg_loss: 0.8924 N_Y: 405203 N_S: 1754772 N: 405203 N_HV: 7922719 Val: 1.1160 Test: 1.0939\n",
    "Epoch: 048, Loss: 5.0686 tsm_loss: 4.1778 reg_loss: 0.8908 N_Y: 405611 N_S: 1754772 N: 405611 N_HV: 7804627 Val: 1.1285 Test: 1.1051\n",
    "Epoch: 049, Loss: 5.4764 tsm_loss: 4.5754 reg_loss: 0.9010 N_Y: 405033 N_S: 1754772 N: 405033 N_HV: 7758364 Val: 1.1090 Test: 1.0825\n",
    "Epoch: 050, Loss: 4.8427 tsm_loss: 3.9496 reg_loss: 0.8931 N_Y: 406347 N_S: 1754772 N: 406347 N_HV: 7836730 Val: 1.1088 Test: 1.0912\n",
    "Epoch: 051, Loss: 6.0015 tsm_loss: 5.1136 reg_loss: 0.8880 N_Y: 403858 N_S: 1754772 N: 403858 N_HV: 7827957 Val: 1.1098 Test: 1.0936\n",
    "Epoch: 052, Loss: 4.5031 tsm_loss: 3.6166 reg_loss: 0.8864 N_Y: 404991 N_S: 1754772 N: 404991 N_HV: 7557500 Val: 1.1216 Test: 1.1036\n",
    "Epoch: 053, Loss: 4.1690 tsm_loss: 3.2818 reg_loss: 0.8872 N_Y: 406386 N_S: 1754772 N: 406386 N_HV: 7361394 Val: 1.1151 Test: 1.0993\n",
    "Epoch: 054, Loss: 4.2005 tsm_loss: 3.3136 reg_loss: 0.8870 N_Y: 407190 N_S: 1754772 N: 407190 N_HV: 7581680 Val: 1.1131 Test: 1.0913\n",
    "Epoch: 055, Loss: 4.1414 tsm_loss: 3.2547 reg_loss: 0.8867 N_Y: 406508 N_S: 1754772 N: 406508 N_HV: 7392246 Val: 1.0949 Test: 1.0785\n",
    "Epoch: 056, Loss: 3.9785 tsm_loss: 3.0964 reg_loss: 0.8821 N_Y: 405172 N_S: 1754772 N: 405172 N_HV: 7411442 Val: 1.1030 Test: 1.0829\n",
    "Epoch: 057, Loss: 3.9716 tsm_loss: 3.0884 reg_loss: 0.8832 N_Y: 405448 N_S: 1754772 N: 405448 N_HV: 7276681 Val: 1.1121 Test: 1.0956\n",
    "Epoch: 058, Loss: 3.8341 tsm_loss: 2.9495 reg_loss: 0.8847 N_Y: 404553 N_S: 1754772 N: 404553 N_HV: 7351858 Val: 1.1026 Test: 1.0798\n",
    "Epoch: 059, Loss: 3.9141 tsm_loss: 3.0293 reg_loss: 0.8849 N_Y: 404818 N_S: 1754772 N: 404818 N_HV: 7196173 Val: 1.0992 Test: 1.0760\n",
    "Epoch: 060, Loss: 3.7666 tsm_loss: 2.8797 reg_loss: 0.8868 N_Y: 406508 N_S: 1754772 N: 406508 N_HV: 7107792 Val: 1.0976 Test: 1.0789\n",
    "Epoch: 061, Loss: 3.7562 tsm_loss: 2.8812 reg_loss: 0.8750 N_Y: 405609 N_S: 1754772 N: 405609 N_HV: 6948064 Val: 1.1016 Test: 1.0852\n",
    "Epoch: 062, Loss: 3.6771 tsm_loss: 2.7960 reg_loss: 0.8811 N_Y: 406004 N_S: 1754772 N: 406004 N_HV: 6879330 Val: 1.1153 Test: 1.0937\n",
    "Epoch: 063, Loss: 3.6846 tsm_loss: 2.8076 reg_loss: 0.8770 N_Y: 403566 N_S: 1754772 N: 403566 N_HV: 6745342 Val: 1.0949 Test: 1.0745\n",
    "Epoch: 064, Loss: 3.6120 tsm_loss: 2.7382 reg_loss: 0.8737 N_Y: 405540 N_S: 1754772 N: 405540 N_HV: 6806963 Val: 1.0945 Test: 1.0732\n",
    "Epoch: 065, Loss: 3.3043 tsm_loss: 2.4269 reg_loss: 0.8774 N_Y: 406625 N_S: 1754772 N: 406625 N_HV: 6541643 Val: 1.1026 Test: 1.0854\n",
    "Epoch: 066, Loss: 3.2799 tsm_loss: 2.4056 reg_loss: 0.8743 N_Y: 406068 N_S: 1754772 N: 406068 N_HV: 6460041 Val: 1.0964 Test: 1.0770\n",
    "Epoch: 067, Loss: 3.3141 tsm_loss: 2.4450 reg_loss: 0.8691 N_Y: 403444 N_S: 1754772 N: 403444 N_HV: 6132364 Val: 1.0963 Test: 1.0758\n",
    "Epoch: 068, Loss: 3.4450 tsm_loss: 2.5703 reg_loss: 0.8747 N_Y: 406253 N_S: 1754772 N: 406253 N_HV: 6913482 Val: 1.0909 Test: 1.0710\n",
    "Epoch: 069, Loss: 3.5357 tsm_loss: 2.6650 reg_loss: 0.8707 N_Y: 406930 N_S: 1754772 N: 406930 N_HV: 6820549 Val: 1.0964 Test: 1.0767\n",
    "Epoch: 070, Loss: 3.3889 tsm_loss: 2.5162 reg_loss: 0.8727 N_Y: 405928 N_S: 1754772 N: 405928 N_HV: 6831304 Val: 1.0841 Test: 1.0685\n",
    "Epoch: 071, Loss: 3.6323 tsm_loss: 2.7680 reg_loss: 0.8643 N_Y: 406425 N_S: 1754772 N: 406425 N_HV: 6619868 Val: 1.0882 Test: 1.0693\n",
    "Epoch: 072, Loss: 3.4390 tsm_loss: 2.5734 reg_loss: 0.8656 N_Y: 406235 N_S: 1754772 N: 406235 N_HV: 6371829 Val: 1.0924 Test: 1.0738\n",
    "Epoch: 073, Loss: 3.1731 tsm_loss: 2.3022 reg_loss: 0.8709 N_Y: 405429 N_S: 1754772 N: 405429 N_HV: 6284679 Val: 1.0823 Test: 1.0650\n",
    "Epoch: 074, Loss: 2.9572 tsm_loss: 2.0953 reg_loss: 0.8619 N_Y: 406853 N_S: 1754772 N: 406853 N_HV: 5786677 Val: 1.0866 Test: 1.0679\n",
    "Epoch: 075, Loss: 2.9429 tsm_loss: 2.0801 reg_loss: 0.8628 N_Y: 405148 N_S: 1754772 N: 405148 N_HV: 5852388 Val: 1.0804 Test: 1.0640\n",
    "Epoch: 076, Loss: 2.7417 tsm_loss: 1.8791 reg_loss: 0.8626 N_Y: 406006 N_S: 1754772 N: 406006 N_HV: 5746844 Val: 1.0776 Test: 1.0601\n",
    "Epoch: 077, Loss: 2.8950 tsm_loss: 2.0357 reg_loss: 0.8593 N_Y: 405756 N_S: 1754772 N: 405756 N_HV: 5978885 Val: 1.0871 Test: 1.0657\n",
    "Epoch: 078, Loss: 3.3632 tsm_loss: 2.5034 reg_loss: 0.8597 N_Y: 406061 N_S: 1754772 N: 406061 N_HV: 5958748 Val: 1.0750 Test: 1.0541\n",
    "Epoch: 079, Loss: 3.4763 tsm_loss: 2.6179 reg_loss: 0.8584 N_Y: 403735 N_S: 1754772 N: 403735 N_HV: 6411100 Val: 1.0778 Test: 1.0627\n",
    "Epoch: 080, Loss: 3.2669 tsm_loss: 2.4189 reg_loss: 0.8480 N_Y: 404921 N_S: 1754772 N: 404921 N_HV: 6530007 Val: 1.0674 Test: 1.0454\n",
    "Epoch: 081, Loss: 3.9032 tsm_loss: 3.0575 reg_loss: 0.8456 N_Y: 405339 N_S: 1754772 N: 405339 N_HV: 6429254 Val: 1.0797 Test: 1.0572\n",
    "Epoch: 082, Loss: 3.2930 tsm_loss: 2.4467 reg_loss: 0.8462 N_Y: 405544 N_S: 1754772 N: 405544 N_HV: 6567809 Val: 1.0734 Test: 1.0567\n",
    "Epoch: 083, Loss: 3.1284 tsm_loss: 2.2842 reg_loss: 0.8442 N_Y: 407783 N_S: 1754772 N: 407783 N_HV: 6277752 Val: 1.0614 Test: 1.0420\n",
    "Epoch: 084, Loss: 3.0763 tsm_loss: 2.2318 reg_loss: 0.8445 N_Y: 406373 N_S: 1754772 N: 406373 N_HV: 6274150 Val: 1.0687 Test: 1.0499\n",
    "Epoch: 085, Loss: 3.0601 tsm_loss: 2.2156 reg_loss: 0.8446 N_Y: 405215 N_S: 1754772 N: 405215 N_HV: 6167134 Val: 1.0606 Test: 1.0419\n",
    "Epoch: 086, Loss: 3.0572 tsm_loss: 2.2227 reg_loss: 0.8345 N_Y: 404274 N_S: 1754772 N: 404274 N_HV: 6036077 Val: 1.0566 Test: 1.0397\n",
    "Epoch: 087, Loss: 3.1218 tsm_loss: 2.2870 reg_loss: 0.8347 N_Y: 406593 N_S: 1754772 N: 406593 N_HV: 6155705 Val: 1.0503 Test: 1.0364\n",
    "Epoch: 088, Loss: 3.0674 tsm_loss: 2.2314 reg_loss: 0.8360 N_Y: 405461 N_S: 1754772 N: 405461 N_HV: 6177015 Val: 1.0586 Test: 1.0400\n",
    "Epoch: 089, Loss: 2.8340 tsm_loss: 2.0037 reg_loss: 0.8303 N_Y: 404562 N_S: 1754772 N: 404562 N_HV: 5878345 Val: 1.0536 Test: 1.0380\n",
    "Epoch: 090, Loss: 2.9095 tsm_loss: 2.0793 reg_loss: 0.8301 N_Y: 406961 N_S: 1754772 N: 406961 N_HV: 5501692 Val: 1.0663 Test: 1.0498\n",
    "Epoch: 091, Loss: 2.9963 tsm_loss: 2.1669 reg_loss: 0.8295 N_Y: 406019 N_S: 1754772 N: 406019 N_HV: 5634796 Val: 1.0420 Test: 1.0258\n",
    "Epoch: 092, Loss: 2.9043 tsm_loss: 2.0853 reg_loss: 0.8190 N_Y: 404786 N_S: 1754772 N: 404786 N_HV: 5959839 Val: 1.0664 Test: 1.0513\n",
    "Epoch: 093, Loss: 3.0049 tsm_loss: 2.1814 reg_loss: 0.8235 N_Y: 406252 N_S: 1754772 N: 406252 N_HV: 5756120 Val: 1.0485 Test: 1.0297\n",
    "Epoch: 094, Loss: 3.1208 tsm_loss: 2.3036 reg_loss: 0.8171 N_Y: 402504 N_S: 1754772 N: 402504 N_HV: 6025506 Val: 1.0442 Test: 1.0274\n",
    "Epoch: 095, Loss: 2.6919 tsm_loss: 1.8755 reg_loss: 0.8164 N_Y: 403756 N_S: 1754772 N: 403756 N_HV: 5698776 Val: 1.0387 Test: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59ad23-a5a4-4366-833d-001a20e59ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31c76b-0328-4014-8c25-a783ad1a785b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ad03-dece-4718-83cb-44d14caed116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552181a-a71b-41b8-ae71-851b5c36b042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe22b4-a6af-4415-a4d8-1789f3d88b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2396a6-fa70-4844-9eb8-1538eb903224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat(res1)\n",
    "df2 = pd.concat(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995c527-19cf-43d0-ada5-b151a620b906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6ac350-4eb4-4656-9b6e-d74343f97067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df1\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./with_aca.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./without_aca.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df1.to_csv('./with_aca.csv')\n",
    "df2.to_csv('./without_aca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c7cc2-eb12-4d33-8446-0b8e3b4cd53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a95e88-f850-48a7-8f0b-f0fe3ed48878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "colors = ['#FFE699','#00B0F0']\n",
    "\n",
    "\n",
    "y = 'val_rmse'\n",
    "\n",
    "n1 = r'With AC-awareness ($\\mathcal{L}_{mae} + \\mathcal{L}_{tsm}$)'\n",
    "n2 = r'Without AC-awareness ($\\mathcal{L}_{mae}$)'\n",
    "\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.60, 1.0)\n",
    "ax.set_ylabel('Validation RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Validation_RMSE.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Validation_RMSE.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae243a-9b9d-4001-baab-16b8750003a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110c566-a71d-4d5c-afe0-185322298de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ab1d9-b7b4-4ec4-bc0c-d46590478bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "colors = ['#FFE699','#00B0F0']\n",
    "\n",
    "\n",
    "y = 'test_rmse'\n",
    "\n",
    "\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.60, 1.0)\n",
    "ax.set_ylabel('Test RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Test_RMSE.svg' , bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Test_RMSE.pdf' , bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7e564-cf50-4d1f-b7d0-c9a7fa127e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "\n",
    "y = 'n_pos_triplets'\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 3, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.3)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.3)\n",
    "\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "ax.set_ylabel(\"No. of HV-ACTs ($M^'$)\")\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "ax.set_xlim(-5,800)\n",
    "\n",
    "\n",
    "fig.savefig('./Number_of_mined_ACTs_during_training.svg' , bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Number_of_mined_ACTs_during_training.pdf' , bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b16ad-c854-408d-9fcc-9ec0014dc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "y = 'train_triplet_loss'\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 3, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.3)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.3)\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.set_xlim(-5,800)\n",
    "ax.set_ylim(-1,10)\n",
    "\n",
    "ax.set_ylabel('Training TSM Loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Triplet_loss_during_training.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Triplet_loss_during_training.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27b9a-5db5-4c6d-a50f-018db4be7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y = 'train_mae_loss'\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2.5, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.3)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.3)\n",
    "\n",
    "ax.set_ylim(0.0, 0.8)\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_ylabel('Training MAE loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Train_mae_los.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Train_mae_los.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce63f4f-1de4-4398-97f2-3399b4236bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d35fb-de10-4bbd-9c8f-d7d8c1121f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c033353-8c60-4880-8267-9c9662862bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001780f-131e-4d1a-a9ab-9584c47b8ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbbcbb-8de0-4739-8922-513fd138bc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5503d97-dcda-4ca3-998b-50e3088ad13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe437b12-87f9-428e-b17e-25a2453c0c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124f1d3-317a-4d23-8ef8-aa94f27ec72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314788a9-afe4-486c-a861-9e1c5708f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f5326-f4d7-42ae-935c-c20f2df351a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8aa35-2eaa-4c90-8168-4d62ff358d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4f511-db6b-4395-b2bd-2e993b9167f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16898f-b4bb-46ae-8d18-229ad9ab42b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
