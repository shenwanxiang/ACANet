{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988ab225-f6f0-44b6-b24d-7d1374847765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "%matplotlib inline\n",
    "#A100 80GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f5cf08-0f69-4dc4-830d-ba271bd8ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "torch.cuda.set_device(gpuid)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a32cb66-5320-421b-9259-880db0f5aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/shenwanxiang/Research/bidd-clsar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde33631-dc5e-419f-b970-26e4b10535fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clsar.dataset import LSSNS, HSSMS\n",
    "from clsar.feature import Gen39AtomFeatures\n",
    "from clsar.model.model import ACANet_PNA, get_deg, _fix_reproducibility # model\n",
    "from clsar.model.loss import ACALoss, get_best_cliff, get_best_structure_batch\n",
    "_fix_reproducibility(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca7d324-4af1-4477-99c4-4b0a96157fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, aca_loss):\n",
    "\n",
    "    total_examples = 0\n",
    "    total_loss =  0    \n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0  \n",
    "    \n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index, \n",
    "                                        data.edge_attr, data.batch)\n",
    "        \n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                            fingerprints = data.fp)\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs        \n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs        \n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "    \n",
    "    train_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    return train_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_loader, model, aca_loss):\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0\n",
    "\n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "    \n",
    "    mse = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index,\n",
    "                                        data.edge_attr, data.batch)\n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                           fingerprints = data.fp)\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs\n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "\n",
    "        mse.append(F.mse_loss(predictions, data.y, reduction='none').cpu())\n",
    "\n",
    "    test_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    test_rmse = float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    \n",
    "    return test_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets, test_rmse\n",
    "\n",
    "\n",
    "\n",
    "def Test_performance(alpha=1.0, similarity_gate = True):\n",
    "    _fix_reproducibility(42)\n",
    "    model = ACANet_PNA(**pub_args, deg=deg).to(device)  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=10**-5)\n",
    "    aca_loss = ACALoss(alpha=alpha, \n",
    "                        cliff_lower = 1., \n",
    "                        cliff_upper = 1.,\n",
    "                        squared = False,\n",
    "                        similarity_gate = similarity_gate,\n",
    "                        similarity_neg = 0.7, #0.\n",
    "                        similarity_pos = 0.3, #1\n",
    "                        dev_mode = True,)\n",
    "    \n",
    "    history = []\n",
    "    #ls_his = []\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, tsm_loss, reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets = train(train_loader, model, optimizer, aca_loss)\n",
    "\n",
    "        _, _, _, _, _, _, train_n_hv_triplets, train_rmse = test(train_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, val_n_hv_triplets, val_rmse = test(val_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, test_n_hv_triplets, test_rmse = test(test_loader, model, aca_loss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f} tsm_loss: {tsm_loss:.4f} reg_loss: {reg_loss:.4f} '\n",
    "              f'N_Y: {n_label_triplets:03d} N_S: {n_structure_triplets:03d} N: {n_triplets:03d} N_HV: {n_hv_triplets:03d} '\n",
    "              f'Val: {val_rmse:.4f} Test: {test_rmse:.4f}')\n",
    "    \n",
    "        history.append({'Epoch':epoch, 'train_loss':train_loss, 'train_triplet_loss':tsm_loss,\n",
    "                        'train_reg_loss':reg_loss, 'val_rmse':val_rmse, \n",
    "                        'test_rmse':test_rmse, 'train_rmse':train_rmse,\n",
    "                        \n",
    "                        'n_label_triplets': n_label_triplets, \n",
    "                        'n_structure_triplets':n_structure_triplets,\n",
    "                        'n_triplets':n_triplets,\n",
    "                        'n_hv_triplets':n_hv_triplets,\n",
    "                        \n",
    "\n",
    "                        'train_n_hv_triplets':train_n_hv_triplets,\n",
    "                        'val_n_hv_triplets':val_n_hv_triplets,\n",
    "                        'test_n_hv_triplets':test_n_hv_triplets,\n",
    "                       \n",
    "                       })\n",
    "        #ls_his.append({'Epoch':epoch, 'mae_loss':float(mae_loss), 'triplet_loss':float(triplet_loss)})\n",
    "    dfh = pd.DataFrame(history)\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa1d63b-0567-406f-9f48-291959b18978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'CHEMBL3979_EC50'\n",
    "Dataset =  HSSMS #LSSNS \n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "\n",
    "pre_transform = Gen39AtomFeatures()\n",
    "in_channels = pre_transform.in_channels\n",
    "path = './data/'\n",
    "\n",
    "## model HPs\n",
    "pub_args = {'in_channels':pre_transform.in_channels, \n",
    "            'edge_dim':pre_transform.edge_dim,\n",
    "            'convs_layers': [64, 128, 256, 512],   \n",
    "            'dense_layers': [256, 128, 32], \n",
    "            'out_channels':1, \n",
    "            'aggregators': ['mean', 'min', 'max', 'sum','std'],\n",
    "            'scalers':['identity', 'amplification', 'attenuation'] ,\n",
    "            'dropout_p': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7b5051-3408-450a-9901-79b1042719d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6da88-ccba-4731-a70b-f53c5c006652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 6.9195 tsm_loss: 0.2143 reg_loss: 6.7053 N_Y: 407843 N_S: 7747 N: 3610 N_HV: 067 Val: 6.9638 Test: 6.9844\n",
      "Epoch: 002, Loss: 6.0050 tsm_loss: 0.0032 reg_loss: 6.0018 N_Y: 403186 N_S: 8258 N: 3619 N_HV: 002 Val: 6.9629 Test: 6.9835\n",
      "Epoch: 003, Loss: 5.0016 tsm_loss: 0.0175 reg_loss: 4.9841 N_Y: 405580 N_S: 9400 N: 3972 N_HV: 009 Val: 6.9588 Test: 6.9795\n",
      "Epoch: 004, Loss: 3.5505 tsm_loss: 0.0089 reg_loss: 3.5416 N_Y: 406000 N_S: 8574 N: 4098 N_HV: 009 Val: 6.9444 Test: 6.9654\n",
      "Epoch: 005, Loss: 1.6734 tsm_loss: 0.0022 reg_loss: 1.6712 N_Y: 406169 N_S: 9669 N: 4128 N_HV: 003 Val: 6.8994 Test: 6.9208\n",
      "Epoch: 006, Loss: 1.0096 tsm_loss: 0.0126 reg_loss: 0.9970 N_Y: 404414 N_S: 9116 N: 3678 N_HV: 009 Val: 6.4842 Test: 6.5055\n",
      "Epoch: 007, Loss: 0.8003 tsm_loss: 0.0370 reg_loss: 0.7633 N_Y: 405844 N_S: 8705 N: 3888 N_HV: 026 Val: 5.5792 Test: 5.5939\n",
      "Epoch: 008, Loss: 0.7028 tsm_loss: 0.0111 reg_loss: 0.6916 N_Y: 406738 N_S: 7717 N: 3442 N_HV: 008 Val: 4.6534 Test: 4.6617\n",
      "Epoch: 009, Loss: 0.6468 tsm_loss: 0.0030 reg_loss: 0.6437 N_Y: 405450 N_S: 8639 N: 3581 N_HV: 002 Val: 3.6705 Test: 3.6850\n",
      "Epoch: 010, Loss: 0.6067 tsm_loss: 0.0039 reg_loss: 0.6029 N_Y: 406218 N_S: 8948 N: 4024 N_HV: 004 Val: 2.9584 Test: 2.9703\n",
      "Epoch: 011, Loss: 0.6051 tsm_loss: 0.0201 reg_loss: 0.5850 N_Y: 405649 N_S: 8591 N: 3884 N_HV: 016 Val: 1.6414 Test: 1.6453\n",
      "Epoch: 012, Loss: 0.5828 tsm_loss: 0.0026 reg_loss: 0.5802 N_Y: 406217 N_S: 8377 N: 3594 N_HV: 003 Val: 1.4087 Test: 1.3897\n",
      "Epoch: 013, Loss: 0.5502 tsm_loss: 0.0033 reg_loss: 0.5470 N_Y: 406272 N_S: 8704 N: 4124 N_HV: 003 Val: 1.1900 Test: 1.1957\n",
      "Epoch: 014, Loss: 0.5789 tsm_loss: 0.0009 reg_loss: 0.5780 N_Y: 408158 N_S: 7794 N: 3523 N_HV: 001 Val: 1.3489 Test: 1.3584\n",
      "Epoch: 015, Loss: 0.5331 tsm_loss: 0.0013 reg_loss: 0.5318 N_Y: 406436 N_S: 9261 N: 4286 N_HV: 002 Val: 0.8122 Test: 0.7722\n",
      "Epoch: 016, Loss: 0.5121 tsm_loss: 0.0004 reg_loss: 0.5117 N_Y: 404438 N_S: 8364 N: 3735 N_HV: 000 Val: 0.8288 Test: 0.8114\n",
      "Epoch: 017, Loss: 0.4934 tsm_loss: 0.0021 reg_loss: 0.4914 N_Y: 406418 N_S: 9039 N: 4159 N_HV: 002 Val: 0.7611 Test: 0.7331\n",
      "Epoch: 018, Loss: 0.4785 tsm_loss: 0.0007 reg_loss: 0.4778 N_Y: 404922 N_S: 8961 N: 4047 N_HV: 001 Val: 0.7525 Test: 0.7407\n",
      "Epoch: 019, Loss: 0.4939 tsm_loss: 0.0003 reg_loss: 0.4936 N_Y: 405618 N_S: 8599 N: 3862 N_HV: 000 Val: 0.9581 Test: 0.9512\n",
      "Epoch: 020, Loss: 0.4801 tsm_loss: 0.0009 reg_loss: 0.4793 N_Y: 407960 N_S: 8786 N: 3528 N_HV: 002 Val: 1.0534 Test: 1.0221\n",
      "Epoch: 021, Loss: 0.4752 tsm_loss: 0.0008 reg_loss: 0.4745 N_Y: 405658 N_S: 8341 N: 3595 N_HV: 001 Val: 0.7981 Test: 0.7660\n",
      "Epoch: 022, Loss: 0.4874 tsm_loss: 0.0012 reg_loss: 0.4862 N_Y: 405801 N_S: 8013 N: 3236 N_HV: 000 Val: 0.7077 Test: 0.7556\n",
      "Epoch: 023, Loss: 0.4590 tsm_loss: 0.0004 reg_loss: 0.4586 N_Y: 406015 N_S: 8307 N: 3654 N_HV: 000 Val: 0.7557 Test: 0.7354\n",
      "Epoch: 024, Loss: 0.4468 tsm_loss: 0.0004 reg_loss: 0.4464 N_Y: 407094 N_S: 8918 N: 3921 N_HV: 000 Val: 0.7457 Test: 0.7578\n",
      "Epoch: 025, Loss: 0.4279 tsm_loss: 0.0020 reg_loss: 0.4259 N_Y: 406662 N_S: 8248 N: 3521 N_HV: 002 Val: 0.7189 Test: 0.7078\n",
      "Epoch: 026, Loss: 0.4203 tsm_loss: 0.0014 reg_loss: 0.4189 N_Y: 407087 N_S: 8467 N: 3686 N_HV: 000 Val: 0.7379 Test: 0.6916\n",
      "Epoch: 027, Loss: 0.4509 tsm_loss: 0.0003 reg_loss: 0.4506 N_Y: 407243 N_S: 9100 N: 3931 N_HV: 000 Val: 0.7872 Test: 0.7355\n",
      "Epoch: 028, Loss: 0.4203 tsm_loss: 0.0002 reg_loss: 0.4201 N_Y: 406722 N_S: 8628 N: 3848 N_HV: 000 Val: 0.7829 Test: 0.7270\n",
      "Epoch: 029, Loss: 0.4364 tsm_loss: 0.0003 reg_loss: 0.4360 N_Y: 403434 N_S: 8709 N: 3714 N_HV: 000 Val: 0.7915 Test: 0.7487\n",
      "Epoch: 030, Loss: 0.4461 tsm_loss: 0.0000 reg_loss: 0.4461 N_Y: 405728 N_S: 7615 N: 3236 N_HV: 000 Val: 0.7964 Test: 0.7892\n",
      "Epoch: 031, Loss: 0.4101 tsm_loss: 0.0011 reg_loss: 0.4089 N_Y: 405494 N_S: 8719 N: 3687 N_HV: 001 Val: 0.8601 Test: 0.7760\n",
      "Epoch: 032, Loss: 0.4147 tsm_loss: 0.0002 reg_loss: 0.4145 N_Y: 407319 N_S: 9680 N: 4190 N_HV: 000 Val: 0.8989 Test: 0.9008\n",
      "Epoch: 033, Loss: 0.4107 tsm_loss: 0.0008 reg_loss: 0.4099 N_Y: 404776 N_S: 8381 N: 3664 N_HV: 000 Val: 0.8051 Test: 0.7561\n",
      "Epoch: 034, Loss: 0.4145 tsm_loss: 0.0004 reg_loss: 0.4141 N_Y: 406471 N_S: 7799 N: 3469 N_HV: 000 Val: 0.8469 Test: 0.7736\n",
      "Epoch: 035, Loss: 0.4061 tsm_loss: 0.0004 reg_loss: 0.4057 N_Y: 405258 N_S: 8793 N: 3953 N_HV: 000 Val: 0.7841 Test: 0.7592\n",
      "Epoch: 036, Loss: 0.3717 tsm_loss: 0.0002 reg_loss: 0.3715 N_Y: 403624 N_S: 7896 N: 3522 N_HV: 000 Val: 0.7607 Test: 0.7326\n",
      "Epoch: 037, Loss: 0.3796 tsm_loss: 0.0000 reg_loss: 0.3796 N_Y: 406362 N_S: 8777 N: 3904 N_HV: 000 Val: 0.7877 Test: 0.7022\n",
      "Epoch: 038, Loss: 0.3849 tsm_loss: 0.0006 reg_loss: 0.3843 N_Y: 406074 N_S: 8459 N: 3423 N_HV: 001 Val: 0.7388 Test: 0.7035\n",
      "Epoch: 039, Loss: 0.3793 tsm_loss: 0.0004 reg_loss: 0.3790 N_Y: 406788 N_S: 8345 N: 3520 N_HV: 000 Val: 0.7487 Test: 0.7258\n",
      "Epoch: 040, Loss: 0.3685 tsm_loss: 0.0000 reg_loss: 0.3685 N_Y: 406732 N_S: 7960 N: 3408 N_HV: 000 Val: 0.7632 Test: 0.6886\n",
      "Epoch: 041, Loss: 0.3718 tsm_loss: 0.0003 reg_loss: 0.3715 N_Y: 405878 N_S: 8255 N: 3854 N_HV: 000 Val: 0.8425 Test: 0.7708\n",
      "Epoch: 042, Loss: 0.4230 tsm_loss: 0.0000 reg_loss: 0.4230 N_Y: 406512 N_S: 8529 N: 3877 N_HV: 000 Val: 0.9795 Test: 0.9729\n",
      "Epoch: 043, Loss: 0.4533 tsm_loss: 0.0001 reg_loss: 0.4532 N_Y: 405416 N_S: 8626 N: 3687 N_HV: 000 Val: 1.3097 Test: 1.2531\n",
      "Epoch: 044, Loss: 0.4625 tsm_loss: 0.0001 reg_loss: 0.4624 N_Y: 407704 N_S: 7973 N: 3378 N_HV: 000 Val: 0.7376 Test: 0.6575\n",
      "Epoch: 045, Loss: 0.4208 tsm_loss: 0.0004 reg_loss: 0.4204 N_Y: 406899 N_S: 8262 N: 3667 N_HV: 000 Val: 1.0609 Test: 1.0152\n",
      "Epoch: 046, Loss: 0.3929 tsm_loss: 0.0000 reg_loss: 0.3929 N_Y: 406169 N_S: 8046 N: 3450 N_HV: 000 Val: 0.7483 Test: 0.7190\n",
      "Epoch: 047, Loss: 0.4117 tsm_loss: 0.0001 reg_loss: 0.4116 N_Y: 407002 N_S: 8979 N: 4140 N_HV: 000 Val: 0.7663 Test: 0.7169\n",
      "Epoch: 048, Loss: 0.3694 tsm_loss: 0.0007 reg_loss: 0.3687 N_Y: 406284 N_S: 9085 N: 4315 N_HV: 000 Val: 0.7537 Test: 0.6923\n",
      "Epoch: 049, Loss: 0.3667 tsm_loss: 0.0000 reg_loss: 0.3667 N_Y: 406758 N_S: 9141 N: 4039 N_HV: 000 Val: 0.9094 Test: 0.8964\n",
      "Epoch: 050, Loss: 0.3762 tsm_loss: 0.0025 reg_loss: 0.3737 N_Y: 405055 N_S: 7712 N: 3326 N_HV: 002 Val: 0.8312 Test: 0.8048\n",
      "Epoch: 051, Loss: 0.3629 tsm_loss: 0.0000 reg_loss: 0.3629 N_Y: 405849 N_S: 8890 N: 3992 N_HV: 000 Val: 0.7872 Test: 0.7151\n",
      "Epoch: 052, Loss: 0.3744 tsm_loss: 0.0004 reg_loss: 0.3741 N_Y: 407089 N_S: 9414 N: 4287 N_HV: 001 Val: 0.8596 Test: 0.8295\n",
      "Epoch: 053, Loss: 0.3833 tsm_loss: 0.0023 reg_loss: 0.3810 N_Y: 407550 N_S: 9465 N: 4186 N_HV: 002 Val: 0.8055 Test: 0.7575\n",
      "Epoch: 054, Loss: 0.4032 tsm_loss: 0.0000 reg_loss: 0.4032 N_Y: 403778 N_S: 7861 N: 3351 N_HV: 000 Val: 0.7784 Test: 0.7415\n",
      "Epoch: 055, Loss: 0.3550 tsm_loss: 0.0000 reg_loss: 0.3550 N_Y: 404629 N_S: 8869 N: 3831 N_HV: 000 Val: 0.7904 Test: 0.7143\n",
      "Epoch: 056, Loss: 0.3515 tsm_loss: 0.0001 reg_loss: 0.3514 N_Y: 404189 N_S: 8016 N: 3798 N_HV: 000 Val: 0.7772 Test: 0.6989\n",
      "Epoch: 057, Loss: 0.3708 tsm_loss: 0.0000 reg_loss: 0.3708 N_Y: 406270 N_S: 7759 N: 3422 N_HV: 000 Val: 0.7642 Test: 0.7400\n",
      "Epoch: 058, Loss: 0.3373 tsm_loss: 0.0005 reg_loss: 0.3368 N_Y: 407048 N_S: 8383 N: 3639 N_HV: 000 Val: 0.7968 Test: 0.7181\n",
      "Epoch: 059, Loss: 0.3474 tsm_loss: 0.0000 reg_loss: 0.3474 N_Y: 405896 N_S: 8243 N: 3614 N_HV: 000 Val: 0.7415 Test: 0.6876\n",
      "Epoch: 060, Loss: 0.3378 tsm_loss: 0.0004 reg_loss: 0.3375 N_Y: 405965 N_S: 8257 N: 3641 N_HV: 000 Val: 0.7917 Test: 0.7306\n",
      "Epoch: 061, Loss: 0.3218 tsm_loss: 0.0001 reg_loss: 0.3217 N_Y: 404825 N_S: 8456 N: 3632 N_HV: 000 Val: 0.7560 Test: 0.6817\n",
      "Epoch: 062, Loss: 0.3363 tsm_loss: 0.0000 reg_loss: 0.3363 N_Y: 407083 N_S: 9179 N: 3779 N_HV: 000 Val: 0.7555 Test: 0.6955\n",
      "Epoch: 063, Loss: 0.3594 tsm_loss: 0.0001 reg_loss: 0.3593 N_Y: 406740 N_S: 7841 N: 3271 N_HV: 000 Val: 0.7587 Test: 0.7058\n",
      "Epoch: 064, Loss: 0.3738 tsm_loss: 0.0013 reg_loss: 0.3725 N_Y: 403919 N_S: 8867 N: 3823 N_HV: 002 Val: 0.9251 Test: 0.8771\n",
      "Epoch: 065, Loss: 0.3343 tsm_loss: 0.0017 reg_loss: 0.3326 N_Y: 405852 N_S: 8378 N: 3747 N_HV: 002 Val: 0.7911 Test: 0.7399\n",
      "Epoch: 066, Loss: 0.3170 tsm_loss: 0.0000 reg_loss: 0.3170 N_Y: 407418 N_S: 8225 N: 3781 N_HV: 000 Val: 0.7501 Test: 0.7167\n",
      "Epoch: 067, Loss: 0.3204 tsm_loss: 0.0000 reg_loss: 0.3204 N_Y: 406228 N_S: 8273 N: 3557 N_HV: 000 Val: 0.9775 Test: 0.9557\n",
      "Epoch: 068, Loss: 0.3065 tsm_loss: 0.0006 reg_loss: 0.3058 N_Y: 405977 N_S: 8532 N: 3730 N_HV: 001 Val: 0.7599 Test: 0.7174\n",
      "Epoch: 069, Loss: 0.3449 tsm_loss: 0.0005 reg_loss: 0.3444 N_Y: 407387 N_S: 9040 N: 4016 N_HV: 000 Val: 0.9100 Test: 0.8773\n",
      "Epoch: 070, Loss: 0.3394 tsm_loss: 0.0001 reg_loss: 0.3393 N_Y: 406059 N_S: 8340 N: 3629 N_HV: 000 Val: 0.9544 Test: 0.9544\n",
      "Epoch: 071, Loss: 0.3713 tsm_loss: 0.0010 reg_loss: 0.3703 N_Y: 405856 N_S: 8670 N: 3632 N_HV: 002 Val: 0.8018 Test: 0.7164\n",
      "Epoch: 072, Loss: 0.3439 tsm_loss: 0.0012 reg_loss: 0.3427 N_Y: 405569 N_S: 9477 N: 4220 N_HV: 002 Val: 0.8085 Test: 0.7907\n",
      "Epoch: 073, Loss: 0.3497 tsm_loss: 0.0001 reg_loss: 0.3496 N_Y: 406456 N_S: 8826 N: 3759 N_HV: 000 Val: 0.7473 Test: 0.6926\n",
      "Epoch: 074, Loss: 0.3352 tsm_loss: 0.0001 reg_loss: 0.3351 N_Y: 406829 N_S: 8588 N: 4112 N_HV: 000 Val: 0.9271 Test: 0.8480\n",
      "Epoch: 075, Loss: 0.3296 tsm_loss: 0.0001 reg_loss: 0.3295 N_Y: 405118 N_S: 7662 N: 3208 N_HV: 000 Val: 0.9419 Test: 0.9111\n",
      "Epoch: 076, Loss: 0.3789 tsm_loss: 0.0001 reg_loss: 0.3788 N_Y: 406922 N_S: 8182 N: 3664 N_HV: 000 Val: 0.7663 Test: 0.7075\n",
      "Epoch: 077, Loss: 0.3312 tsm_loss: 0.0005 reg_loss: 0.3307 N_Y: 406491 N_S: 8555 N: 3575 N_HV: 000 Val: 0.7575 Test: 0.7064\n",
      "Epoch: 078, Loss: 0.3202 tsm_loss: 0.0002 reg_loss: 0.3200 N_Y: 404984 N_S: 9075 N: 3970 N_HV: 000 Val: 0.9295 Test: 0.9145\n",
      "Epoch: 079, Loss: 0.3124 tsm_loss: 0.0001 reg_loss: 0.3123 N_Y: 404318 N_S: 8341 N: 3502 N_HV: 000 Val: 0.7931 Test: 0.7328\n",
      "Epoch: 080, Loss: 0.3076 tsm_loss: 0.0001 reg_loss: 0.3075 N_Y: 407036 N_S: 10009 N: 4218 N_HV: 000 Val: 0.7346 Test: 0.6782\n",
      "Epoch: 081, Loss: 0.3066 tsm_loss: 0.0002 reg_loss: 0.3063 N_Y: 406158 N_S: 8615 N: 3856 N_HV: 000 Val: 0.8156 Test: 0.7955\n",
      "Epoch: 082, Loss: 0.2905 tsm_loss: 0.0000 reg_loss: 0.2905 N_Y: 405766 N_S: 8799 N: 3673 N_HV: 000 Val: 0.7539 Test: 0.6846\n",
      "Epoch: 083, Loss: 0.3065 tsm_loss: 0.0000 reg_loss: 0.3065 N_Y: 405193 N_S: 7801 N: 3620 N_HV: 000 Val: 0.7388 Test: 0.6856\n",
      "Epoch: 084, Loss: 0.2730 tsm_loss: 0.0000 reg_loss: 0.2730 N_Y: 406090 N_S: 8959 N: 4065 N_HV: 000 Val: 0.8321 Test: 0.8056\n",
      "Epoch: 085, Loss: 0.2858 tsm_loss: 0.0004 reg_loss: 0.2855 N_Y: 406434 N_S: 8639 N: 3940 N_HV: 001 Val: 0.7589 Test: 0.7212\n",
      "Epoch: 086, Loss: 0.2866 tsm_loss: 0.0000 reg_loss: 0.2866 N_Y: 407006 N_S: 8285 N: 3433 N_HV: 000 Val: 0.7499 Test: 0.6808\n",
      "Epoch: 087, Loss: 0.2697 tsm_loss: 0.0009 reg_loss: 0.2688 N_Y: 405222 N_S: 8426 N: 3520 N_HV: 001 Val: 0.7699 Test: 0.7036\n",
      "Epoch: 088, Loss: 0.2717 tsm_loss: 0.0002 reg_loss: 0.2715 N_Y: 404675 N_S: 8042 N: 3490 N_HV: 000 Val: 0.7403 Test: 0.6855\n",
      "Epoch: 089, Loss: 0.2689 tsm_loss: 0.0002 reg_loss: 0.2686 N_Y: 406870 N_S: 8893 N: 4127 N_HV: 000 Val: 0.7518 Test: 0.6883\n",
      "Epoch: 090, Loss: 0.2871 tsm_loss: 0.0000 reg_loss: 0.2871 N_Y: 407519 N_S: 8240 N: 3469 N_HV: 000 Val: 0.7638 Test: 0.6899\n",
      "Epoch: 091, Loss: 0.2955 tsm_loss: 0.0003 reg_loss: 0.2953 N_Y: 404021 N_S: 8345 N: 3762 N_HV: 000 Val: 0.7789 Test: 0.6633\n",
      "Epoch: 092, Loss: 0.2941 tsm_loss: 0.0001 reg_loss: 0.2940 N_Y: 405637 N_S: 8851 N: 3920 N_HV: 000 Val: 0.7376 Test: 0.6969\n",
      "Epoch: 093, Loss: 0.3172 tsm_loss: 0.0003 reg_loss: 0.3169 N_Y: 404801 N_S: 8680 N: 3461 N_HV: 000 Val: 0.7490 Test: 0.6874\n",
      "Epoch: 094, Loss: 0.2981 tsm_loss: 0.0000 reg_loss: 0.2981 N_Y: 407779 N_S: 8257 N: 3598 N_HV: 000 Val: 0.7488 Test: 0.6871\n",
      "Epoch: 095, Loss: 0.2945 tsm_loss: 0.0000 reg_loss: 0.2945 N_Y: 406181 N_S: 8018 N: 3394 N_HV: 000 Val: 0.7689 Test: 0.7688\n",
      "Epoch: 096, Loss: 0.3058 tsm_loss: 0.0000 reg_loss: 0.3058 N_Y: 406479 N_S: 8901 N: 3924 N_HV: 000 Val: 0.8278 Test: 0.7719\n",
      "Epoch: 097, Loss: 0.2889 tsm_loss: 0.0003 reg_loss: 0.2886 N_Y: 407406 N_S: 9300 N: 4148 N_HV: 001 Val: 0.8257 Test: 0.7547\n",
      "Epoch: 098, Loss: 0.2917 tsm_loss: 0.0003 reg_loss: 0.2913 N_Y: 406066 N_S: 8630 N: 3517 N_HV: 001 Val: 0.9535 Test: 0.9436\n",
      "Epoch: 099, Loss: 0.3250 tsm_loss: 0.0000 reg_loss: 0.3250 N_Y: 404909 N_S: 8601 N: 3707 N_HV: 000 Val: 0.7706 Test: 0.7154\n",
      "Epoch: 100, Loss: 0.2880 tsm_loss: 0.0002 reg_loss: 0.2878 N_Y: 405644 N_S: 8687 N: 3855 N_HV: 000 Val: 0.7232 Test: 0.6637\n",
      "Epoch: 101, Loss: 0.2766 tsm_loss: 0.0000 reg_loss: 0.2766 N_Y: 406463 N_S: 8525 N: 3563 N_HV: 000 Val: 0.7198 Test: 0.6809\n",
      "Epoch: 102, Loss: 0.2651 tsm_loss: 0.0000 reg_loss: 0.2651 N_Y: 406729 N_S: 8817 N: 3937 N_HV: 000 Val: 0.8793 Test: 0.8645\n",
      "Epoch: 103, Loss: 0.2780 tsm_loss: 0.0000 reg_loss: 0.2780 N_Y: 406258 N_S: 8163 N: 3339 N_HV: 000 Val: 0.7472 Test: 0.6759\n",
      "Epoch: 104, Loss: 0.2943 tsm_loss: 0.0000 reg_loss: 0.2942 N_Y: 406326 N_S: 9154 N: 4284 N_HV: 000 Val: 0.7481 Test: 0.7004\n",
      "Epoch: 105, Loss: 0.2718 tsm_loss: 0.0000 reg_loss: 0.2718 N_Y: 406876 N_S: 8515 N: 3598 N_HV: 000 Val: 0.7622 Test: 0.7025\n",
      "Epoch: 106, Loss: 0.2624 tsm_loss: 0.0011 reg_loss: 0.2613 N_Y: 406131 N_S: 8331 N: 3676 N_HV: 001 Val: 0.7646 Test: 0.7209\n",
      "Epoch: 107, Loss: 0.2368 tsm_loss: 0.0000 reg_loss: 0.2368 N_Y: 406707 N_S: 8819 N: 3797 N_HV: 000 Val: 0.7303 Test: 0.6562\n",
      "Epoch: 108, Loss: 0.2613 tsm_loss: 0.0000 reg_loss: 0.2613 N_Y: 406186 N_S: 8678 N: 3617 N_HV: 000 Val: 0.7164 Test: 0.6621\n",
      "Epoch: 109, Loss: 0.2403 tsm_loss: 0.0000 reg_loss: 0.2403 N_Y: 405316 N_S: 8546 N: 3699 N_HV: 000 Val: 0.7166 Test: 0.6608\n",
      "Epoch: 110, Loss: 0.2500 tsm_loss: 0.0000 reg_loss: 0.2500 N_Y: 404106 N_S: 9111 N: 3993 N_HV: 000 Val: 0.7252 Test: 0.6624\n",
      "Epoch: 111, Loss: 0.2512 tsm_loss: 0.0000 reg_loss: 0.2512 N_Y: 406924 N_S: 9176 N: 4301 N_HV: 000 Val: 0.7949 Test: 0.7264\n",
      "Epoch: 112, Loss: 0.2430 tsm_loss: 0.0000 reg_loss: 0.2430 N_Y: 405292 N_S: 8815 N: 3972 N_HV: 000 Val: 0.8406 Test: 0.7898\n",
      "Epoch: 113, Loss: 0.2571 tsm_loss: 0.0000 reg_loss: 0.2571 N_Y: 405187 N_S: 7928 N: 3210 N_HV: 000 Val: 0.7494 Test: 0.6938\n",
      "Epoch: 114, Loss: 0.2371 tsm_loss: 0.0001 reg_loss: 0.2370 N_Y: 407354 N_S: 9188 N: 3829 N_HV: 000 Val: 0.7364 Test: 0.6609\n",
      "Epoch: 115, Loss: 0.2575 tsm_loss: 0.0017 reg_loss: 0.2558 N_Y: 404537 N_S: 8701 N: 3670 N_HV: 002 Val: 0.7209 Test: 0.6818\n",
      "Epoch: 116, Loss: 0.2662 tsm_loss: 0.0002 reg_loss: 0.2660 N_Y: 405245 N_S: 9151 N: 4019 N_HV: 000 Val: 0.8250 Test: 0.7577\n",
      "Epoch: 117, Loss: 0.2874 tsm_loss: 0.0000 reg_loss: 0.2874 N_Y: 403369 N_S: 9113 N: 4086 N_HV: 000 Val: 0.7380 Test: 0.6600\n",
      "Epoch: 118, Loss: 0.2688 tsm_loss: 0.0006 reg_loss: 0.2682 N_Y: 406939 N_S: 9069 N: 4247 N_HV: 001 Val: 0.8361 Test: 0.7787\n",
      "Epoch: 119, Loss: 0.2401 tsm_loss: 0.0001 reg_loss: 0.2400 N_Y: 405775 N_S: 7934 N: 3597 N_HV: 000 Val: 0.7385 Test: 0.6658\n",
      "Epoch: 120, Loss: 0.2333 tsm_loss: 0.0003 reg_loss: 0.2331 N_Y: 405718 N_S: 8505 N: 3446 N_HV: 000 Val: 0.7494 Test: 0.6751\n",
      "Epoch: 121, Loss: 0.2191 tsm_loss: 0.0000 reg_loss: 0.2191 N_Y: 407439 N_S: 8842 N: 3808 N_HV: 000 Val: 0.8356 Test: 0.7925\n",
      "Epoch: 122, Loss: 0.2467 tsm_loss: 0.0000 reg_loss: 0.2466 N_Y: 406141 N_S: 7760 N: 3523 N_HV: 000 Val: 0.7597 Test: 0.6640\n",
      "Epoch: 123, Loss: 0.2353 tsm_loss: 0.0002 reg_loss: 0.2351 N_Y: 406872 N_S: 8873 N: 3958 N_HV: 000 Val: 0.7385 Test: 0.7043\n",
      "Epoch: 124, Loss: 0.2341 tsm_loss: 0.0011 reg_loss: 0.2330 N_Y: 407102 N_S: 8756 N: 3795 N_HV: 001 Val: 0.8855 Test: 0.8300\n",
      "Epoch: 125, Loss: 0.2584 tsm_loss: 0.0001 reg_loss: 0.2582 N_Y: 406330 N_S: 8246 N: 3364 N_HV: 000 Val: 0.7785 Test: 0.7176\n",
      "Epoch: 126, Loss: 0.2540 tsm_loss: 0.0003 reg_loss: 0.2537 N_Y: 404927 N_S: 8640 N: 3741 N_HV: 001 Val: 0.7144 Test: 0.6589\n",
      "Epoch: 127, Loss: 0.2427 tsm_loss: 0.0055 reg_loss: 0.2372 N_Y: 406536 N_S: 9111 N: 4025 N_HV: 008 Val: 0.7347 Test: 0.7076\n",
      "Epoch: 128, Loss: 0.2830 tsm_loss: 0.0000 reg_loss: 0.2830 N_Y: 406174 N_S: 8485 N: 3653 N_HV: 000 Val: 0.7960 Test: 0.7125\n",
      "Epoch: 129, Loss: 0.2614 tsm_loss: 0.0004 reg_loss: 0.2610 N_Y: 405806 N_S: 8263 N: 3807 N_HV: 000 Val: 0.7392 Test: 0.6844\n",
      "Epoch: 130, Loss: 0.2497 tsm_loss: 0.0004 reg_loss: 0.2493 N_Y: 406162 N_S: 8228 N: 3631 N_HV: 000 Val: 0.7415 Test: 0.6692\n",
      "Epoch: 131, Loss: 0.2344 tsm_loss: 0.0000 reg_loss: 0.2344 N_Y: 406333 N_S: 8172 N: 3474 N_HV: 000 Val: 0.7726 Test: 0.7169\n",
      "Epoch: 132, Loss: 0.2220 tsm_loss: 0.0000 reg_loss: 0.2220 N_Y: 406173 N_S: 8563 N: 3882 N_HV: 000 Val: 0.7350 Test: 0.6865\n",
      "Epoch: 133, Loss: 0.2300 tsm_loss: 0.0000 reg_loss: 0.2299 N_Y: 406469 N_S: 8093 N: 3483 N_HV: 000 Val: 0.7428 Test: 0.6645\n",
      "Epoch: 134, Loss: 0.3051 tsm_loss: 0.0000 reg_loss: 0.3051 N_Y: 407921 N_S: 8588 N: 3893 N_HV: 000 Val: 0.9211 Test: 0.8811\n",
      "Epoch: 135, Loss: 0.2409 tsm_loss: 0.0021 reg_loss: 0.2387 N_Y: 406910 N_S: 8729 N: 3867 N_HV: 004 Val: 0.7618 Test: 0.6630\n",
      "Epoch: 136, Loss: 0.2726 tsm_loss: 0.0000 reg_loss: 0.2726 N_Y: 405676 N_S: 9427 N: 3962 N_HV: 000 Val: 0.7741 Test: 0.7103\n",
      "Epoch: 137, Loss: 0.2287 tsm_loss: 0.0000 reg_loss: 0.2287 N_Y: 405759 N_S: 8890 N: 3956 N_HV: 000 Val: 0.7320 Test: 0.6866\n",
      "Epoch: 138, Loss: 0.2345 tsm_loss: 0.0000 reg_loss: 0.2345 N_Y: 405236 N_S: 8478 N: 3627 N_HV: 000 Val: 0.7628 Test: 0.7250\n",
      "Epoch: 139, Loss: 0.2514 tsm_loss: 0.0088 reg_loss: 0.2426 N_Y: 406129 N_S: 8399 N: 3631 N_HV: 006 Val: 0.7543 Test: 0.6886\n",
      "Epoch: 140, Loss: 0.3319 tsm_loss: 0.0000 reg_loss: 0.3319 N_Y: 404875 N_S: 8460 N: 3904 N_HV: 000 Val: 0.7784 Test: 0.7265\n",
      "Epoch: 141, Loss: 0.2802 tsm_loss: 0.0012 reg_loss: 0.2790 N_Y: 405828 N_S: 8205 N: 3683 N_HV: 001 Val: 0.8475 Test: 0.7941\n",
      "Epoch: 142, Loss: 0.3157 tsm_loss: 0.0003 reg_loss: 0.3155 N_Y: 404139 N_S: 8703 N: 3871 N_HV: 000 Val: 0.7422 Test: 0.6833\n",
      "Epoch: 143, Loss: 0.2671 tsm_loss: 0.0000 reg_loss: 0.2671 N_Y: 406562 N_S: 8850 N: 4068 N_HV: 000 Val: 0.7356 Test: 0.7085\n",
      "Epoch: 144, Loss: 0.2484 tsm_loss: 0.0000 reg_loss: 0.2484 N_Y: 406577 N_S: 8639 N: 3930 N_HV: 000 Val: 0.7456 Test: 0.7142\n",
      "Epoch: 145, Loss: 0.2391 tsm_loss: 0.0000 reg_loss: 0.2391 N_Y: 406681 N_S: 8904 N: 3830 N_HV: 000 Val: 0.7374 Test: 0.6767\n",
      "Epoch: 146, Loss: 0.2321 tsm_loss: 0.0000 reg_loss: 0.2320 N_Y: 406289 N_S: 8988 N: 4055 N_HV: 000 Val: 0.7297 Test: 0.6409\n",
      "Epoch: 147, Loss: 0.2507 tsm_loss: 0.0107 reg_loss: 0.2401 N_Y: 406057 N_S: 7910 N: 3292 N_HV: 007 Val: 0.7744 Test: 0.7172\n",
      "Epoch: 148, Loss: 0.2604 tsm_loss: 0.0012 reg_loss: 0.2592 N_Y: 405415 N_S: 7636 N: 3559 N_HV: 001 Val: 0.7306 Test: 0.6634\n",
      "Epoch: 149, Loss: 0.2888 tsm_loss: 0.0000 reg_loss: 0.2888 N_Y: 405036 N_S: 8725 N: 3813 N_HV: 000 Val: 0.7601 Test: 0.7163\n",
      "Epoch: 150, Loss: 0.2239 tsm_loss: 0.0000 reg_loss: 0.2239 N_Y: 407155 N_S: 8710 N: 3709 N_HV: 000 Val: 0.7227 Test: 0.6979\n",
      "Epoch: 151, Loss: 0.2456 tsm_loss: 0.0000 reg_loss: 0.2456 N_Y: 406000 N_S: 7959 N: 3395 N_HV: 000 Val: 0.8858 Test: 0.7782\n",
      "Epoch: 152, Loss: 0.2444 tsm_loss: 0.0000 reg_loss: 0.2444 N_Y: 407945 N_S: 9361 N: 4333 N_HV: 000 Val: 0.7376 Test: 0.6651\n",
      "Epoch: 153, Loss: 0.2244 tsm_loss: 0.0001 reg_loss: 0.2243 N_Y: 406630 N_S: 10431 N: 4578 N_HV: 000 Val: 0.7392 Test: 0.6949\n",
      "Epoch: 154, Loss: 0.2260 tsm_loss: 0.0003 reg_loss: 0.2257 N_Y: 406408 N_S: 8514 N: 3499 N_HV: 000 Val: 0.7075 Test: 0.6476\n",
      "Epoch: 155, Loss: 0.2466 tsm_loss: 0.0001 reg_loss: 0.2465 N_Y: 407058 N_S: 8378 N: 3677 N_HV: 000 Val: 0.8116 Test: 0.7659\n",
      "Epoch: 156, Loss: 0.2713 tsm_loss: 0.0000 reg_loss: 0.2713 N_Y: 403853 N_S: 10031 N: 4215 N_HV: 000 Val: 0.7680 Test: 0.7245\n",
      "Epoch: 157, Loss: 0.2666 tsm_loss: 0.0000 reg_loss: 0.2666 N_Y: 406257 N_S: 8653 N: 4035 N_HV: 000 Val: 0.7226 Test: 0.6647\n",
      "Epoch: 158, Loss: 0.2221 tsm_loss: 0.0000 reg_loss: 0.2221 N_Y: 405295 N_S: 8257 N: 3907 N_HV: 000 Val: 0.7516 Test: 0.7152\n",
      "Epoch: 159, Loss: 0.2228 tsm_loss: 0.0000 reg_loss: 0.2228 N_Y: 405908 N_S: 8336 N: 3716 N_HV: 000 Val: 0.7341 Test: 0.6804\n",
      "Epoch: 160, Loss: 0.2134 tsm_loss: 0.0000 reg_loss: 0.2134 N_Y: 406271 N_S: 7954 N: 3410 N_HV: 000 Val: 0.7227 Test: 0.6742\n",
      "Epoch: 161, Loss: 0.2064 tsm_loss: 0.0000 reg_loss: 0.2064 N_Y: 407799 N_S: 8601 N: 3868 N_HV: 000 Val: 0.7311 Test: 0.6948\n",
      "Epoch: 162, Loss: 0.2214 tsm_loss: 0.0000 reg_loss: 0.2214 N_Y: 406273 N_S: 8132 N: 3470 N_HV: 000 Val: 0.7335 Test: 0.6831\n",
      "Epoch: 163, Loss: 0.2050 tsm_loss: 0.0000 reg_loss: 0.2050 N_Y: 407078 N_S: 9034 N: 3966 N_HV: 000 Val: 0.7622 Test: 0.7076\n",
      "Epoch: 164, Loss: 0.2292 tsm_loss: 0.0000 reg_loss: 0.2292 N_Y: 407295 N_S: 9114 N: 4180 N_HV: 000 Val: 0.7495 Test: 0.6750\n",
      "Epoch: 165, Loss: 0.2461 tsm_loss: 0.0001 reg_loss: 0.2460 N_Y: 406269 N_S: 8628 N: 3834 N_HV: 000 Val: 0.7114 Test: 0.6923\n",
      "Epoch: 166, Loss: 0.2062 tsm_loss: 0.0000 reg_loss: 0.2062 N_Y: 406299 N_S: 9024 N: 4080 N_HV: 000 Val: 0.7204 Test: 0.6663\n",
      "Epoch: 167, Loss: 0.2307 tsm_loss: 0.0000 reg_loss: 0.2307 N_Y: 405672 N_S: 8069 N: 3276 N_HV: 000 Val: 0.7428 Test: 0.6925\n",
      "Epoch: 168, Loss: 0.2044 tsm_loss: 0.0000 reg_loss: 0.2044 N_Y: 407344 N_S: 9196 N: 4010 N_HV: 000 Val: 0.7231 Test: 0.6627\n",
      "Epoch: 169, Loss: 0.1884 tsm_loss: 0.0000 reg_loss: 0.1884 N_Y: 406736 N_S: 9203 N: 4129 N_HV: 000 Val: 0.7457 Test: 0.6752\n",
      "Epoch: 170, Loss: 0.1936 tsm_loss: 0.0000 reg_loss: 0.1936 N_Y: 405863 N_S: 8757 N: 3924 N_HV: 000 Val: 0.7336 Test: 0.6606\n",
      "Epoch: 171, Loss: 0.1897 tsm_loss: 0.0001 reg_loss: 0.1896 N_Y: 407109 N_S: 8529 N: 3745 N_HV: 000 Val: 0.7523 Test: 0.7049\n",
      "Epoch: 172, Loss: 0.1995 tsm_loss: 0.0001 reg_loss: 0.1995 N_Y: 406730 N_S: 8430 N: 3754 N_HV: 000 Val: 0.8015 Test: 0.7742\n",
      "Epoch: 173, Loss: 0.2272 tsm_loss: 0.0000 reg_loss: 0.2272 N_Y: 406542 N_S: 8677 N: 3984 N_HV: 000 Val: 0.7374 Test: 0.6664\n",
      "Epoch: 174, Loss: 0.2209 tsm_loss: 0.0000 reg_loss: 0.2209 N_Y: 403669 N_S: 8331 N: 3769 N_HV: 000 Val: 0.7128 Test: 0.6364\n",
      "Epoch: 175, Loss: 0.2138 tsm_loss: 0.0001 reg_loss: 0.2137 N_Y: 407396 N_S: 8327 N: 3818 N_HV: 000 Val: 0.8073 Test: 0.7606\n",
      "Epoch: 176, Loss: 0.2124 tsm_loss: 0.0000 reg_loss: 0.2124 N_Y: 406142 N_S: 7658 N: 3590 N_HV: 000 Val: 0.7088 Test: 0.6462\n",
      "Epoch: 177, Loss: 0.1782 tsm_loss: 0.0000 reg_loss: 0.1782 N_Y: 405266 N_S: 9080 N: 4215 N_HV: 000 Val: 0.7351 Test: 0.6537\n",
      "Epoch: 178, Loss: 0.1910 tsm_loss: 0.0000 reg_loss: 0.1910 N_Y: 406765 N_S: 9334 N: 4000 N_HV: 000 Val: 0.7288 Test: 0.6603\n",
      "Epoch: 179, Loss: 0.2051 tsm_loss: 0.0000 reg_loss: 0.2051 N_Y: 406957 N_S: 8833 N: 3962 N_HV: 000 Val: 0.7173 Test: 0.6502\n",
      "Epoch: 180, Loss: 0.2021 tsm_loss: 0.0000 reg_loss: 0.2021 N_Y: 404782 N_S: 8529 N: 3771 N_HV: 000 Val: 0.7073 Test: 0.6424\n",
      "Epoch: 181, Loss: 0.2217 tsm_loss: 0.0010 reg_loss: 0.2206 N_Y: 405579 N_S: 8703 N: 3959 N_HV: 002 Val: 0.7859 Test: 0.7347\n",
      "Epoch: 182, Loss: 0.2859 tsm_loss: 0.0000 reg_loss: 0.2859 N_Y: 406194 N_S: 8664 N: 3752 N_HV: 000 Val: 0.7580 Test: 0.6844\n",
      "Epoch: 183, Loss: 0.2254 tsm_loss: 0.0002 reg_loss: 0.2252 N_Y: 405616 N_S: 8922 N: 3714 N_HV: 000 Val: 0.7139 Test: 0.6443\n",
      "Epoch: 184, Loss: 0.1844 tsm_loss: 0.0000 reg_loss: 0.1844 N_Y: 407099 N_S: 9697 N: 4304 N_HV: 000 Val: 0.7223 Test: 0.6465\n",
      "Epoch: 185, Loss: 0.1995 tsm_loss: 0.0000 reg_loss: 0.1995 N_Y: 405148 N_S: 7936 N: 3427 N_HV: 000 Val: 0.8354 Test: 0.7395\n",
      "Epoch: 186, Loss: 0.2339 tsm_loss: 0.0005 reg_loss: 0.2334 N_Y: 405871 N_S: 8274 N: 3480 N_HV: 000 Val: 0.7285 Test: 0.6740\n",
      "Epoch: 187, Loss: 0.2008 tsm_loss: 0.0000 reg_loss: 0.2008 N_Y: 406539 N_S: 8463 N: 3864 N_HV: 000 Val: 0.7222 Test: 0.6527\n",
      "Epoch: 188, Loss: 0.1923 tsm_loss: 0.0000 reg_loss: 0.1923 N_Y: 404984 N_S: 8842 N: 3759 N_HV: 000 Val: 0.7471 Test: 0.6907\n",
      "Epoch: 189, Loss: 0.1860 tsm_loss: 0.0000 reg_loss: 0.1860 N_Y: 406673 N_S: 9306 N: 3974 N_HV: 000 Val: 0.7533 Test: 0.7003\n",
      "Epoch: 190, Loss: 0.1849 tsm_loss: 0.0002 reg_loss: 0.1847 N_Y: 405937 N_S: 8404 N: 3849 N_HV: 000 Val: 0.7157 Test: 0.6666\n",
      "Epoch: 191, Loss: 0.1799 tsm_loss: 0.0000 reg_loss: 0.1799 N_Y: 405366 N_S: 8512 N: 3852 N_HV: 000 Val: 0.7127 Test: 0.6618\n",
      "Epoch: 192, Loss: 0.1751 tsm_loss: 0.0052 reg_loss: 0.1699 N_Y: 406747 N_S: 9206 N: 4074 N_HV: 004 Val: 0.6931 Test: 0.6656\n",
      "Epoch: 193, Loss: 0.1640 tsm_loss: 0.0000 reg_loss: 0.1640 N_Y: 405199 N_S: 8437 N: 3452 N_HV: 000 Val: 0.7336 Test: 0.6889\n",
      "Epoch: 194, Loss: 0.1574 tsm_loss: 0.0000 reg_loss: 0.1574 N_Y: 407539 N_S: 8128 N: 3516 N_HV: 000 Val: 0.7298 Test: 0.6828\n",
      "Epoch: 195, Loss: 0.1815 tsm_loss: 0.0000 reg_loss: 0.1815 N_Y: 406072 N_S: 8398 N: 3534 N_HV: 000 Val: 0.7408 Test: 0.6743\n",
      "Epoch: 196, Loss: 0.1836 tsm_loss: 0.0000 reg_loss: 0.1836 N_Y: 406493 N_S: 8194 N: 3424 N_HV: 000 Val: 0.7466 Test: 0.6892\n",
      "Epoch: 197, Loss: 0.1915 tsm_loss: 0.0000 reg_loss: 0.1915 N_Y: 406052 N_S: 7589 N: 3108 N_HV: 000 Val: 0.7514 Test: 0.6809\n",
      "Epoch: 198, Loss: 0.2176 tsm_loss: 0.0000 reg_loss: 0.2176 N_Y: 405360 N_S: 7956 N: 3541 N_HV: 000 Val: 0.8425 Test: 0.8120\n",
      "Epoch: 199, Loss: 0.2380 tsm_loss: 0.0006 reg_loss: 0.2374 N_Y: 405635 N_S: 8954 N: 3607 N_HV: 000 Val: 0.8004 Test: 0.7268\n",
      "Epoch: 200, Loss: 0.2232 tsm_loss: 0.0000 reg_loss: 0.2232 N_Y: 407098 N_S: 8785 N: 4046 N_HV: 000 Val: 0.7243 Test: 0.6621\n",
      "Epoch: 201, Loss: 0.1942 tsm_loss: 0.0000 reg_loss: 0.1942 N_Y: 405911 N_S: 8734 N: 4041 N_HV: 000 Val: 0.7252 Test: 0.6768\n",
      "Epoch: 202, Loss: 0.1860 tsm_loss: 0.0000 reg_loss: 0.1859 N_Y: 404144 N_S: 8928 N: 3762 N_HV: 000 Val: 0.7090 Test: 0.6558\n",
      "Epoch: 203, Loss: 0.1652 tsm_loss: 0.0000 reg_loss: 0.1652 N_Y: 405784 N_S: 9561 N: 4131 N_HV: 000 Val: 0.7097 Test: 0.6670\n",
      "Epoch: 204, Loss: 0.1618 tsm_loss: 0.0000 reg_loss: 0.1618 N_Y: 405318 N_S: 8431 N: 3533 N_HV: 000 Val: 0.7232 Test: 0.6576\n",
      "Epoch: 205, Loss: 0.1652 tsm_loss: 0.0000 reg_loss: 0.1652 N_Y: 406470 N_S: 8397 N: 3886 N_HV: 000 Val: 0.7260 Test: 0.6887\n",
      "Epoch: 206, Loss: 0.1724 tsm_loss: 0.0000 reg_loss: 0.1724 N_Y: 405566 N_S: 8268 N: 3493 N_HV: 000 Val: 0.7554 Test: 0.7001\n",
      "Epoch: 207, Loss: 0.1800 tsm_loss: 0.0000 reg_loss: 0.1800 N_Y: 405253 N_S: 8351 N: 3546 N_HV: 000 Val: 0.7054 Test: 0.6498\n",
      "Epoch: 208, Loss: 0.1753 tsm_loss: 0.0000 reg_loss: 0.1753 N_Y: 405789 N_S: 8554 N: 4120 N_HV: 000 Val: 0.7277 Test: 0.6751\n",
      "Epoch: 209, Loss: 0.1927 tsm_loss: 0.0000 reg_loss: 0.1927 N_Y: 406144 N_S: 8528 N: 3865 N_HV: 000 Val: 0.7322 Test: 0.6491\n",
      "Epoch: 210, Loss: 0.2102 tsm_loss: 0.0000 reg_loss: 0.2102 N_Y: 403127 N_S: 8230 N: 3521 N_HV: 000 Val: 0.7501 Test: 0.6730\n",
      "Epoch: 211, Loss: 0.1971 tsm_loss: 0.0000 reg_loss: 0.1971 N_Y: 407135 N_S: 8559 N: 3796 N_HV: 000 Val: 0.7409 Test: 0.6734\n",
      "Epoch: 212, Loss: 0.1797 tsm_loss: 0.0041 reg_loss: 0.1756 N_Y: 406901 N_S: 8777 N: 4161 N_HV: 005 Val: 0.7192 Test: 0.6922\n",
      "Epoch: 213, Loss: 0.1612 tsm_loss: 0.0000 reg_loss: 0.1611 N_Y: 405890 N_S: 8908 N: 3877 N_HV: 000 Val: 0.7124 Test: 0.6562\n",
      "Epoch: 214, Loss: 0.1752 tsm_loss: 0.0002 reg_loss: 0.1750 N_Y: 406296 N_S: 8680 N: 3638 N_HV: 000 Val: 0.7471 Test: 0.6999\n",
      "Epoch: 215, Loss: 0.1568 tsm_loss: 0.0001 reg_loss: 0.1567 N_Y: 403586 N_S: 8484 N: 3768 N_HV: 000 Val: 0.7158 Test: 0.6633\n",
      "Epoch: 216, Loss: 0.1655 tsm_loss: 0.0000 reg_loss: 0.1655 N_Y: 406639 N_S: 8713 N: 3865 N_HV: 000 Val: 0.7495 Test: 0.6824\n",
      "Epoch: 217, Loss: 0.1680 tsm_loss: 0.0002 reg_loss: 0.1679 N_Y: 406361 N_S: 8517 N: 3732 N_HV: 000 Val: 0.7202 Test: 0.6608\n",
      "Epoch: 218, Loss: 0.1691 tsm_loss: 0.0000 reg_loss: 0.1691 N_Y: 406864 N_S: 9342 N: 4130 N_HV: 000 Val: 0.7286 Test: 0.6574\n",
      "Epoch: 219, Loss: 0.1749 tsm_loss: 0.0000 reg_loss: 0.1749 N_Y: 406069 N_S: 8421 N: 3473 N_HV: 000 Val: 0.7446 Test: 0.6685\n",
      "Epoch: 220, Loss: 0.2289 tsm_loss: 0.0002 reg_loss: 0.2287 N_Y: 406819 N_S: 8594 N: 3664 N_HV: 000 Val: 0.7415 Test: 0.6574\n",
      "Epoch: 221, Loss: 0.1805 tsm_loss: 0.0000 reg_loss: 0.1805 N_Y: 406537 N_S: 7821 N: 3554 N_HV: 000 Val: 0.7062 Test: 0.6594\n",
      "Epoch: 222, Loss: 0.1859 tsm_loss: 0.0000 reg_loss: 0.1859 N_Y: 406436 N_S: 8642 N: 3892 N_HV: 000 Val: 0.7419 Test: 0.6707\n",
      "Epoch: 223, Loss: 0.1776 tsm_loss: 0.0000 reg_loss: 0.1776 N_Y: 405589 N_S: 9114 N: 3921 N_HV: 000 Val: 0.7568 Test: 0.6554\n",
      "Epoch: 224, Loss: 0.2043 tsm_loss: 0.0000 reg_loss: 0.2043 N_Y: 406410 N_S: 8640 N: 3697 N_HV: 000 Val: 0.7208 Test: 0.6485\n",
      "Epoch: 225, Loss: 0.1817 tsm_loss: 0.0000 reg_loss: 0.1817 N_Y: 405524 N_S: 8763 N: 4059 N_HV: 000 Val: 0.7300 Test: 0.6516\n",
      "Epoch: 226, Loss: 0.1603 tsm_loss: 0.0000 reg_loss: 0.1603 N_Y: 406650 N_S: 9334 N: 4008 N_HV: 000 Val: 0.7210 Test: 0.6739\n",
      "Epoch: 227, Loss: 0.1816 tsm_loss: 0.0000 reg_loss: 0.1816 N_Y: 405631 N_S: 9111 N: 4048 N_HV: 000 Val: 0.7397 Test: 0.6428\n",
      "Epoch: 228, Loss: 0.1474 tsm_loss: 0.0001 reg_loss: 0.1473 N_Y: 406719 N_S: 8763 N: 3972 N_HV: 000 Val: 0.7157 Test: 0.6503\n",
      "Epoch: 229, Loss: 0.1610 tsm_loss: 0.0001 reg_loss: 0.1609 N_Y: 406502 N_S: 9573 N: 4462 N_HV: 000 Val: 0.7260 Test: 0.6691\n",
      "Epoch: 230, Loss: 0.1721 tsm_loss: 0.0000 reg_loss: 0.1721 N_Y: 406822 N_S: 8179 N: 3880 N_HV: 000 Val: 0.7147 Test: 0.6637\n",
      "Epoch: 231, Loss: 0.1561 tsm_loss: 0.0004 reg_loss: 0.1557 N_Y: 406432 N_S: 8715 N: 3804 N_HV: 000 Val: 0.7128 Test: 0.6690\n",
      "Epoch: 232, Loss: 0.1652 tsm_loss: 0.0000 reg_loss: 0.1652 N_Y: 406902 N_S: 7954 N: 3525 N_HV: 000 Val: 0.7503 Test: 0.6773\n",
      "Epoch: 233, Loss: 0.1827 tsm_loss: 0.0000 reg_loss: 0.1827 N_Y: 406744 N_S: 8073 N: 3787 N_HV: 000 Val: 0.7084 Test: 0.6533\n",
      "Epoch: 234, Loss: 0.1555 tsm_loss: 0.0000 reg_loss: 0.1555 N_Y: 405061 N_S: 8759 N: 3548 N_HV: 000 Val: 0.7250 Test: 0.6611\n",
      "Epoch: 235, Loss: 0.1810 tsm_loss: 0.0000 reg_loss: 0.1810 N_Y: 406032 N_S: 8206 N: 3728 N_HV: 000 Val: 0.7139 Test: 0.6514\n",
      "Epoch: 236, Loss: 0.1605 tsm_loss: 0.0000 reg_loss: 0.1605 N_Y: 405734 N_S: 9044 N: 4092 N_HV: 000 Val: 0.7275 Test: 0.6591\n",
      "Epoch: 237, Loss: 0.1507 tsm_loss: 0.0000 reg_loss: 0.1507 N_Y: 406021 N_S: 7832 N: 3401 N_HV: 000 Val: 0.7167 Test: 0.6473\n",
      "Epoch: 238, Loss: 0.1569 tsm_loss: 0.0000 reg_loss: 0.1569 N_Y: 404844 N_S: 7480 N: 3306 N_HV: 000 Val: 0.7332 Test: 0.6676\n",
      "Epoch: 239, Loss: 0.1857 tsm_loss: 0.0000 reg_loss: 0.1857 N_Y: 404242 N_S: 9433 N: 4246 N_HV: 000 Val: 0.7274 Test: 0.6622\n",
      "Epoch: 240, Loss: 0.1653 tsm_loss: 0.0000 reg_loss: 0.1653 N_Y: 405252 N_S: 8374 N: 4044 N_HV: 000 Val: 0.7212 Test: 0.6529\n",
      "Epoch: 241, Loss: 0.1546 tsm_loss: 0.0000 reg_loss: 0.1545 N_Y: 406443 N_S: 8574 N: 3861 N_HV: 000 Val: 0.7377 Test: 0.6880\n",
      "Epoch: 242, Loss: 0.1375 tsm_loss: 0.0000 reg_loss: 0.1375 N_Y: 406082 N_S: 7131 N: 3066 N_HV: 000 Val: 0.7130 Test: 0.6579\n",
      "Epoch: 243, Loss: 0.1525 tsm_loss: 0.0000 reg_loss: 0.1525 N_Y: 407247 N_S: 8946 N: 4035 N_HV: 000 Val: 0.7139 Test: 0.6655\n",
      "Epoch: 244, Loss: 0.1791 tsm_loss: 0.0000 reg_loss: 0.1791 N_Y: 406834 N_S: 9078 N: 3921 N_HV: 000 Val: 0.7321 Test: 0.6633\n",
      "Epoch: 245, Loss: 0.1492 tsm_loss: 0.0002 reg_loss: 0.1490 N_Y: 405576 N_S: 8708 N: 3717 N_HV: 000 Val: 0.7205 Test: 0.6808\n",
      "Epoch: 246, Loss: 0.1463 tsm_loss: 0.0000 reg_loss: 0.1463 N_Y: 404776 N_S: 8495 N: 3647 N_HV: 000 Val: 0.7359 Test: 0.6845\n",
      "Epoch: 247, Loss: 0.1618 tsm_loss: 0.0009 reg_loss: 0.1609 N_Y: 406891 N_S: 8122 N: 3512 N_HV: 000 Val: 0.7204 Test: 0.6514\n",
      "Epoch: 248, Loss: 0.1539 tsm_loss: 0.0001 reg_loss: 0.1538 N_Y: 405142 N_S: 8887 N: 4023 N_HV: 000 Val: 0.7857 Test: 0.7152\n",
      "Epoch: 249, Loss: 0.2102 tsm_loss: 0.0000 reg_loss: 0.2102 N_Y: 406625 N_S: 8103 N: 3516 N_HV: 000 Val: 0.7236 Test: 0.6516\n",
      "Epoch: 250, Loss: 0.1815 tsm_loss: 0.0000 reg_loss: 0.1815 N_Y: 405641 N_S: 8553 N: 3877 N_HV: 000 Val: 0.7210 Test: 0.6755\n",
      "Epoch: 251, Loss: 0.1776 tsm_loss: 0.0000 reg_loss: 0.1776 N_Y: 404299 N_S: 7993 N: 3614 N_HV: 000 Val: 0.7286 Test: 0.6812\n",
      "Epoch: 252, Loss: 0.1451 tsm_loss: 0.0000 reg_loss: 0.1451 N_Y: 406095 N_S: 8763 N: 4036 N_HV: 000 Val: 0.7166 Test: 0.6836\n",
      "Epoch: 253, Loss: 0.1540 tsm_loss: 0.0001 reg_loss: 0.1539 N_Y: 407035 N_S: 8029 N: 3603 N_HV: 000 Val: 0.7259 Test: 0.6644\n",
      "Epoch: 254, Loss: 0.1396 tsm_loss: 0.0000 reg_loss: 0.1396 N_Y: 406414 N_S: 9361 N: 4019 N_HV: 000 Val: 0.7270 Test: 0.6749\n",
      "Epoch: 255, Loss: 0.1524 tsm_loss: 0.0005 reg_loss: 0.1519 N_Y: 408029 N_S: 8229 N: 3636 N_HV: 000 Val: 0.7174 Test: 0.6518\n",
      "Epoch: 256, Loss: 0.1585 tsm_loss: 0.0000 reg_loss: 0.1585 N_Y: 405826 N_S: 9157 N: 4355 N_HV: 000 Val: 0.7447 Test: 0.6684\n",
      "Epoch: 257, Loss: 0.1617 tsm_loss: 0.0000 reg_loss: 0.1616 N_Y: 405439 N_S: 9665 N: 4411 N_HV: 000 Val: 0.7265 Test: 0.6899\n",
      "Epoch: 258, Loss: 0.1416 tsm_loss: 0.0000 reg_loss: 0.1416 N_Y: 406304 N_S: 9162 N: 4436 N_HV: 000 Val: 0.7267 Test: 0.6573\n",
      "Epoch: 259, Loss: 0.1298 tsm_loss: 0.0001 reg_loss: 0.1298 N_Y: 406878 N_S: 8754 N: 3998 N_HV: 000 Val: 0.7374 Test: 0.6514\n",
      "Epoch: 260, Loss: 0.2029 tsm_loss: 0.0000 reg_loss: 0.2029 N_Y: 406689 N_S: 8234 N: 3858 N_HV: 000 Val: 0.7492 Test: 0.6870\n",
      "Epoch: 261, Loss: 0.1862 tsm_loss: 0.0000 reg_loss: 0.1862 N_Y: 406121 N_S: 8672 N: 3876 N_HV: 000 Val: 0.7365 Test: 0.6756\n",
      "Epoch: 262, Loss: 0.1504 tsm_loss: 0.0000 reg_loss: 0.1504 N_Y: 407349 N_S: 8197 N: 3609 N_HV: 000 Val: 0.7356 Test: 0.6561\n",
      "Epoch: 263, Loss: 0.1395 tsm_loss: 0.0000 reg_loss: 0.1395 N_Y: 406115 N_S: 7861 N: 3578 N_HV: 000 Val: 0.7277 Test: 0.6499\n",
      "Epoch: 264, Loss: 0.1469 tsm_loss: 0.0000 reg_loss: 0.1469 N_Y: 407381 N_S: 8376 N: 3819 N_HV: 000 Val: 0.7424 Test: 0.6767\n",
      "Epoch: 265, Loss: 0.1517 tsm_loss: 0.0001 reg_loss: 0.1515 N_Y: 404742 N_S: 8732 N: 3840 N_HV: 000 Val: 0.7478 Test: 0.6755\n",
      "Epoch: 266, Loss: 0.1634 tsm_loss: 0.0000 reg_loss: 0.1634 N_Y: 406645 N_S: 7759 N: 3314 N_HV: 000 Val: 0.7325 Test: 0.6361\n",
      "Epoch: 267, Loss: 0.1527 tsm_loss: 0.0000 reg_loss: 0.1527 N_Y: 405541 N_S: 8733 N: 4103 N_HV: 000 Val: 0.7453 Test: 0.6532\n",
      "Epoch: 268, Loss: 0.1386 tsm_loss: 0.0001 reg_loss: 0.1385 N_Y: 406361 N_S: 8423 N: 3748 N_HV: 000 Val: 0.7323 Test: 0.6531\n",
      "Epoch: 269, Loss: 0.1609 tsm_loss: 0.0000 reg_loss: 0.1609 N_Y: 405062 N_S: 9008 N: 3684 N_HV: 000 Val: 0.7269 Test: 0.6681\n",
      "Epoch: 270, Loss: 0.2033 tsm_loss: 0.0000 reg_loss: 0.2033 N_Y: 405297 N_S: 8428 N: 3465 N_HV: 000 Val: 0.7132 Test: 0.6496\n",
      "Epoch: 271, Loss: 0.1921 tsm_loss: 0.0000 reg_loss: 0.1921 N_Y: 407073 N_S: 8823 N: 3702 N_HV: 000 Val: 0.7615 Test: 0.6726\n",
      "Epoch: 272, Loss: 0.2177 tsm_loss: 0.0001 reg_loss: 0.2176 N_Y: 406788 N_S: 8609 N: 3781 N_HV: 000 Val: 0.7460 Test: 0.6751\n",
      "Epoch: 273, Loss: 0.1819 tsm_loss: 0.0000 reg_loss: 0.1819 N_Y: 403583 N_S: 8889 N: 4051 N_HV: 000 Val: 0.7418 Test: 0.6677\n",
      "Epoch: 274, Loss: 0.1795 tsm_loss: 0.0000 reg_loss: 0.1795 N_Y: 406181 N_S: 8958 N: 3936 N_HV: 000 Val: 0.7358 Test: 0.6556\n",
      "Epoch: 275, Loss: 0.1586 tsm_loss: 0.0000 reg_loss: 0.1586 N_Y: 404184 N_S: 9008 N: 3976 N_HV: 000 Val: 0.7329 Test: 0.6442\n",
      "Epoch: 276, Loss: 0.1586 tsm_loss: 0.0002 reg_loss: 0.1584 N_Y: 405246 N_S: 8913 N: 3838 N_HV: 000 Val: 0.7593 Test: 0.6995\n",
      "Epoch: 277, Loss: 0.1526 tsm_loss: 0.0000 reg_loss: 0.1526 N_Y: 406328 N_S: 8836 N: 3675 N_HV: 000 Val: 0.7277 Test: 0.6596\n",
      "Epoch: 278, Loss: 0.1612 tsm_loss: 0.0000 reg_loss: 0.1612 N_Y: 406011 N_S: 8228 N: 3697 N_HV: 000 Val: 0.7649 Test: 0.6784\n",
      "Epoch: 279, Loss: 0.1467 tsm_loss: 0.0001 reg_loss: 0.1467 N_Y: 402922 N_S: 9266 N: 4097 N_HV: 000 Val: 0.7607 Test: 0.6546\n",
      "Epoch: 280, Loss: 0.1234 tsm_loss: 0.0000 reg_loss: 0.1234 N_Y: 405462 N_S: 8531 N: 3550 N_HV: 000 Val: 0.7422 Test: 0.6546\n",
      "Epoch: 281, Loss: 0.1398 tsm_loss: 0.0000 reg_loss: 0.1398 N_Y: 405230 N_S: 9426 N: 4094 N_HV: 000 Val: 0.7521 Test: 0.6416\n",
      "Epoch: 282, Loss: 0.1310 tsm_loss: 0.0000 reg_loss: 0.1310 N_Y: 406514 N_S: 8619 N: 3877 N_HV: 000 Val: 0.7557 Test: 0.7032\n",
      "Epoch: 283, Loss: 0.1430 tsm_loss: 0.0000 reg_loss: 0.1430 N_Y: 406814 N_S: 9437 N: 4347 N_HV: 000 Val: 0.7771 Test: 0.6875\n",
      "Epoch: 284, Loss: 0.1990 tsm_loss: 0.0000 reg_loss: 0.1990 N_Y: 407713 N_S: 8120 N: 3700 N_HV: 000 Val: 0.7355 Test: 0.6800\n",
      "Epoch: 285, Loss: 0.1758 tsm_loss: 0.0000 reg_loss: 0.1758 N_Y: 406414 N_S: 9014 N: 4043 N_HV: 000 Val: 0.7354 Test: 0.6895\n",
      "Epoch: 286, Loss: 0.1736 tsm_loss: 0.0000 reg_loss: 0.1736 N_Y: 405746 N_S: 8297 N: 3587 N_HV: 000 Val: 0.7294 Test: 0.6664\n",
      "Epoch: 287, Loss: 0.1532 tsm_loss: 0.0000 reg_loss: 0.1532 N_Y: 404873 N_S: 9066 N: 3953 N_HV: 000 Val: 0.7288 Test: 0.6627\n",
      "Epoch: 288, Loss: 0.1467 tsm_loss: 0.0000 reg_loss: 0.1466 N_Y: 406414 N_S: 9281 N: 4275 N_HV: 000 Val: 0.7113 Test: 0.6491\n",
      "Epoch: 289, Loss: 0.1421 tsm_loss: 0.0000 reg_loss: 0.1421 N_Y: 404390 N_S: 8264 N: 3706 N_HV: 000 Val: 0.7375 Test: 0.6418\n",
      "Epoch: 290, Loss: 0.1416 tsm_loss: 0.0000 reg_loss: 0.1416 N_Y: 405676 N_S: 8687 N: 3681 N_HV: 000 Val: 0.7197 Test: 0.6566\n",
      "Epoch: 291, Loss: 0.1342 tsm_loss: 0.0000 reg_loss: 0.1342 N_Y: 407135 N_S: 8469 N: 3558 N_HV: 000 Val: 0.7469 Test: 0.6952\n",
      "Epoch: 292, Loss: 0.1582 tsm_loss: 0.0000 reg_loss: 0.1582 N_Y: 405760 N_S: 8098 N: 3405 N_HV: 000 Val: 0.7307 Test: 0.6669\n",
      "Epoch: 293, Loss: 0.1560 tsm_loss: 0.0001 reg_loss: 0.1559 N_Y: 405159 N_S: 9239 N: 3993 N_HV: 000 Val: 0.7503 Test: 0.6916\n",
      "Epoch: 294, Loss: 0.1312 tsm_loss: 0.0000 reg_loss: 0.1312 N_Y: 405869 N_S: 8901 N: 4049 N_HV: 000 Val: 0.7359 Test: 0.6516\n",
      "Epoch: 295, Loss: 0.1420 tsm_loss: 0.0000 reg_loss: 0.1420 N_Y: 406339 N_S: 7539 N: 3256 N_HV: 000 Val: 0.7617 Test: 0.6792\n",
      "Epoch: 296, Loss: 0.1576 tsm_loss: 0.0000 reg_loss: 0.1576 N_Y: 406727 N_S: 7780 N: 3487 N_HV: 000 Val: 0.7159 Test: 0.6515\n",
      "Epoch: 297, Loss: 0.1241 tsm_loss: 0.0000 reg_loss: 0.1241 N_Y: 406440 N_S: 7814 N: 3462 N_HV: 000 Val: 0.7250 Test: 0.6721\n",
      "Epoch: 298, Loss: 0.1350 tsm_loss: 0.0000 reg_loss: 0.1350 N_Y: 406206 N_S: 9642 N: 4172 N_HV: 000 Val: 0.7490 Test: 0.6897\n",
      "Epoch: 299, Loss: 0.1445 tsm_loss: 0.0001 reg_loss: 0.1444 N_Y: 405469 N_S: 8141 N: 3581 N_HV: 000 Val: 0.7139 Test: 0.6406\n",
      "Epoch: 300, Loss: 0.1422 tsm_loss: 0.0000 reg_loss: 0.1422 N_Y: 404108 N_S: 7661 N: 3374 N_HV: 000 Val: 0.7168 Test: 0.6425\n",
      "Epoch: 301, Loss: 0.1326 tsm_loss: 0.0000 reg_loss: 0.1326 N_Y: 404449 N_S: 9188 N: 4176 N_HV: 000 Val: 0.7400 Test: 0.6517\n",
      "Epoch: 302, Loss: 0.1853 tsm_loss: 0.0000 reg_loss: 0.1853 N_Y: 406549 N_S: 8561 N: 3825 N_HV: 000 Val: 0.7364 Test: 0.6507\n",
      "Epoch: 303, Loss: 0.1397 tsm_loss: 0.0002 reg_loss: 0.1395 N_Y: 404805 N_S: 8474 N: 3632 N_HV: 000 Val: 0.7520 Test: 0.6867\n",
      "Epoch: 304, Loss: 0.1529 tsm_loss: 0.0001 reg_loss: 0.1528 N_Y: 405193 N_S: 8279 N: 3729 N_HV: 000 Val: 0.7370 Test: 0.6374\n",
      "Epoch: 305, Loss: 0.1439 tsm_loss: 0.0000 reg_loss: 0.1439 N_Y: 405731 N_S: 7718 N: 3488 N_HV: 000 Val: 0.7238 Test: 0.6433\n",
      "Epoch: 306, Loss: 0.1308 tsm_loss: 0.0000 reg_loss: 0.1308 N_Y: 406022 N_S: 9482 N: 3889 N_HV: 000 Val: 0.7201 Test: 0.6439\n",
      "Epoch: 307, Loss: 0.1400 tsm_loss: 0.0000 reg_loss: 0.1400 N_Y: 405041 N_S: 8494 N: 4022 N_HV: 000 Val: 0.7332 Test: 0.6600\n",
      "Epoch: 308, Loss: 0.1119 tsm_loss: 0.0000 reg_loss: 0.1119 N_Y: 405739 N_S: 7150 N: 3374 N_HV: 000 Val: 0.7145 Test: 0.6581\n",
      "Epoch: 309, Loss: 0.1341 tsm_loss: 0.0000 reg_loss: 0.1341 N_Y: 406710 N_S: 7902 N: 3511 N_HV: 000 Val: 0.7219 Test: 0.6498\n",
      "Epoch: 310, Loss: 0.1215 tsm_loss: 0.0000 reg_loss: 0.1215 N_Y: 406561 N_S: 9294 N: 4294 N_HV: 000 Val: 0.7214 Test: 0.6474\n",
      "Epoch: 311, Loss: 0.1235 tsm_loss: 0.0000 reg_loss: 0.1235 N_Y: 406066 N_S: 9158 N: 3974 N_HV: 000 Val: 0.7208 Test: 0.6352\n",
      "Epoch: 312, Loss: 0.1382 tsm_loss: 0.0000 reg_loss: 0.1381 N_Y: 405356 N_S: 9190 N: 4022 N_HV: 000 Val: 0.7430 Test: 0.6720\n",
      "Epoch: 313, Loss: 0.1686 tsm_loss: 0.0000 reg_loss: 0.1686 N_Y: 407217 N_S: 8049 N: 3458 N_HV: 000 Val: 0.7770 Test: 0.7178\n",
      "Epoch: 314, Loss: 0.1630 tsm_loss: 0.0000 reg_loss: 0.1630 N_Y: 405885 N_S: 7790 N: 3273 N_HV: 000 Val: 0.7765 Test: 0.7015\n",
      "Epoch: 315, Loss: 0.1714 tsm_loss: 0.0000 reg_loss: 0.1714 N_Y: 406199 N_S: 8315 N: 3797 N_HV: 000 Val: 0.7476 Test: 0.6862\n",
      "Epoch: 316, Loss: 0.1840 tsm_loss: 0.0000 reg_loss: 0.1840 N_Y: 405187 N_S: 8571 N: 3560 N_HV: 000 Val: 0.7259 Test: 0.6408\n",
      "Epoch: 317, Loss: 0.1643 tsm_loss: 0.0000 reg_loss: 0.1643 N_Y: 406856 N_S: 8538 N: 3681 N_HV: 000 Val: 0.6942 Test: 0.6504\n",
      "Epoch: 318, Loss: 0.1829 tsm_loss: 0.0004 reg_loss: 0.1825 N_Y: 406986 N_S: 9168 N: 4462 N_HV: 000 Val: 0.7196 Test: 0.6587\n",
      "Epoch: 319, Loss: 0.1878 tsm_loss: 0.0001 reg_loss: 0.1877 N_Y: 405159 N_S: 9537 N: 4244 N_HV: 000 Val: 0.7231 Test: 0.6641\n",
      "Epoch: 320, Loss: 0.1615 tsm_loss: 0.0000 reg_loss: 0.1615 N_Y: 407001 N_S: 8454 N: 3761 N_HV: 000 Val: 0.7050 Test: 0.6335\n",
      "Epoch: 321, Loss: 0.1342 tsm_loss: 0.0000 reg_loss: 0.1342 N_Y: 406065 N_S: 9159 N: 4058 N_HV: 000 Val: 0.7321 Test: 0.6562\n",
      "Epoch: 322, Loss: 0.1348 tsm_loss: 0.0000 reg_loss: 0.1348 N_Y: 405940 N_S: 8127 N: 3388 N_HV: 000 Val: 0.7255 Test: 0.6612\n",
      "Epoch: 323, Loss: 0.1477 tsm_loss: 0.0000 reg_loss: 0.1477 N_Y: 404848 N_S: 8733 N: 3865 N_HV: 000 Val: 0.7441 Test: 0.6523\n",
      "Epoch: 324, Loss: 0.1941 tsm_loss: 0.0001 reg_loss: 0.1940 N_Y: 406716 N_S: 8440 N: 3588 N_HV: 000 Val: 0.7404 Test: 0.6774\n",
      "Epoch: 325, Loss: 0.1339 tsm_loss: 0.0000 reg_loss: 0.1338 N_Y: 404574 N_S: 8546 N: 3775 N_HV: 000 Val: 0.7057 Test: 0.6395\n",
      "Epoch: 326, Loss: 0.1513 tsm_loss: 0.0000 reg_loss: 0.1512 N_Y: 406556 N_S: 8938 N: 3706 N_HV: 000 Val: 0.7219 Test: 0.6336\n",
      "Epoch: 327, Loss: 0.1098 tsm_loss: 0.0000 reg_loss: 0.1098 N_Y: 405558 N_S: 8956 N: 3907 N_HV: 000 Val: 0.7291 Test: 0.6366\n",
      "Epoch: 328, Loss: 0.1436 tsm_loss: 0.0001 reg_loss: 0.1435 N_Y: 406371 N_S: 7903 N: 3482 N_HV: 000 Val: 0.7244 Test: 0.6440\n",
      "Epoch: 329, Loss: 0.1281 tsm_loss: 0.0000 reg_loss: 0.1281 N_Y: 406013 N_S: 8617 N: 3876 N_HV: 000 Val: 0.7363 Test: 0.6580\n",
      "Epoch: 330, Loss: 0.1400 tsm_loss: 0.0000 reg_loss: 0.1400 N_Y: 405517 N_S: 8689 N: 3586 N_HV: 000 Val: 0.7241 Test: 0.6495\n",
      "Epoch: 331, Loss: 0.1734 tsm_loss: 0.0001 reg_loss: 0.1733 N_Y: 405623 N_S: 9042 N: 4032 N_HV: 000 Val: 0.7323 Test: 0.6529\n",
      "Epoch: 332, Loss: 0.1242 tsm_loss: 0.0000 reg_loss: 0.1242 N_Y: 404253 N_S: 8583 N: 3700 N_HV: 000 Val: 0.7233 Test: 0.6512\n",
      "Epoch: 333, Loss: 0.1423 tsm_loss: 0.0000 reg_loss: 0.1423 N_Y: 406732 N_S: 9332 N: 3988 N_HV: 000 Val: 0.7352 Test: 0.6573\n",
      "Epoch: 334, Loss: 0.1150 tsm_loss: 0.0000 reg_loss: 0.1150 N_Y: 407152 N_S: 8724 N: 3991 N_HV: 000 Val: 0.7468 Test: 0.6475\n",
      "Epoch: 335, Loss: 0.1127 tsm_loss: 0.0000 reg_loss: 0.1127 N_Y: 406655 N_S: 8195 N: 3694 N_HV: 000 Val: 0.7261 Test: 0.6521\n",
      "Epoch: 336, Loss: 0.1141 tsm_loss: 0.0000 reg_loss: 0.1141 N_Y: 405895 N_S: 7679 N: 3203 N_HV: 000 Val: 0.7289 Test: 0.6529\n",
      "Epoch: 337, Loss: 0.1161 tsm_loss: 0.0000 reg_loss: 0.1161 N_Y: 405636 N_S: 9124 N: 3996 N_HV: 000 Val: 0.7328 Test: 0.6722\n",
      "Epoch: 338, Loss: 0.1332 tsm_loss: 0.0000 reg_loss: 0.1332 N_Y: 406117 N_S: 10022 N: 4548 N_HV: 000 Val: 0.7377 Test: 0.6658\n",
      "Epoch: 339, Loss: 0.1064 tsm_loss: 0.0000 reg_loss: 0.1064 N_Y: 405840 N_S: 7946 N: 3511 N_HV: 000 Val: 0.7247 Test: 0.6456\n",
      "Epoch: 340, Loss: 0.1210 tsm_loss: 0.0000 reg_loss: 0.1210 N_Y: 407036 N_S: 8118 N: 3615 N_HV: 000 Val: 0.7357 Test: 0.6534\n",
      "Epoch: 341, Loss: 0.1085 tsm_loss: 0.0000 reg_loss: 0.1085 N_Y: 406529 N_S: 8887 N: 3988 N_HV: 000 Val: 0.7388 Test: 0.6553\n",
      "Epoch: 342, Loss: 0.1242 tsm_loss: 0.0001 reg_loss: 0.1242 N_Y: 406923 N_S: 8117 N: 3707 N_HV: 000 Val: 0.7556 Test: 0.6955\n",
      "Epoch: 343, Loss: 0.1968 tsm_loss: 0.0000 reg_loss: 0.1968 N_Y: 406316 N_S: 9141 N: 3999 N_HV: 000 Val: 0.7093 Test: 0.6590\n",
      "Epoch: 344, Loss: 0.1452 tsm_loss: 0.0000 reg_loss: 0.1452 N_Y: 407063 N_S: 9172 N: 3967 N_HV: 000 Val: 0.7126 Test: 0.6423\n",
      "Epoch: 345, Loss: 0.1265 tsm_loss: 0.0002 reg_loss: 0.1264 N_Y: 406412 N_S: 8818 N: 3958 N_HV: 000 Val: 0.7184 Test: 0.6529\n",
      "Epoch: 346, Loss: 0.1257 tsm_loss: 0.0001 reg_loss: 0.1256 N_Y: 406204 N_S: 8691 N: 3473 N_HV: 000 Val: 0.7241 Test: 0.6589\n",
      "Epoch: 347, Loss: 0.1475 tsm_loss: 0.0000 reg_loss: 0.1475 N_Y: 406305 N_S: 9541 N: 4389 N_HV: 000 Val: 0.7092 Test: 0.6386\n",
      "Epoch: 348, Loss: 0.1463 tsm_loss: 0.0000 reg_loss: 0.1463 N_Y: 405982 N_S: 7445 N: 3080 N_HV: 000 Val: 0.7228 Test: 0.6297\n",
      "Epoch: 349, Loss: 0.1441 tsm_loss: 0.0000 reg_loss: 0.1441 N_Y: 407161 N_S: 8787 N: 3829 N_HV: 000 Val: 0.7199 Test: 0.6648\n",
      "Epoch: 350, Loss: 0.1298 tsm_loss: 0.0000 reg_loss: 0.1298 N_Y: 406451 N_S: 7972 N: 3416 N_HV: 000 Val: 0.7161 Test: 0.6454\n",
      "Epoch: 351, Loss: 0.1119 tsm_loss: 0.0000 reg_loss: 0.1119 N_Y: 407895 N_S: 8350 N: 3711 N_HV: 000 Val: 0.7118 Test: 0.6313\n",
      "Epoch: 352, Loss: 0.1301 tsm_loss: 0.0000 reg_loss: 0.1301 N_Y: 406297 N_S: 9012 N: 3920 N_HV: 000 Val: 0.7264 Test: 0.6524\n",
      "Epoch: 353, Loss: 0.1461 tsm_loss: 0.0002 reg_loss: 0.1459 N_Y: 405620 N_S: 7729 N: 3322 N_HV: 000 Val: 0.7169 Test: 0.6446\n",
      "Epoch: 354, Loss: 0.1497 tsm_loss: 0.0000 reg_loss: 0.1497 N_Y: 405196 N_S: 8348 N: 3557 N_HV: 000 Val: 0.7288 Test: 0.6720\n",
      "Epoch: 355, Loss: 0.1966 tsm_loss: 0.0000 reg_loss: 0.1965 N_Y: 404082 N_S: 8522 N: 3687 N_HV: 000 Val: 0.7433 Test: 0.6621\n",
      "Epoch: 356, Loss: 0.1486 tsm_loss: 0.0000 reg_loss: 0.1486 N_Y: 405656 N_S: 8658 N: 3566 N_HV: 000 Val: 0.7262 Test: 0.6394\n",
      "Epoch: 357, Loss: 0.1393 tsm_loss: 0.0000 reg_loss: 0.1393 N_Y: 406342 N_S: 8668 N: 3590 N_HV: 000 Val: 0.7544 Test: 0.6736\n",
      "Epoch: 358, Loss: 0.1710 tsm_loss: 0.0002 reg_loss: 0.1708 N_Y: 405049 N_S: 8725 N: 3687 N_HV: 000 Val: 0.7260 Test: 0.6703\n",
      "Epoch: 359, Loss: 0.1609 tsm_loss: 0.0000 reg_loss: 0.1609 N_Y: 404673 N_S: 9821 N: 4125 N_HV: 000 Val: 0.7070 Test: 0.6381\n",
      "Epoch: 360, Loss: 0.1205 tsm_loss: 0.0001 reg_loss: 0.1204 N_Y: 406973 N_S: 9158 N: 3975 N_HV: 000 Val: 0.7292 Test: 0.6315\n",
      "Epoch: 361, Loss: 0.1072 tsm_loss: 0.0000 reg_loss: 0.1072 N_Y: 406607 N_S: 8933 N: 4068 N_HV: 000 Val: 0.7348 Test: 0.6560\n",
      "Epoch: 362, Loss: 0.1298 tsm_loss: 0.0000 reg_loss: 0.1298 N_Y: 406359 N_S: 8670 N: 4168 N_HV: 000 Val: 0.7246 Test: 0.6521\n",
      "Epoch: 363, Loss: 0.1195 tsm_loss: 0.0000 reg_loss: 0.1195 N_Y: 406123 N_S: 8053 N: 3564 N_HV: 000 Val: 0.7253 Test: 0.6459\n",
      "Epoch: 364, Loss: 0.1177 tsm_loss: 0.0000 reg_loss: 0.1177 N_Y: 407195 N_S: 8899 N: 3944 N_HV: 000 Val: 0.7254 Test: 0.6298\n",
      "Epoch: 365, Loss: 0.1348 tsm_loss: 0.0000 reg_loss: 0.1348 N_Y: 405720 N_S: 9007 N: 4005 N_HV: 000 Val: 0.7291 Test: 0.6419\n",
      "Epoch: 366, Loss: 0.1093 tsm_loss: 0.0000 reg_loss: 0.1092 N_Y: 406046 N_S: 8719 N: 3860 N_HV: 000 Val: 0.7625 Test: 0.6643\n",
      "Epoch: 367, Loss: 0.1046 tsm_loss: 0.0000 reg_loss: 0.1046 N_Y: 406423 N_S: 9700 N: 4348 N_HV: 000 Val: 0.7320 Test: 0.6678\n",
      "Epoch: 368, Loss: 0.1123 tsm_loss: 0.0002 reg_loss: 0.1121 N_Y: 405552 N_S: 8994 N: 3887 N_HV: 000 Val: 0.7181 Test: 0.6338\n",
      "Epoch: 369, Loss: 0.1282 tsm_loss: 0.0000 reg_loss: 0.1282 N_Y: 405840 N_S: 8944 N: 3874 N_HV: 000 Val: 0.7275 Test: 0.6540\n",
      "Epoch: 370, Loss: 0.1113 tsm_loss: 0.0002 reg_loss: 0.1111 N_Y: 405772 N_S: 9204 N: 4284 N_HV: 000 Val: 0.7352 Test: 0.6459\n",
      "Epoch: 371, Loss: 0.1381 tsm_loss: 0.0000 reg_loss: 0.1381 N_Y: 405417 N_S: 8586 N: 3874 N_HV: 000 Val: 0.7164 Test: 0.6450\n",
      "Epoch: 372, Loss: 0.1379 tsm_loss: 0.0001 reg_loss: 0.1379 N_Y: 406648 N_S: 8582 N: 3937 N_HV: 000 Val: 0.7643 Test: 0.6934\n",
      "Epoch: 373, Loss: 0.1587 tsm_loss: 0.0003 reg_loss: 0.1584 N_Y: 407996 N_S: 8917 N: 4045 N_HV: 000 Val: 0.7233 Test: 0.6437\n",
      "Epoch: 374, Loss: 0.1328 tsm_loss: 0.0000 reg_loss: 0.1328 N_Y: 405927 N_S: 9927 N: 4506 N_HV: 000 Val: 0.7195 Test: 0.6458\n",
      "Epoch: 375, Loss: 0.1520 tsm_loss: 0.0000 reg_loss: 0.1520 N_Y: 405277 N_S: 9065 N: 4156 N_HV: 000 Val: 0.7172 Test: 0.6357\n",
      "Epoch: 376, Loss: 0.1148 tsm_loss: 0.0000 reg_loss: 0.1148 N_Y: 405724 N_S: 8556 N: 4037 N_HV: 000 Val: 0.7653 Test: 0.6880\n",
      "Epoch: 377, Loss: 0.1304 tsm_loss: 0.0000 reg_loss: 0.1304 N_Y: 406675 N_S: 7913 N: 3293 N_HV: 000 Val: 0.7317 Test: 0.6631\n",
      "Epoch: 378, Loss: 0.1432 tsm_loss: 0.0000 reg_loss: 0.1432 N_Y: 406793 N_S: 8259 N: 3748 N_HV: 000 Val: 0.7158 Test: 0.6392\n",
      "Epoch: 379, Loss: 0.1224 tsm_loss: 0.0000 reg_loss: 0.1224 N_Y: 405124 N_S: 7982 N: 3301 N_HV: 000 Val: 0.7040 Test: 0.6391\n",
      "Epoch: 380, Loss: 0.1329 tsm_loss: 0.0000 reg_loss: 0.1329 N_Y: 405105 N_S: 9124 N: 4068 N_HV: 000 Val: 0.7440 Test: 0.6738\n",
      "Epoch: 381, Loss: 0.1429 tsm_loss: 0.0000 reg_loss: 0.1429 N_Y: 406296 N_S: 9175 N: 3885 N_HV: 000 Val: 0.7453 Test: 0.6508\n",
      "Epoch: 382, Loss: 0.1286 tsm_loss: 0.0000 reg_loss: 0.1286 N_Y: 405211 N_S: 8595 N: 3934 N_HV: 000 Val: 0.7269 Test: 0.6512\n",
      "Epoch: 383, Loss: 0.1262 tsm_loss: 0.0000 reg_loss: 0.1262 N_Y: 404147 N_S: 8635 N: 3781 N_HV: 000 Val: 0.7194 Test: 0.6331\n",
      "Epoch: 384, Loss: 0.1298 tsm_loss: 0.0000 reg_loss: 0.1298 N_Y: 406786 N_S: 8800 N: 4014 N_HV: 000 Val: 0.7178 Test: 0.6331\n",
      "Epoch: 385, Loss: 0.1194 tsm_loss: 0.0000 reg_loss: 0.1194 N_Y: 407053 N_S: 8654 N: 3789 N_HV: 000 Val: 0.7402 Test: 0.6674\n",
      "Epoch: 386, Loss: 0.1380 tsm_loss: 0.0000 reg_loss: 0.1380 N_Y: 405573 N_S: 8960 N: 3805 N_HV: 000 Val: 0.7338 Test: 0.6603\n",
      "Epoch: 387, Loss: 0.1118 tsm_loss: 0.0000 reg_loss: 0.1118 N_Y: 406838 N_S: 8341 N: 3506 N_HV: 000 Val: 0.7382 Test: 0.6588\n",
      "Epoch: 388, Loss: 0.1149 tsm_loss: 0.0008 reg_loss: 0.1140 N_Y: 407879 N_S: 8691 N: 4136 N_HV: 001 Val: 0.7202 Test: 0.6383\n",
      "Epoch: 389, Loss: 0.1202 tsm_loss: 0.0000 reg_loss: 0.1202 N_Y: 404104 N_S: 8973 N: 3891 N_HV: 000 Val: 0.7396 Test: 0.6488\n",
      "Epoch: 390, Loss: 0.1404 tsm_loss: 0.0000 reg_loss: 0.1404 N_Y: 405536 N_S: 9087 N: 3989 N_HV: 000 Val: 0.7506 Test: 0.6254\n",
      "Epoch: 391, Loss: 0.1144 tsm_loss: 0.0000 reg_loss: 0.1144 N_Y: 406087 N_S: 8120 N: 3891 N_HV: 000 Val: 0.7270 Test: 0.6532\n",
      "Epoch: 392, Loss: 0.1273 tsm_loss: 0.0000 reg_loss: 0.1273 N_Y: 406017 N_S: 8415 N: 3672 N_HV: 000 Val: 0.7162 Test: 0.6407\n",
      "Epoch: 393, Loss: 0.1175 tsm_loss: 0.0000 reg_loss: 0.1175 N_Y: 402317 N_S: 9434 N: 4051 N_HV: 000 Val: 0.7337 Test: 0.6674\n",
      "Epoch: 394, Loss: 0.1268 tsm_loss: 0.0000 reg_loss: 0.1268 N_Y: 406700 N_S: 8255 N: 3891 N_HV: 000 Val: 0.7270 Test: 0.6606\n",
      "Epoch: 395, Loss: 0.1237 tsm_loss: 0.0001 reg_loss: 0.1236 N_Y: 405928 N_S: 9178 N: 3911 N_HV: 000 Val: 0.7369 Test: 0.6355\n",
      "Epoch: 396, Loss: 0.1187 tsm_loss: 0.0000 reg_loss: 0.1187 N_Y: 406150 N_S: 8457 N: 3757 N_HV: 000 Val: 0.7506 Test: 0.6472\n",
      "Epoch: 397, Loss: 0.1411 tsm_loss: 0.0000 reg_loss: 0.1411 N_Y: 406108 N_S: 8529 N: 3573 N_HV: 000 Val: 0.7337 Test: 0.6423\n",
      "Epoch: 398, Loss: 0.1549 tsm_loss: 0.0000 reg_loss: 0.1549 N_Y: 406194 N_S: 9192 N: 4045 N_HV: 000 Val: 0.7460 Test: 0.6519\n",
      "Epoch: 399, Loss: 0.1433 tsm_loss: 0.0000 reg_loss: 0.1433 N_Y: 405980 N_S: 7605 N: 3317 N_HV: 000 Val: 0.7272 Test: 0.6377\n",
      "Epoch: 400, Loss: 0.1337 tsm_loss: 0.0000 reg_loss: 0.1337 N_Y: 406868 N_S: 9112 N: 3681 N_HV: 000 Val: 0.7153 Test: 0.6402\n",
      "Epoch: 401, Loss: 0.1322 tsm_loss: 0.0000 reg_loss: 0.1321 N_Y: 406012 N_S: 9905 N: 4443 N_HV: 000 Val: 0.7284 Test: 0.6451\n",
      "Epoch: 402, Loss: 0.1188 tsm_loss: 0.0000 reg_loss: 0.1188 N_Y: 405777 N_S: 8439 N: 3573 N_HV: 000 Val: 0.7222 Test: 0.6594\n",
      "Epoch: 403, Loss: 0.1189 tsm_loss: 0.0000 reg_loss: 0.1189 N_Y: 405863 N_S: 8873 N: 4041 N_HV: 000 Val: 0.7172 Test: 0.6365\n",
      "Epoch: 404, Loss: 0.1177 tsm_loss: 0.0000 reg_loss: 0.1177 N_Y: 406567 N_S: 8066 N: 3245 N_HV: 000 Val: 0.7188 Test: 0.6379\n",
      "Epoch: 405, Loss: 0.1130 tsm_loss: 0.0000 reg_loss: 0.1130 N_Y: 404835 N_S: 8450 N: 3798 N_HV: 000 Val: 0.7414 Test: 0.6572\n",
      "Epoch: 406, Loss: 0.1883 tsm_loss: 0.0000 reg_loss: 0.1883 N_Y: 406385 N_S: 8481 N: 3898 N_HV: 000 Val: 0.7742 Test: 0.7070\n",
      "Epoch: 407, Loss: 0.1775 tsm_loss: 0.0000 reg_loss: 0.1775 N_Y: 405890 N_S: 7550 N: 3201 N_HV: 000 Val: 0.7336 Test: 0.6474\n",
      "Epoch: 408, Loss: 0.1688 tsm_loss: 0.0000 reg_loss: 0.1688 N_Y: 404739 N_S: 8509 N: 3557 N_HV: 000 Val: 0.7233 Test: 0.6482\n",
      "Epoch: 409, Loss: 0.1508 tsm_loss: 0.0000 reg_loss: 0.1508 N_Y: 407854 N_S: 9198 N: 3908 N_HV: 000 Val: 0.7500 Test: 0.7003\n",
      "Epoch: 410, Loss: 0.1505 tsm_loss: 0.0000 reg_loss: 0.1505 N_Y: 405691 N_S: 7942 N: 3754 N_HV: 000 Val: 0.7210 Test: 0.6508\n",
      "Epoch: 411, Loss: 0.1191 tsm_loss: 0.0000 reg_loss: 0.1191 N_Y: 406351 N_S: 9098 N: 3940 N_HV: 000 Val: 0.7245 Test: 0.6260\n",
      "Epoch: 412, Loss: 0.1235 tsm_loss: 0.0000 reg_loss: 0.1235 N_Y: 405283 N_S: 8419 N: 3665 N_HV: 000 Val: 0.7283 Test: 0.6515\n",
      "Epoch: 413, Loss: 0.1255 tsm_loss: 0.0001 reg_loss: 0.1254 N_Y: 405618 N_S: 8811 N: 4148 N_HV: 000 Val: 0.7711 Test: 0.6748\n",
      "Epoch: 414, Loss: 0.1601 tsm_loss: 0.0000 reg_loss: 0.1601 N_Y: 405763 N_S: 7992 N: 3762 N_HV: 000 Val: 0.7283 Test: 0.6359\n",
      "Epoch: 415, Loss: 0.1267 tsm_loss: 0.0000 reg_loss: 0.1267 N_Y: 404405 N_S: 9169 N: 4106 N_HV: 000 Val: 0.7111 Test: 0.6356\n",
      "Epoch: 416, Loss: 0.1291 tsm_loss: 0.0000 reg_loss: 0.1291 N_Y: 406339 N_S: 8508 N: 3765 N_HV: 000 Val: 0.7164 Test: 0.6209\n",
      "Epoch: 417, Loss: 0.1283 tsm_loss: 0.0000 reg_loss: 0.1283 N_Y: 407131 N_S: 7743 N: 3319 N_HV: 000 Val: 0.7348 Test: 0.6278\n",
      "Epoch: 418, Loss: 0.1121 tsm_loss: 0.0001 reg_loss: 0.1120 N_Y: 406599 N_S: 8600 N: 3610 N_HV: 000 Val: 0.7387 Test: 0.6588\n",
      "Epoch: 419, Loss: 0.1159 tsm_loss: 0.0000 reg_loss: 0.1159 N_Y: 405727 N_S: 8642 N: 3957 N_HV: 000 Val: 0.7194 Test: 0.6250\n",
      "Epoch: 420, Loss: 0.1166 tsm_loss: 0.0000 reg_loss: 0.1166 N_Y: 406040 N_S: 8818 N: 3923 N_HV: 000 Val: 0.7261 Test: 0.6394\n",
      "Epoch: 421, Loss: 0.1075 tsm_loss: 0.0000 reg_loss: 0.1075 N_Y: 406017 N_S: 8370 N: 3868 N_HV: 000 Val: 0.7110 Test: 0.6378\n",
      "Epoch: 422, Loss: 0.1098 tsm_loss: 0.0001 reg_loss: 0.1097 N_Y: 405988 N_S: 7867 N: 3562 N_HV: 000 Val: 0.7433 Test: 0.6557\n",
      "Epoch: 423, Loss: 0.1234 tsm_loss: 0.0001 reg_loss: 0.1234 N_Y: 404829 N_S: 8058 N: 3587 N_HV: 000 Val: 0.7278 Test: 0.6556\n",
      "Epoch: 424, Loss: 0.1152 tsm_loss: 0.0000 reg_loss: 0.1152 N_Y: 406745 N_S: 7965 N: 3348 N_HV: 000 Val: 0.7359 Test: 0.6561\n",
      "Epoch: 425, Loss: 0.1250 tsm_loss: 0.0000 reg_loss: 0.1250 N_Y: 404851 N_S: 9024 N: 4080 N_HV: 000 Val: 0.7183 Test: 0.6438\n",
      "Epoch: 426, Loss: 0.1209 tsm_loss: 0.0000 reg_loss: 0.1209 N_Y: 404844 N_S: 7724 N: 3277 N_HV: 000 Val: 0.7383 Test: 0.6375\n",
      "Epoch: 427, Loss: 0.1082 tsm_loss: 0.0000 reg_loss: 0.1081 N_Y: 407267 N_S: 9008 N: 3931 N_HV: 000 Val: 0.7152 Test: 0.6190\n",
      "Epoch: 428, Loss: 0.1382 tsm_loss: 0.0000 reg_loss: 0.1382 N_Y: 406832 N_S: 8837 N: 3827 N_HV: 000 Val: 0.7197 Test: 0.6406\n",
      "Epoch: 429, Loss: 0.1129 tsm_loss: 0.0000 reg_loss: 0.1129 N_Y: 405685 N_S: 8077 N: 3408 N_HV: 000 Val: 0.7200 Test: 0.6481\n",
      "Epoch: 430, Loss: 0.1157 tsm_loss: 0.0000 reg_loss: 0.1157 N_Y: 406369 N_S: 9172 N: 3991 N_HV: 000 Val: 0.7293 Test: 0.6532\n",
      "Epoch: 431, Loss: 0.1041 tsm_loss: 0.0000 reg_loss: 0.1041 N_Y: 405507 N_S: 8999 N: 4026 N_HV: 000 Val: 0.7396 Test: 0.6544\n",
      "Epoch: 432, Loss: 0.1304 tsm_loss: 0.0000 reg_loss: 0.1304 N_Y: 405227 N_S: 9186 N: 4190 N_HV: 000 Val: 0.7344 Test: 0.6215\n",
      "Epoch: 433, Loss: 0.1088 tsm_loss: 0.0000 reg_loss: 0.1088 N_Y: 406365 N_S: 8156 N: 3749 N_HV: 000 Val: 0.7063 Test: 0.6302\n",
      "Epoch: 434, Loss: 0.1059 tsm_loss: 0.0000 reg_loss: 0.1059 N_Y: 406852 N_S: 8826 N: 3943 N_HV: 000 Val: 0.7270 Test: 0.6439\n",
      "Epoch: 435, Loss: 0.0952 tsm_loss: 0.0001 reg_loss: 0.0951 N_Y: 406311 N_S: 8067 N: 3622 N_HV: 000 Val: 0.7437 Test: 0.6746\n",
      "Epoch: 436, Loss: 0.1054 tsm_loss: 0.0000 reg_loss: 0.1054 N_Y: 407414 N_S: 8953 N: 3919 N_HV: 000 Val: 0.7218 Test: 0.6416\n",
      "Epoch: 437, Loss: 0.1332 tsm_loss: 0.0002 reg_loss: 0.1330 N_Y: 406175 N_S: 8400 N: 3621 N_HV: 000 Val: 0.7214 Test: 0.6403\n",
      "Epoch: 438, Loss: 0.1165 tsm_loss: 0.0001 reg_loss: 0.1164 N_Y: 405294 N_S: 8876 N: 3987 N_HV: 000 Val: 0.7375 Test: 0.6600\n",
      "Epoch: 439, Loss: 0.1224 tsm_loss: 0.0004 reg_loss: 0.1220 N_Y: 406234 N_S: 9132 N: 3950 N_HV: 001 Val: 0.7450 Test: 0.6541\n",
      "Epoch: 440, Loss: 0.1448 tsm_loss: 0.0002 reg_loss: 0.1446 N_Y: 406603 N_S: 8589 N: 3696 N_HV: 000 Val: 0.7486 Test: 0.6434\n",
      "Epoch: 441, Loss: 0.1047 tsm_loss: 0.0000 reg_loss: 0.1047 N_Y: 407237 N_S: 8057 N: 3519 N_HV: 000 Val: 0.7356 Test: 0.6510\n",
      "Epoch: 442, Loss: 0.1032 tsm_loss: 0.0000 reg_loss: 0.1032 N_Y: 405782 N_S: 8185 N: 4013 N_HV: 000 Val: 0.7830 Test: 0.7021\n",
      "Epoch: 443, Loss: 0.1563 tsm_loss: 0.0000 reg_loss: 0.1563 N_Y: 406100 N_S: 8925 N: 4091 N_HV: 000 Val: 0.7017 Test: 0.6290\n",
      "Epoch: 444, Loss: 0.1230 tsm_loss: 0.0000 reg_loss: 0.1230 N_Y: 406517 N_S: 8657 N: 3727 N_HV: 000 Val: 0.7192 Test: 0.6311\n",
      "Epoch: 445, Loss: 0.1490 tsm_loss: 0.0000 reg_loss: 0.1490 N_Y: 405435 N_S: 8619 N: 3829 N_HV: 000 Val: 0.7227 Test: 0.6413\n",
      "Epoch: 446, Loss: 0.0980 tsm_loss: 0.0000 reg_loss: 0.0979 N_Y: 403214 N_S: 8907 N: 3945 N_HV: 000 Val: 0.7192 Test: 0.6510\n",
      "Epoch: 447, Loss: 0.1014 tsm_loss: 0.0000 reg_loss: 0.1014 N_Y: 405965 N_S: 8677 N: 3892 N_HV: 000 Val: 0.7278 Test: 0.6295\n",
      "Epoch: 448, Loss: 0.1715 tsm_loss: 0.0002 reg_loss: 0.1713 N_Y: 407486 N_S: 8118 N: 3484 N_HV: 000 Val: 0.7185 Test: 0.6415\n",
      "Epoch: 449, Loss: 0.1145 tsm_loss: 0.0000 reg_loss: 0.1145 N_Y: 405937 N_S: 8914 N: 3706 N_HV: 000 Val: 0.7176 Test: 0.6572\n",
      "Epoch: 450, Loss: 0.1232 tsm_loss: 0.0000 reg_loss: 0.1232 N_Y: 404943 N_S: 9613 N: 4124 N_HV: 000 Val: 0.7116 Test: 0.6407\n",
      "Epoch: 451, Loss: 0.0975 tsm_loss: 0.0000 reg_loss: 0.0975 N_Y: 406571 N_S: 7699 N: 3474 N_HV: 000 Val: 0.7170 Test: 0.6392\n",
      "Epoch: 452, Loss: 0.0940 tsm_loss: 0.0001 reg_loss: 0.0939 N_Y: 404846 N_S: 8188 N: 3612 N_HV: 000 Val: 0.7214 Test: 0.6312\n",
      "Epoch: 453, Loss: 0.0986 tsm_loss: 0.0000 reg_loss: 0.0986 N_Y: 406886 N_S: 8415 N: 3464 N_HV: 000 Val: 0.7203 Test: 0.6331\n",
      "Epoch: 454, Loss: 0.1264 tsm_loss: 0.0000 reg_loss: 0.1263 N_Y: 405700 N_S: 8106 N: 3597 N_HV: 000 Val: 0.7254 Test: 0.6305\n",
      "Epoch: 455, Loss: 0.0998 tsm_loss: 0.0001 reg_loss: 0.0997 N_Y: 405408 N_S: 8409 N: 3574 N_HV: 000 Val: 0.7289 Test: 0.6422\n",
      "Epoch: 456, Loss: 0.1018 tsm_loss: 0.0000 reg_loss: 0.1018 N_Y: 406468 N_S: 8042 N: 3439 N_HV: 000 Val: 0.7642 Test: 0.6717\n",
      "Epoch: 457, Loss: 0.1076 tsm_loss: 0.0000 reg_loss: 0.1076 N_Y: 405611 N_S: 8577 N: 4061 N_HV: 000 Val: 0.7232 Test: 0.6382\n",
      "Epoch: 458, Loss: 0.1041 tsm_loss: 0.0001 reg_loss: 0.1040 N_Y: 405630 N_S: 8759 N: 3861 N_HV: 000 Val: 0.7349 Test: 0.6386\n",
      "Epoch: 459, Loss: 0.1217 tsm_loss: 0.0000 reg_loss: 0.1217 N_Y: 406251 N_S: 8965 N: 3989 N_HV: 000 Val: 0.7266 Test: 0.6367\n",
      "Epoch: 460, Loss: 0.1116 tsm_loss: 0.0010 reg_loss: 0.1106 N_Y: 406969 N_S: 8748 N: 3869 N_HV: 000 Val: 0.7284 Test: 0.6439\n",
      "Epoch: 461, Loss: 0.1153 tsm_loss: 0.0001 reg_loss: 0.1153 N_Y: 407199 N_S: 8575 N: 3783 N_HV: 000 Val: 0.7223 Test: 0.6352\n",
      "Epoch: 462, Loss: 0.1336 tsm_loss: 0.0000 reg_loss: 0.1336 N_Y: 403692 N_S: 8056 N: 3354 N_HV: 000 Val: 0.7400 Test: 0.6505\n"
     ]
    }
   ],
   "source": [
    "# train, valid, test splitting\n",
    "res1 = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "for seed in [8, 16, 24, 42, 64, 128, 256, 512, 1024, 2048]: #, \n",
    "    dataset = Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42)\n",
    "    N = len(dataset) // 5\n",
    "    val_dataset = dataset[:N]\n",
    "    test_dataset = dataset[N:2 * N]\n",
    "    train_dataset = dataset[2 * N:]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    deg = get_deg(train_dataset)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # With AC-Awareness and structure gate\n",
    "    df3 = Test_performance(alpha=1.0, similarity_gate = True)\n",
    "    df3['seed'] = seed\n",
    "    \n",
    "    # With AC-Awareness ($\\alpha = 1$)\n",
    "    df1 = Test_performance(alpha=1.0, similarity_gate = False)\n",
    "    df1['seed'] = seed\n",
    "    # Without AC-Awareness ($\\alpha = 0$)\n",
    "    df2 = Test_performance(alpha=0.0, similarity_gate = False)\n",
    "    df2['seed'] = seed\n",
    "\n",
    "    \n",
    "    res1.append(df1)\n",
    "    res2.append(df2) #212814\n",
    "    res3.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706585de-fc76-487c-a8c9-f90a3cc76fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    res1.append(df1)\n",
    "    res2.append(df2) #212814\n",
    "    res3.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59ad23-a5a4-4366-833d-001a20e59ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(res1)\n",
    "df2 = pd.concat(res2)\n",
    "df3 = pd.concat(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31c76b-0328-4014-8c25-a783ad1a785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./with_aca.csv')\n",
    "df2.to_csv('./without_aca.csv')\n",
    "df3.to_csv('./without_aca_with_gate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995c527-19cf-43d0-ada5-b151a620b906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a95e88-f850-48a7-8f0b-f0fe3ed48878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "colors = ['#FFE699','#00B0F0','red']\n",
    "\n",
    "y = 'val_rmse'\n",
    "\n",
    "n1 = r'With AC-awareness' # ($\\mathcal{L}_{mae}$)\n",
    "n2 = r'Without AC-awareness'\n",
    "n3 = r'With AC-awareness and structure gate'\n",
    "\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').mean()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').std()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n3], (dfp + dfp_std)[n3], color=colors[2], alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.60, 1.0)\n",
    "ax.set_ylabel('Validation RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "#ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Validation_RMSE.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Validation_RMSE.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae243a-9b9d-4001-baab-16b8750003a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110c566-a71d-4d5c-afe0-185322298de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ab1d9-b7b4-4ec4-bc0c-d46590478bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "y = 'test_rmse'\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').mean()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').std()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n3], (dfp + dfp_std)[n3], color=colors[2], alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.60, 1.0)\n",
    "ax.set_ylabel('Test RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "#ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Test_RMSE.svg' , bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Test_RMSE.pdf' , bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7e564-cf50-4d1f-b7d0-c9a7fa127e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "\n",
    "y = 'train_n_hv_triplets'\n",
    "\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').mean()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').std()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n3], (dfp + dfp_std)[n3], color=colors[2], alpha=0.2)\n",
    "\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "ax.set_ylabel(\"No. of HV-ACTs ($M^'$)\")\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "#ax.set_xlim(-5,800)\n",
    "\n",
    "\n",
    "fig.savefig('./Number_of_mined_ACTs_during_training.svg' , bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Number_of_mined_ACTs_during_training.pdf' , bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b16ad-c854-408d-9fcc-9ec0014dc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "y = 'train_triplet_loss'\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').mean()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').std()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n3], (dfp + dfp_std)[n3], color=colors[2], alpha=0.2)\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "# ax.set_xlim(-5,800)\n",
    "ax.set_ylim(-1,10)\n",
    "\n",
    "ax.set_ylabel('Training TSM Loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Triplet_loss_during_training.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Triplet_loss_during_training.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27b9a-5db5-4c6d-a50f-018db4be7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y = 'train_reg_loss'\n",
    "\n",
    "dfp = df2.groupby('Epoch').mean()[y].to_frame(name = n2).join(df1.groupby('Epoch').mean()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').mean()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "dfp_std = df2.groupby('Epoch').std()[y].to_frame(name = n2).join(df1.groupby('Epoch').std()[y].to_frame(name = n1)).rolling(1).mean().join(df3.groupby('Epoch').std()[y].to_frame(name = n3)).rolling(1).mean()\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n1], (dfp + dfp_std)[n1], color=colors[1], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n2], (dfp + dfp_std)[n2], color=colors[0], alpha=0.2)\n",
    "ax.fill_between(dfp.index, (dfp - dfp_std)[n3], (dfp + dfp_std)[n3], color=colors[2], alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.0, 0.8)\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_ylabel('Training MAE loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "#ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./Train_mae_los.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./Train_mae_los.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314788a9-afe4-486c-a861-9e1c5708f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f5326-f4d7-42ae-935c-c20f2df351a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8aa35-2eaa-4c90-8168-4d62ff358d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4f511-db6b-4395-b2bd-2e993b9167f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16898f-b4bb-46ae-8d18-229ad9ab42b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d62189-e6b5-4d5c-8e51-bafc0c9fe2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
