{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44813ba1-cf79-4aca-85a4-c074cf81ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "\n",
    "\n",
    "class GenAttentiveFeatures(object):\n",
    "    '''\n",
    "    AttentiveFP 39 node features generation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.symbols = [\n",
    "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
    "            'Te', 'I', 'At', 'other'\n",
    "        ]\n",
    "\n",
    "        self.hybridizations = [\n",
    "            Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D,\n",
    "            Chem.rdchem.HybridizationType.SP3D2,\n",
    "            'other',\n",
    "        ]\n",
    "\n",
    "        self.stereos = [\n",
    "            Chem.rdchem.BondStereo.STEREONONE,\n",
    "            Chem.rdchem.BondStereo.STEREOANY,\n",
    "            Chem.rdchem.BondStereo.STEREOZ,\n",
    "            Chem.rdchem.BondStereo.STEREOE,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = [0.] * len(self.symbols)\n",
    "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
    "            degree = [0.] * 6\n",
    "            degree[atom.GetDegree()] = 1.\n",
    "            formal_charge = atom.GetFormalCharge()\n",
    "            radical_electrons = atom.GetNumRadicalElectrons()\n",
    "            hybridization = [0.] * len(self.hybridizations)\n",
    "            hybridization[self.hybridizations.index(\n",
    "                atom.GetHybridization())] = 1.\n",
    "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
    "            hydrogens = [0.] * 5\n",
    "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
    "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
    "            chirality_type = [0.] * 2\n",
    "            if atom.HasProp('_CIPCode'):\n",
    "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
    "\n",
    "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
    "                             [radical_electrons] + hybridization +\n",
    "                             [aromaticity] + hydrogens + [chirality] +\n",
    "                             chirality_type)\n",
    "            xs.append(x)\n",
    "\n",
    "        data.x = torch.stack(xs, dim=0)\n",
    "\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
    "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
    "\n",
    "            bond_type = bond.GetBondType()\n",
    "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
    "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
    "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
    "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
    "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
    "            ring = 1. if bond.IsInRing() else 0.\n",
    "            stereo = [0.] * 4\n",
    "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
    "\n",
    "            edge_attr = torch.tensor(\n",
    "                [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
    "\n",
    "            edge_attrs += [edge_attr, edge_attr]\n",
    "\n",
    "        if len(edge_attrs) == 0:\n",
    "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
    "        else:\n",
    "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class GenAtomFeatures(object):\n",
    "    '''\n",
    "    Our features: @todo\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04ff461-3e16-4460-89ff-fb13aef52541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mglur2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/bidd-group/LSSinhibitors/main/data/mGluR2.csv\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 5.2638 Val: 2.7361 Test: 3.1182\n",
      "Epoch: 002, Loss: 2.8597 Val: 1.0392 Test: 1.1940\n",
      "Epoch: 003, Loss: 1.1562 Val: 1.0386 Test: 1.2838\n",
      "Epoch: 004, Loss: 1.0233 Val: 0.9924 Test: 1.0265\n",
      "Epoch: 005, Loss: 0.9474 Val: 0.6431 Test: 1.2452\n",
      "Epoch: 006, Loss: 1.1027 Val: 0.3823 Test: 0.8079\n",
      "Epoch: 007, Loss: 1.0575 Val: 0.5822 Test: 0.9224\n",
      "Epoch: 008, Loss: 0.9790 Val: 0.7652 Test: 0.5707\n",
      "Epoch: 009, Loss: 1.2451 Val: 1.3015 Test: 0.9719\n",
      "Epoch: 010, Loss: 1.1985 Val: 0.6167 Test: 0.6222\n",
      "Epoch: 011, Loss: 1.1093 Val: 1.1619 Test: 0.9489\n",
      "Epoch: 012, Loss: 1.0772 Val: 0.5794 Test: 0.6898\n",
      "Epoch: 013, Loss: 1.0647 Val: 0.8794 Test: 0.8425\n",
      "Epoch: 014, Loss: 1.0211 Val: 0.9489 Test: 0.3953\n",
      "Epoch: 015, Loss: 0.8168 Val: 0.6625 Test: 0.7807\n",
      "Epoch: 016, Loss: 0.8211 Val: 0.4875 Test: 0.8978\n",
      "Epoch: 017, Loss: 0.9327 Val: 0.5638 Test: 0.5655\n",
      "Epoch: 018, Loss: 1.0621 Val: 0.6356 Test: 0.6743\n",
      "Epoch: 019, Loss: 0.6539 Val: 0.4807 Test: 0.7503\n",
      "Epoch: 020, Loss: 1.1352 Val: 0.9272 Test: 0.5853\n",
      "Epoch: 021, Loss: 0.8918 Val: 0.7710 Test: 0.5520\n",
      "Epoch: 022, Loss: 0.7686 Val: 0.5674 Test: 0.8949\n",
      "Epoch: 023, Loss: 0.8894 Val: 1.1336 Test: 0.9265\n",
      "Epoch: 024, Loss: 0.6852 Val: 0.8605 Test: 0.6796\n",
      "Epoch: 025, Loss: 0.7035 Val: 0.4472 Test: 0.5874\n",
      "Epoch: 026, Loss: 0.9172 Val: 1.0853 Test: 0.8061\n",
      "Epoch: 027, Loss: 0.8467 Val: 0.6490 Test: 0.5976\n",
      "Epoch: 028, Loss: 0.6728 Val: 0.3980 Test: 0.4946\n",
      "Epoch: 029, Loss: 0.7507 Val: 0.6525 Test: 0.6649\n",
      "Epoch: 030, Loss: 0.8456 Val: 0.8493 Test: 0.4183\n",
      "Epoch: 031, Loss: 0.7278 Val: 0.8598 Test: 0.3511\n",
      "Epoch: 032, Loss: 0.7031 Val: 0.6209 Test: 0.7498\n",
      "Epoch: 033, Loss: 0.7356 Val: 0.7600 Test: 0.5061\n",
      "Epoch: 034, Loss: 0.6407 Val: 1.0005 Test: 0.7762\n",
      "Epoch: 035, Loss: 0.7255 Val: 0.8513 Test: 0.7900\n",
      "Epoch: 036, Loss: 0.6891 Val: 0.8812 Test: 0.3576\n",
      "Epoch: 037, Loss: 0.7610 Val: 0.3993 Test: 0.8006\n",
      "Epoch: 038, Loss: 0.5783 Val: 0.9886 Test: 0.7936\n",
      "Epoch: 039, Loss: 0.9277 Val: 0.6028 Test: 0.7151\n",
      "Epoch: 040, Loss: 0.6602 Val: 0.7658 Test: 0.6490\n",
      "Epoch: 041, Loss: 0.7138 Val: 0.7181 Test: 0.2972\n",
      "Epoch: 042, Loss: 0.6953 Val: 0.4798 Test: 0.9018\n",
      "Epoch: 043, Loss: 0.7973 Val: 0.9976 Test: 0.7718\n",
      "Epoch: 044, Loss: 0.7849 Val: 0.7007 Test: 0.6238\n",
      "Epoch: 045, Loss: 0.7319 Val: 0.6089 Test: 0.4257\n",
      "Epoch: 046, Loss: 0.5936 Val: 0.4842 Test: 0.4161\n",
      "Epoch: 047, Loss: 0.5265 Val: 0.6947 Test: 0.9734\n",
      "Epoch: 048, Loss: 0.6895 Val: 0.7522 Test: 0.6924\n",
      "Epoch: 049, Loss: 0.7964 Val: 0.1920 Test: 0.4062\n",
      "usp7\n",
      "Epoch: 001, Loss: 5.6159 Val: 12.0397 Test: 11.8499\n",
      "Epoch: 002, Loss: 8.1138 Val: 2.6698 Test: 3.2879\n",
      "Epoch: 003, Loss: 3.2894 Val: 3.8182 Test: 4.2349\n",
      "Epoch: 004, Loss: 3.7129 Val: 3.2778 Test: 3.2012\n",
      "Epoch: 005, Loss: 2.6970 Val: 0.9691 Test: 1.5420\n",
      "Epoch: 006, Loss: 1.4772 Val: 2.1701 Test: 2.2416\n",
      "Epoch: 007, Loss: 2.4843 Val: 1.6034 Test: 0.9535\n",
      "Epoch: 008, Loss: 1.5135 Val: 0.9005 Test: 0.9257\n",
      "Epoch: 009, Loss: 1.5275 Val: 1.4716 Test: 1.9026\n",
      "Epoch: 010, Loss: 1.5429 Val: 1.2928 Test: 1.7244\n",
      "Epoch: 011, Loss: 1.2889 Val: 0.8632 Test: 1.0651\n",
      "Epoch: 012, Loss: 1.2464 Val: 0.8997 Test: 0.9054\n",
      "Epoch: 013, Loss: 1.0733 Val: 0.8981 Test: 0.4877\n",
      "Epoch: 014, Loss: 1.2479 Val: 0.8154 Test: 0.6171\n",
      "Epoch: 015, Loss: 0.8018 Val: 0.5434 Test: 0.8011\n",
      "Epoch: 016, Loss: 0.7955 Val: 0.8300 Test: 0.7659\n",
      "Epoch: 017, Loss: 0.9345 Val: 0.9108 Test: 1.1877\n",
      "Epoch: 018, Loss: 0.7041 Val: 0.8382 Test: 0.8650\n",
      "Epoch: 019, Loss: 1.1472 Val: 0.8010 Test: 0.4418\n",
      "Epoch: 020, Loss: 0.8317 Val: 0.4598 Test: 0.8113\n",
      "Epoch: 021, Loss: 1.0686 Val: 0.6807 Test: 1.2562\n",
      "Epoch: 022, Loss: 1.2460 Val: 0.3642 Test: 1.0114\n",
      "Epoch: 023, Loss: 1.0217 Val: 0.6338 Test: 0.7460\n",
      "Epoch: 024, Loss: 1.0811 Val: 0.6928 Test: 1.0661\n",
      "Epoch: 025, Loss: 0.9275 Val: 0.8385 Test: 1.2776\n",
      "Epoch: 026, Loss: 1.1908 Val: 0.8877 Test: 0.7169\n",
      "Epoch: 027, Loss: 0.9627 Val: 0.9917 Test: 0.4845\n",
      "Epoch: 028, Loss: 0.9601 Val: 1.0844 Test: 0.6142\n",
      "Epoch: 029, Loss: 1.0435 Val: 0.6446 Test: 0.7681\n",
      "Epoch: 030, Loss: 0.8022 Val: 1.0025 Test: 0.7779\n",
      "Epoch: 031, Loss: 1.0322 Val: 0.6688 Test: 0.8306\n",
      "Epoch: 032, Loss: 0.8270 Val: 0.6136 Test: 1.0819\n",
      "Epoch: 033, Loss: 0.6703 Val: 0.9571 Test: 1.1632\n",
      "Epoch: 034, Loss: 0.8236 Val: 0.9971 Test: 0.8663\n",
      "Epoch: 035, Loss: 0.9440 Val: 0.8223 Test: 0.4781\n",
      "Epoch: 036, Loss: 0.7807 Val: 0.5855 Test: 0.8007\n",
      "Epoch: 037, Loss: 0.9223 Val: 0.6406 Test: 0.5913\n",
      "Epoch: 038, Loss: 0.9211 Val: 0.7765 Test: 0.9683\n",
      "Epoch: 039, Loss: 0.6704 Val: 0.6416 Test: 0.4595\n",
      "Epoch: 040, Loss: 0.8001 Val: 1.0094 Test: 0.6307\n",
      "Epoch: 041, Loss: 0.6828 Val: 0.3600 Test: 0.5824\n",
      "Epoch: 042, Loss: 0.9614 Val: 0.6879 Test: 1.1475\n",
      "Epoch: 043, Loss: 0.7286 Val: 1.1018 Test: 0.9047\n",
      "Epoch: 044, Loss: 0.8420 Val: 0.6876 Test: 0.6698\n",
      "Epoch: 045, Loss: 0.8647 Val: 0.9275 Test: 1.2464\n",
      "Epoch: 046, Loss: 0.6615 Val: 0.5816 Test: 0.5655\n",
      "Epoch: 047, Loss: 0.7884 Val: 0.7625 Test: 0.8621\n",
      "Epoch: 048, Loss: 0.8601 Val: 0.7571 Test: 1.0026\n",
      "Epoch: 049, Loss: 0.8404 Val: 0.5633 Test: 0.6749\n",
      "mth1\n",
      "Epoch: 001, Loss: 5.2217 Val: 2.0205 Test: 2.9936\n",
      "Epoch: 002, Loss: 2.2813 Val: 1.7248 Test: 1.5205\n",
      "Epoch: 003, Loss: 1.3793 Val: 1.2714 Test: 2.4108\n",
      "Epoch: 004, Loss: 1.1246 Val: 1.4019 Test: 0.8160\n",
      "Epoch: 005, Loss: 1.3576 Val: 1.0584 Test: 0.9153\n",
      "Epoch: 006, Loss: 1.3534 Val: 1.1588 Test: 2.3010\n",
      "Epoch: 007, Loss: 0.9623 Val: 1.0173 Test: 1.5401\n",
      "Epoch: 008, Loss: 0.9129 Val: 0.3418 Test: 1.1723\n",
      "Epoch: 009, Loss: 0.9187 Val: 1.1198 Test: 1.4071\n",
      "Epoch: 010, Loss: 1.1183 Val: 0.5965 Test: 1.0139\n",
      "Epoch: 011, Loss: 0.8719 Val: 0.6970 Test: 1.4222\n",
      "Epoch: 012, Loss: 0.8587 Val: 1.0970 Test: 1.5502\n",
      "Epoch: 013, Loss: 0.7758 Val: 0.4535 Test: 0.6043\n",
      "Epoch: 014, Loss: 0.7939 Val: 0.8231 Test: 1.1574\n",
      "Epoch: 015, Loss: 0.6301 Val: 0.5214 Test: 1.3222\n",
      "Epoch: 016, Loss: 0.9056 Val: 0.7654 Test: 1.0620\n",
      "Epoch: 017, Loss: 0.6498 Val: 0.5411 Test: 2.1426\n",
      "Epoch: 018, Loss: 0.8108 Val: 0.8834 Test: 1.2878\n",
      "Epoch: 019, Loss: 0.8243 Val: 0.5732 Test: 1.4833\n",
      "Epoch: 020, Loss: 0.8654 Val: 0.4722 Test: 1.0181\n",
      "Epoch: 021, Loss: 0.7262 Val: 0.5428 Test: 1.2450\n",
      "Epoch: 022, Loss: 0.5724 Val: 0.5729 Test: 1.7579\n",
      "Epoch: 023, Loss: 0.8203 Val: 0.5651 Test: 1.5565\n",
      "Epoch: 024, Loss: 0.5598 Val: 0.3180 Test: 1.0349\n",
      "Epoch: 025, Loss: 0.8028 Val: 0.3093 Test: 1.2293\n",
      "Epoch: 026, Loss: 0.5963 Val: 0.5684 Test: 0.9128\n",
      "Epoch: 027, Loss: 0.6010 Val: 0.4972 Test: 1.2643\n",
      "Epoch: 028, Loss: 0.7970 Val: 0.3973 Test: 1.3565\n",
      "Epoch: 029, Loss: 0.5711 Val: 0.6459 Test: 0.9895\n",
      "Epoch: 030, Loss: 0.5564 Val: 0.3623 Test: 0.7256\n",
      "Epoch: 031, Loss: 0.5937 Val: 0.6041 Test: 1.2164\n",
      "Epoch: 032, Loss: 0.5143 Val: 0.6170 Test: 1.4481\n",
      "Epoch: 033, Loss: 0.6028 Val: 0.5077 Test: 1.2544\n",
      "Epoch: 034, Loss: 0.6324 Val: 0.7885 Test: 1.2268\n",
      "Epoch: 035, Loss: 0.5057 Val: 0.3796 Test: 1.1090\n",
      "Epoch: 036, Loss: 0.5691 Val: 0.4952 Test: 1.2672\n",
      "Epoch: 037, Loss: 0.5837 Val: 0.7958 Test: 1.4980\n",
      "Epoch: 038, Loss: 0.7409 Val: 0.5489 Test: 1.3400\n",
      "Epoch: 039, Loss: 0.3767 Val: 0.6531 Test: 1.1824\n",
      "Epoch: 040, Loss: 0.6420 Val: 0.3252 Test: 0.9190\n",
      "Epoch: 041, Loss: 0.6418 Val: 0.5035 Test: 1.0252\n",
      "Epoch: 042, Loss: 0.4723 Val: 0.2051 Test: 1.1213\n",
      "Epoch: 043, Loss: 0.5453 Val: 0.4948 Test: 1.5070\n",
      "Epoch: 044, Loss: 0.6786 Val: 0.3593 Test: 1.2594\n",
      "Epoch: 045, Loss: 0.6102 Val: 0.6271 Test: 0.7951\n",
      "Epoch: 046, Loss: 0.6650 Val: 0.4041 Test: 1.0966\n",
      "Epoch: 047, Loss: 0.5243 Val: 0.6700 Test: 1.0529\n",
      "Epoch: 048, Loss: 0.5003 Val: 0.2299 Test: 1.1091\n",
      "Epoch: 049, Loss: 0.4470 Val: 0.3098 Test: 1.0777\n",
      "rip2\n",
      "Epoch: 001, Loss: 5.3109 Val: 2.2408 Test: 2.0220\n",
      "Epoch: 002, Loss: 1.4180 Val: 1.3426 Test: 0.7737\n",
      "Epoch: 003, Loss: 1.3415 Val: 1.2046 Test: 1.1299\n",
      "Epoch: 004, Loss: 1.1874 Val: 1.1609 Test: 0.6443\n",
      "Epoch: 005, Loss: 1.4085 Val: 1.1971 Test: 1.5231\n",
      "Epoch: 006, Loss: 1.3785 Val: 0.8411 Test: 1.0508\n",
      "Epoch: 007, Loss: 1.1437 Val: 0.8438 Test: 0.9866\n",
      "Epoch: 008, Loss: 1.1396 Val: 0.8089 Test: 0.9289\n",
      "Epoch: 009, Loss: 1.4136 Val: 0.9599 Test: 0.5205\n",
      "Epoch: 010, Loss: 1.0523 Val: 0.4791 Test: 1.0376\n",
      "Epoch: 011, Loss: 0.8016 Val: 0.7163 Test: 0.9265\n",
      "Epoch: 012, Loss: 0.9328 Val: 0.9428 Test: 0.2927\n",
      "Epoch: 013, Loss: 1.1194 Val: 0.6771 Test: 1.2397\n",
      "Epoch: 014, Loss: 0.7597 Val: 1.1163 Test: 1.0262\n",
      "Epoch: 015, Loss: 0.7804 Val: 0.7435 Test: 0.8905\n",
      "Epoch: 016, Loss: 1.2127 Val: 0.7758 Test: 0.3186\n",
      "Epoch: 017, Loss: 1.2159 Val: 1.3732 Test: 0.6784\n",
      "Epoch: 018, Loss: 1.1960 Val: 0.7447 Test: 0.6396\n",
      "Epoch: 019, Loss: 0.9750 Val: 0.7039 Test: 0.5895\n",
      "Epoch: 020, Loss: 1.1123 Val: 0.7683 Test: 0.6047\n",
      "Epoch: 021, Loss: 1.1345 Val: 0.7784 Test: 0.9482\n",
      "Epoch: 022, Loss: 1.0768 Val: 0.5038 Test: 0.7961\n",
      "Epoch: 023, Loss: 0.8232 Val: 0.5440 Test: 0.6956\n",
      "Epoch: 024, Loss: 1.0172 Val: 0.6266 Test: 0.3675\n",
      "Epoch: 025, Loss: 0.8607 Val: 0.6935 Test: 0.4329\n",
      "Epoch: 026, Loss: 0.9209 Val: 0.6149 Test: 0.8660\n",
      "Epoch: 027, Loss: 0.5966 Val: 0.4471 Test: 0.5957\n",
      "Epoch: 028, Loss: 0.7682 Val: 0.3996 Test: 0.4927\n",
      "Epoch: 029, Loss: 0.8277 Val: 0.7723 Test: 0.3881\n",
      "Epoch: 030, Loss: 0.7850 Val: 0.2713 Test: 0.6946\n",
      "Epoch: 031, Loss: 0.6766 Val: 0.8188 Test: 0.5857\n",
      "Epoch: 032, Loss: 0.6236 Val: 0.4585 Test: 0.4496\n",
      "Epoch: 033, Loss: 0.7179 Val: 0.5102 Test: 0.7257\n",
      "Epoch: 034, Loss: 0.9340 Val: 0.7190 Test: 0.5642\n",
      "Epoch: 035, Loss: 0.9632 Val: 0.4612 Test: 0.3952\n",
      "Epoch: 036, Loss: 0.8794 Val: 0.6595 Test: 0.2828\n",
      "Epoch: 037, Loss: 0.7111 Val: 0.8977 Test: 0.5166\n",
      "Epoch: 038, Loss: 0.9572 Val: 0.3070 Test: 0.9339\n",
      "Epoch: 039, Loss: 0.8839 Val: 0.3638 Test: 0.4951\n",
      "Epoch: 040, Loss: 0.5302 Val: 0.4374 Test: 0.6654\n",
      "Epoch: 041, Loss: 0.6240 Val: 0.5542 Test: 0.4526\n",
      "Epoch: 042, Loss: 0.9107 Val: 0.5755 Test: 0.4390\n",
      "Epoch: 043, Loss: 0.7574 Val: 0.5021 Test: 0.6836\n",
      "Epoch: 044, Loss: 0.6585 Val: 0.7652 Test: 0.7495\n",
      "Epoch: 045, Loss: 0.7866 Val: 0.5258 Test: 0.3719\n",
      "Epoch: 046, Loss: 0.6230 Val: 0.5410 Test: 0.5458\n",
      "Epoch: 047, Loss: 0.8165 Val: 0.4780 Test: 0.4191\n",
      "Epoch: 048, Loss: 0.8667 Val: 0.3128 Test: 0.6152\n",
      "Epoch: 049, Loss: 0.7147 Val: 0.3273 Test: 0.7994\n",
      "pkci\n",
      "Epoch: 001, Loss: 5.4175 Val: 13.2400 Test: 11.6450\n",
      "Epoch: 002, Loss: 9.1042 Val: 3.5983 Test: 3.7275\n",
      "Epoch: 003, Loss: 3.8764 Val: 4.8439 Test: 4.9893\n",
      "Epoch: 004, Loss: 4.3778 Val: 4.2578 Test: 4.2411\n",
      "Epoch: 005, Loss: 3.6521 Val: 2.4671 Test: 2.5897\n",
      "Epoch: 006, Loss: 1.5844 Val: 2.1820 Test: 1.4319\n",
      "Epoch: 007, Loss: 2.3615 Val: 0.9944 Test: 1.0882\n",
      "Epoch: 008, Loss: 1.3428 Val: 1.2675 Test: 1.4303\n",
      "Epoch: 009, Loss: 1.4561 Val: 0.7635 Test: 1.7375\n",
      "Epoch: 010, Loss: 1.0806 Val: 1.4710 Test: 1.6741\n",
      "Epoch: 011, Loss: 1.0389 Val: 1.0439 Test: 0.7889\n",
      "Epoch: 012, Loss: 0.9876 Val: 0.8042 Test: 1.2003\n",
      "Epoch: 013, Loss: 1.3063 Val: 0.9959 Test: 0.2514\n",
      "Epoch: 014, Loss: 1.3146 Val: 0.9790 Test: 1.4264\n",
      "Epoch: 015, Loss: 1.2629 Val: 0.5054 Test: 1.1095\n",
      "Epoch: 016, Loss: 0.8657 Val: 1.2561 Test: 1.3894\n",
      "Epoch: 017, Loss: 1.0890 Val: 1.2984 Test: 1.0206\n",
      "Epoch: 018, Loss: 1.0833 Val: 0.7192 Test: 1.1577\n",
      "Epoch: 019, Loss: 1.0940 Val: 0.4002 Test: 0.6964\n",
      "Epoch: 020, Loss: 1.1427 Val: 1.0169 Test: 0.6526\n",
      "Epoch: 021, Loss: 0.9802 Val: 0.7568 Test: 0.7833\n",
      "Epoch: 022, Loss: 1.0970 Val: 0.6350 Test: 1.4004\n",
      "Epoch: 023, Loss: 0.8038 Val: 0.9552 Test: 1.0686\n",
      "Epoch: 024, Loss: 1.0553 Val: 0.8573 Test: 1.3791\n",
      "Epoch: 025, Loss: 1.0227 Val: 0.7979 Test: 1.1024\n",
      "Epoch: 026, Loss: 1.0669 Val: 0.7003 Test: 0.9910\n",
      "Epoch: 027, Loss: 0.8870 Val: 1.1738 Test: 0.5692\n",
      "Epoch: 028, Loss: 1.0275 Val: 0.5799 Test: 0.6465\n",
      "Epoch: 029, Loss: 0.9625 Val: 0.4735 Test: 0.5150\n",
      "Epoch: 030, Loss: 1.0490 Val: 0.8013 Test: 0.9530\n",
      "Epoch: 031, Loss: 0.8524 Val: 0.7664 Test: 0.8146\n",
      "Epoch: 032, Loss: 0.9047 Val: 0.6746 Test: 0.6950\n",
      "Epoch: 033, Loss: 1.0909 Val: 1.1385 Test: 0.5600\n",
      "Epoch: 034, Loss: 0.8209 Val: 0.6109 Test: 0.5676\n",
      "Epoch: 035, Loss: 0.8457 Val: 1.0696 Test: 1.4819\n",
      "Epoch: 036, Loss: 0.7038 Val: 0.7341 Test: 0.5432\n",
      "Epoch: 037, Loss: 0.7463 Val: 0.7246 Test: 0.6743\n",
      "Epoch: 038, Loss: 0.8449 Val: 1.0800 Test: 0.8079\n",
      "Epoch: 039, Loss: 0.6069 Val: 0.9705 Test: 0.8356\n",
      "Epoch: 040, Loss: 0.7754 Val: 1.3418 Test: 0.7577\n",
      "Epoch: 041, Loss: 0.9027 Val: 0.8990 Test: 0.4748\n",
      "Epoch: 042, Loss: 0.8225 Val: 0.6913 Test: 0.8366\n",
      "Epoch: 043, Loss: 0.8515 Val: 1.1495 Test: 1.4767\n",
      "Epoch: 044, Loss: 0.9960 Val: 1.3417 Test: 0.8828\n",
      "Epoch: 045, Loss: 1.0962 Val: 1.5179 Test: 0.9264\n",
      "Epoch: 046, Loss: 0.9833 Val: 0.9685 Test: 0.5420\n",
      "Epoch: 047, Loss: 0.8550 Val: 0.9203 Test: 1.1105\n",
      "Epoch: 048, Loss: 0.9310 Val: 1.3403 Test: 1.3190\n",
      "Epoch: 049, Loss: 0.7776 Val: 1.4898 Test: 0.7254\n",
      "phgdh\n",
      "Epoch: 001, Loss: 5.0821 Val: 1.7540 Test: 2.2937\n",
      "Epoch: 002, Loss: 1.1037 Val: 2.0863 Test: 1.6623\n",
      "Epoch: 003, Loss: 1.6662 Val: 1.5773 Test: 2.1736\n",
      "Epoch: 004, Loss: 1.4871 Val: 1.2267 Test: 1.5565\n",
      "Epoch: 005, Loss: 1.7494 Val: 0.7399 Test: 1.7966\n",
      "Epoch: 006, Loss: 1.1760 Val: 1.1837 Test: 1.5188\n",
      "Epoch: 007, Loss: 0.6984 Val: 1.0739 Test: 1.5199\n",
      "Epoch: 008, Loss: 0.8252 Val: 0.8961 Test: 1.4728\n",
      "Epoch: 009, Loss: 1.0514 Val: 0.6012 Test: 1.4416\n",
      "Epoch: 010, Loss: 1.1427 Val: 0.9257 Test: 1.8009\n",
      "Epoch: 011, Loss: 1.0123 Val: 0.9491 Test: 1.2460\n",
      "Epoch: 012, Loss: 0.8886 Val: 0.8133 Test: 1.5236\n",
      "Epoch: 013, Loss: 0.8553 Val: 0.7617 Test: 1.1738\n",
      "Epoch: 014, Loss: 0.9249 Val: 0.3646 Test: 1.5425\n",
      "Epoch: 015, Loss: 0.8731 Val: 0.6146 Test: 1.8400\n",
      "Epoch: 016, Loss: 0.7352 Val: 0.4391 Test: 1.5893\n",
      "Epoch: 017, Loss: 0.6413 Val: 0.3681 Test: 1.3255\n",
      "Epoch: 018, Loss: 0.7397 Val: 0.4102 Test: 1.0895\n",
      "Epoch: 019, Loss: 0.7897 Val: 0.5299 Test: 1.2527\n",
      "Epoch: 020, Loss: 0.7307 Val: 0.5154 Test: 1.2298\n",
      "Epoch: 021, Loss: 0.8187 Val: 0.6051 Test: 1.2959\n",
      "Epoch: 022, Loss: 0.7232 Val: 0.6442 Test: 0.7998\n",
      "Epoch: 023, Loss: 0.7515 Val: 0.7501 Test: 1.2960\n",
      "Epoch: 024, Loss: 0.7922 Val: 0.7355 Test: 1.0858\n",
      "Epoch: 025, Loss: 1.0428 Val: 0.4919 Test: 1.0750\n",
      "Epoch: 026, Loss: 0.5260 Val: 0.7616 Test: 1.3207\n",
      "Epoch: 027, Loss: 0.6643 Val: 0.5204 Test: 0.8577\n",
      "Epoch: 028, Loss: 0.7090 Val: 0.6290 Test: 1.0206\n",
      "Epoch: 029, Loss: 0.6073 Val: 0.7250 Test: 0.9959\n",
      "Epoch: 030, Loss: 0.7459 Val: 0.2865 Test: 0.6855\n",
      "Epoch: 031, Loss: 0.6465 Val: 0.5303 Test: 0.5643\n",
      "Epoch: 032, Loss: 0.5681 Val: 0.5774 Test: 0.9408\n",
      "Epoch: 033, Loss: 0.5142 Val: 0.5631 Test: 0.6900\n",
      "Epoch: 034, Loss: 0.8041 Val: 0.9369 Test: 1.3555\n",
      "Epoch: 035, Loss: 0.6253 Val: 0.4742 Test: 1.1217\n",
      "Epoch: 036, Loss: 0.8610 Val: 0.3519 Test: 0.6766\n",
      "Epoch: 037, Loss: 0.6536 Val: 0.7230 Test: 1.3291\n",
      "Epoch: 038, Loss: 0.7624 Val: 0.4948 Test: 0.9863\n",
      "Epoch: 039, Loss: 0.5385 Val: 0.5647 Test: 0.8766\n",
      "Epoch: 040, Loss: 0.6744 Val: 0.5314 Test: 0.7706\n",
      "Epoch: 041, Loss: 0.5415 Val: 0.3201 Test: 0.9207\n",
      "Epoch: 042, Loss: 0.6678 Val: 0.7714 Test: 1.4007\n",
      "Epoch: 043, Loss: 0.6224 Val: 0.5296 Test: 1.0360\n",
      "Epoch: 044, Loss: 0.6326 Val: 0.5730 Test: 0.6645\n",
      "Epoch: 045, Loss: 0.6377 Val: 0.3638 Test: 1.2094\n",
      "Epoch: 046, Loss: 0.7383 Val: 0.6862 Test: 1.1080\n",
      "Epoch: 047, Loss: 0.7497 Val: 0.5384 Test: 1.1985\n",
      "Epoch: 048, Loss: 0.6331 Val: 0.3166 Test: 1.0493\n",
      "Epoch: 049, Loss: 0.6553 Val: 0.5087 Test: 0.6251\n",
      "rorg\n",
      "Epoch: 001, Loss: 5.4410 Val: 3.0085 Test: 3.1792\n",
      "Epoch: 002, Loss: 3.3434 Val: 1.3388 Test: 1.9766\n",
      "Epoch: 003, Loss: 1.9631 Val: 2.2603 Test: 3.2891\n",
      "Epoch: 004, Loss: 1.3200 Val: 1.4442 Test: 1.5756\n",
      "Epoch: 005, Loss: 0.7519 Val: 1.0182 Test: 1.3619\n",
      "Epoch: 006, Loss: 1.3499 Val: 0.8248 Test: 1.5507\n",
      "Epoch: 007, Loss: 0.8826 Val: 1.4718 Test: 1.3527\n",
      "Epoch: 008, Loss: 0.9896 Val: 1.8502 Test: 1.3192\n",
      "Epoch: 009, Loss: 0.6084 Val: 1.0559 Test: 1.6181\n",
      "Epoch: 010, Loss: 0.7285 Val: 0.8757 Test: 1.6830\n",
      "Epoch: 011, Loss: 0.9472 Val: 1.3753 Test: 1.4534\n",
      "Epoch: 012, Loss: 0.8795 Val: 1.3719 Test: 1.4587\n",
      "Epoch: 013, Loss: 0.6432 Val: 1.5062 Test: 1.5670\n",
      "Epoch: 014, Loss: 0.7257 Val: 0.7479 Test: 1.3150\n",
      "Epoch: 015, Loss: 0.7057 Val: 0.8376 Test: 1.7483\n",
      "Epoch: 016, Loss: 0.8826 Val: 1.3457 Test: 0.8338\n",
      "Epoch: 017, Loss: 0.6509 Val: 1.2158 Test: 1.6459\n",
      "Epoch: 018, Loss: 0.7155 Val: 0.6433 Test: 1.0140\n",
      "Epoch: 019, Loss: 0.6557 Val: 0.9952 Test: 1.6091\n",
      "Epoch: 020, Loss: 0.5043 Val: 1.1343 Test: 1.6462\n",
      "Epoch: 021, Loss: 0.8008 Val: 1.2205 Test: 1.7052\n",
      "Epoch: 022, Loss: 0.5445 Val: 0.7652 Test: 0.9981\n",
      "Epoch: 023, Loss: 0.4753 Val: 1.3256 Test: 1.5187\n",
      "Epoch: 024, Loss: 0.6528 Val: 1.5100 Test: 1.6773\n",
      "Epoch: 025, Loss: 0.7223 Val: 1.6231 Test: 1.3820\n",
      "Epoch: 026, Loss: 0.7983 Val: 0.8603 Test: 1.3400\n",
      "Epoch: 027, Loss: 0.6110 Val: 0.8973 Test: 1.1809\n",
      "Epoch: 028, Loss: 0.9066 Val: 0.5366 Test: 1.3014\n",
      "Epoch: 029, Loss: 0.7877 Val: 1.3286 Test: 1.3116\n",
      "Epoch: 030, Loss: 0.9736 Val: 0.5226 Test: 1.3645\n",
      "Epoch: 031, Loss: 0.6119 Val: 0.7670 Test: 0.9131\n",
      "Epoch: 032, Loss: 0.5711 Val: 0.8512 Test: 0.8294\n",
      "Epoch: 033, Loss: 0.6781 Val: 0.5997 Test: 1.0579\n",
      "Epoch: 034, Loss: 0.5417 Val: 1.0351 Test: 1.1096\n",
      "Epoch: 035, Loss: 0.5855 Val: 0.5516 Test: 1.1297\n",
      "Epoch: 036, Loss: 0.6705 Val: 0.9534 Test: 1.1850\n",
      "Epoch: 037, Loss: 0.7275 Val: 0.6462 Test: 1.1322\n",
      "Epoch: 038, Loss: 0.3998 Val: 0.9298 Test: 1.3747\n",
      "Epoch: 039, Loss: 0.5714 Val: 1.6476 Test: 1.3185\n",
      "Epoch: 040, Loss: 0.7401 Val: 0.7376 Test: 1.2139\n",
      "Epoch: 041, Loss: 0.7118 Val: 0.7377 Test: 0.8436\n",
      "Epoch: 042, Loss: 0.6044 Val: 1.2320 Test: 1.2889\n",
      "Epoch: 043, Loss: 0.6281 Val: 0.6450 Test: 1.1701\n",
      "Epoch: 044, Loss: 0.6278 Val: 0.8424 Test: 1.3493\n",
      "Epoch: 045, Loss: 0.8114 Val: 1.2773 Test: 1.2592\n",
      "Epoch: 046, Loss: 0.5616 Val: 0.7837 Test: 1.0037\n",
      "Epoch: 047, Loss: 0.6176 Val: 1.3278 Test: 1.3866\n",
      "Epoch: 048, Loss: 0.4487 Val: 0.8528 Test: 0.9393\n",
      "Epoch: 049, Loss: 0.6859 Val: 1.0892 Test: 1.5093\n",
      "ido1\n",
      "Epoch: 001, Loss: 5.3071 Val: 2.7072 Test: 2.6487\n",
      "Epoch: 002, Loss: 2.1802 Val: 2.8357 Test: 2.6384\n",
      "Epoch: 003, Loss: 2.5054 Val: 1.8014 Test: 1.6516\n",
      "Epoch: 004, Loss: 1.6924 Val: 1.4643 Test: 2.0535\n",
      "Epoch: 005, Loss: 1.8122 Val: 1.2370 Test: 0.6604\n",
      "Epoch: 006, Loss: 1.6496 Val: 1.8440 Test: 0.9597\n",
      "Epoch: 007, Loss: 1.1936 Val: 0.8982 Test: 0.9895\n",
      "Epoch: 008, Loss: 1.0475 Val: 1.0575 Test: 0.3310\n",
      "Epoch: 009, Loss: 1.1301 Val: 1.0072 Test: 1.1507\n",
      "Epoch: 010, Loss: 1.0244 Val: 0.5255 Test: 0.6789\n",
      "Epoch: 011, Loss: 1.0167 Val: 0.9701 Test: 0.9965\n",
      "Epoch: 012, Loss: 1.0244 Val: 0.6360 Test: 0.6798\n",
      "Epoch: 013, Loss: 0.8181 Val: 0.8987 Test: 0.9556\n",
      "Epoch: 014, Loss: 0.7514 Val: 1.2133 Test: 1.0847\n",
      "Epoch: 015, Loss: 0.8956 Val: 1.0319 Test: 0.9107\n",
      "Epoch: 016, Loss: 0.8875 Val: 0.9724 Test: 0.2484\n",
      "Epoch: 017, Loss: 0.7845 Val: 0.4981 Test: 0.6662\n",
      "Epoch: 018, Loss: 0.8587 Val: 0.7178 Test: 0.7780\n",
      "Epoch: 019, Loss: 0.9857 Val: 0.9379 Test: 0.5486\n",
      "Epoch: 020, Loss: 0.8762 Val: 0.7404 Test: 0.4429\n",
      "Epoch: 021, Loss: 1.0584 Val: 0.8648 Test: 1.2400\n",
      "Epoch: 022, Loss: 0.8138 Val: 0.7661 Test: 0.9306\n",
      "Epoch: 023, Loss: 0.9411 Val: 0.6519 Test: 0.6553\n",
      "Epoch: 024, Loss: 0.8019 Val: 1.0925 Test: 0.4342\n",
      "Epoch: 025, Loss: 0.7006 Val: 0.6043 Test: 0.7402\n",
      "Epoch: 026, Loss: 0.5905 Val: 0.9140 Test: 1.0053\n",
      "Epoch: 027, Loss: 0.8950 Val: 0.4463 Test: 0.5736\n",
      "Epoch: 028, Loss: 0.7570 Val: 0.8623 Test: 0.5366\n",
      "Epoch: 029, Loss: 0.8812 Val: 0.7457 Test: 0.8627\n",
      "Epoch: 030, Loss: 0.8372 Val: 0.6946 Test: 0.4909\n",
      "Epoch: 031, Loss: 0.7169 Val: 0.6645 Test: 0.7663\n",
      "Epoch: 032, Loss: 0.7902 Val: 0.4636 Test: 0.5989\n",
      "Epoch: 033, Loss: 0.6928 Val: 0.5907 Test: 0.3201\n",
      "Epoch: 034, Loss: 0.7321 Val: 0.6579 Test: 0.6545\n",
      "Epoch: 035, Loss: 0.9372 Val: 0.5983 Test: 0.7770\n",
      "Epoch: 036, Loss: 0.8077 Val: 1.0418 Test: 0.6833\n",
      "Epoch: 037, Loss: 0.9963 Val: 1.1715 Test: 0.6776\n",
      "Epoch: 038, Loss: 0.8848 Val: 0.8584 Test: 0.4573\n",
      "Epoch: 039, Loss: 0.7711 Val: 0.8909 Test: 0.5532\n",
      "Epoch: 040, Loss: 0.7823 Val: 0.6236 Test: 0.3612\n",
      "Epoch: 041, Loss: 0.6930 Val: 0.4859 Test: 0.4389\n",
      "Epoch: 042, Loss: 0.8845 Val: 1.0353 Test: 0.4598\n",
      "Epoch: 043, Loss: 0.9294 Val: 0.6519 Test: 0.6790\n",
      "Epoch: 044, Loss: 0.5914 Val: 0.5534 Test: 0.6130\n",
      "Epoch: 045, Loss: 0.5086 Val: 0.6491 Test: 0.5988\n",
      "Epoch: 046, Loss: 0.6926 Val: 0.4968 Test: 0.5935\n",
      "Epoch: 047, Loss: 0.8489 Val: 0.9479 Test: 0.5765\n",
      "Epoch: 048, Loss: 0.5520 Val: 0.5803 Test: 0.7883\n",
      "Epoch: 049, Loss: 0.6216 Val: 0.5965 Test: 0.8257\n",
      "klk5\n",
      "Epoch: 001, Loss: 5.5514 Val: 11.4353 Test: 10.4135\n",
      "Epoch: 002, Loss: 8.1595 Val: 2.7383 Test: 3.3191\n",
      "Epoch: 003, Loss: 3.6622 Val: 4.0644 Test: 4.3337\n",
      "Epoch: 004, Loss: 4.3391 Val: 3.3269 Test: 3.7786\n",
      "Epoch: 005, Loss: 3.3773 Val: 1.9157 Test: 1.9213\n",
      "Epoch: 006, Loss: 1.6793 Val: 1.5167 Test: 1.4241\n",
      "Epoch: 007, Loss: 1.9621 Val: 1.3123 Test: 1.3659\n",
      "Epoch: 008, Loss: 1.4518 Val: 0.8016 Test: 0.8885\n",
      "Epoch: 009, Loss: 1.1228 Val: 1.2804 Test: 1.5265\n",
      "Epoch: 010, Loss: 1.4157 Val: 1.4518 Test: 1.3727\n",
      "Epoch: 011, Loss: 1.2401 Val: 1.2939 Test: 1.4713\n",
      "Epoch: 012, Loss: 1.3385 Val: 1.0506 Test: 0.7784\n",
      "Epoch: 013, Loss: 1.2461 Val: 1.0143 Test: 0.7433\n",
      "Epoch: 014, Loss: 1.1199 Val: 0.7179 Test: 0.7748\n",
      "Epoch: 015, Loss: 0.9724 Val: 1.4123 Test: 0.3542\n",
      "Epoch: 016, Loss: 1.1217 Val: 0.5404 Test: 0.5285\n",
      "Epoch: 017, Loss: 0.8026 Val: 1.1331 Test: 0.8839\n",
      "Epoch: 018, Loss: 1.1775 Val: 1.0348 Test: 1.1587\n",
      "Epoch: 019, Loss: 1.0480 Val: 0.7633 Test: 1.2038\n",
      "Epoch: 020, Loss: 1.0977 Val: 0.9632 Test: 0.4279\n",
      "Epoch: 021, Loss: 0.9680 Val: 0.9258 Test: 1.3736\n",
      "Epoch: 022, Loss: 0.9218 Val: 0.6508 Test: 0.7254\n",
      "Epoch: 023, Loss: 0.8733 Val: 1.0223 Test: 0.6840\n",
      "Epoch: 024, Loss: 0.8504 Val: 0.6386 Test: 0.5322\n",
      "Epoch: 025, Loss: 0.9255 Val: 1.2445 Test: 0.4657\n",
      "Epoch: 026, Loss: 0.7473 Val: 0.7885 Test: 0.6708\n",
      "Epoch: 027, Loss: 0.8987 Val: 0.5069 Test: 0.2369\n",
      "Epoch: 028, Loss: 0.9386 Val: 0.8328 Test: 0.3555\n",
      "Epoch: 029, Loss: 0.9811 Val: 0.6315 Test: 0.6218\n",
      "Epoch: 030, Loss: 0.9132 Val: 0.3663 Test: 0.5957\n",
      "Epoch: 031, Loss: 0.9894 Val: 0.7773 Test: 1.0049\n",
      "Epoch: 032, Loss: 1.0230 Val: 0.8501 Test: 0.3551\n",
      "Epoch: 033, Loss: 0.8689 Val: 0.8593 Test: 0.4214\n",
      "Epoch: 034, Loss: 0.6399 Val: 0.9528 Test: 0.4743\n",
      "Epoch: 035, Loss: 0.6615 Val: 0.2121 Test: 0.2588\n",
      "Epoch: 036, Loss: 0.8142 Val: 0.5845 Test: 0.8720\n",
      "Epoch: 037, Loss: 0.8982 Val: 1.1323 Test: 1.0564\n",
      "Epoch: 038, Loss: 0.9956 Val: 0.6600 Test: 0.3465\n",
      "Epoch: 039, Loss: 0.7742 Val: 0.9358 Test: 0.7163\n",
      "Epoch: 040, Loss: 1.0743 Val: 0.5130 Test: 0.4826\n",
      "Epoch: 041, Loss: 1.0057 Val: 0.8664 Test: 1.0074\n",
      "Epoch: 042, Loss: 0.8722 Val: 0.6160 Test: 0.8603\n",
      "Epoch: 043, Loss: 0.9984 Val: 0.7093 Test: 0.4474\n",
      "Epoch: 044, Loss: 0.9264 Val: 0.6599 Test: 0.5785\n",
      "Epoch: 045, Loss: 0.9712 Val: 0.4831 Test: 0.9493\n",
      "Epoch: 046, Loss: 0.8329 Val: 0.8992 Test: 0.4553\n",
      "Epoch: 047, Loss: 0.6741 Val: 1.2142 Test: 0.7484\n",
      "Epoch: 048, Loss: 0.7483 Val: 1.3603 Test: 0.7395\n",
      "Epoch: 049, Loss: 1.0253 Val: 0.6651 Test: 0.4734\n",
      "notum\n",
      "Epoch: 001, Loss: 5.8824 Val: 11.3841 Test: 10.5838\n",
      "Epoch: 002, Loss: 7.2440 Val: 2.9802 Test: 2.6605\n",
      "Epoch: 003, Loss: 3.6861 Val: 4.2632 Test: 3.9505\n",
      "Epoch: 004, Loss: 4.0294 Val: 3.7455 Test: 3.0833\n",
      "Epoch: 005, Loss: 3.1002 Val: 1.3223 Test: 1.0093\n",
      "Epoch: 006, Loss: 1.5566 Val: 2.5368 Test: 2.3814\n",
      "Epoch: 007, Loss: 2.1267 Val: 2.1427 Test: 2.2146\n",
      "Epoch: 008, Loss: 1.6720 Val: 0.6256 Test: 1.3533\n",
      "Epoch: 009, Loss: 1.1664 Val: 1.3283 Test: 1.6032\n",
      "Epoch: 010, Loss: 1.7468 Val: 1.5632 Test: 1.6904\n",
      "Epoch: 011, Loss: 1.5591 Val: 1.1647 Test: 0.8707\n",
      "Epoch: 012, Loss: 1.0534 Val: 0.8289 Test: 1.5004\n",
      "Epoch: 013, Loss: 1.1171 Val: 0.7469 Test: 1.4442\n",
      "Epoch: 014, Loss: 0.9825 Val: 1.0379 Test: 2.0131\n",
      "Epoch: 015, Loss: 0.9217 Val: 0.5839 Test: 1.8022\n",
      "Epoch: 016, Loss: 0.9585 Val: 0.6014 Test: 1.4841\n",
      "Epoch: 017, Loss: 0.7832 Val: 0.8938 Test: 0.8911\n",
      "Epoch: 018, Loss: 0.8193 Val: 0.6353 Test: 0.8797\n",
      "Epoch: 019, Loss: 0.8507 Val: 1.0011 Test: 0.6968\n",
      "Epoch: 020, Loss: 0.6076 Val: 1.1937 Test: 0.7235\n",
      "Epoch: 021, Loss: 0.7926 Val: 0.5546 Test: 1.1639\n",
      "Epoch: 022, Loss: 0.9506 Val: 0.5112 Test: 1.1508\n",
      "Epoch: 023, Loss: 0.7664 Val: 0.7588 Test: 0.6236\n",
      "Epoch: 024, Loss: 0.8417 Val: 0.5178 Test: 0.9932\n",
      "Epoch: 025, Loss: 0.5317 Val: 0.7369 Test: 1.0221\n",
      "Epoch: 026, Loss: 1.0956 Val: 0.5023 Test: 0.9559\n",
      "Epoch: 027, Loss: 0.8128 Val: 0.7002 Test: 0.5514\n",
      "Epoch: 028, Loss: 1.0961 Val: 0.6610 Test: 1.1850\n",
      "Epoch: 029, Loss: 0.8183 Val: 0.5463 Test: 1.3021\n",
      "Epoch: 030, Loss: 0.8563 Val: 0.5594 Test: 1.3389\n",
      "Epoch: 031, Loss: 0.7073 Val: 1.1045 Test: 1.6959\n",
      "Epoch: 032, Loss: 0.6829 Val: 1.1346 Test: 1.1554\n",
      "Epoch: 033, Loss: 0.6459 Val: 0.5598 Test: 0.9954\n",
      "Epoch: 034, Loss: 0.7850 Val: 0.6163 Test: 1.3725\n",
      "Epoch: 035, Loss: 0.8265 Val: 0.6324 Test: 0.7103\n",
      "Epoch: 036, Loss: 0.6587 Val: 0.5224 Test: 1.3894\n",
      "Epoch: 037, Loss: 0.8218 Val: 0.6131 Test: 1.2889\n",
      "Epoch: 038, Loss: 0.7582 Val: 0.5450 Test: 1.2723\n",
      "Epoch: 039, Loss: 0.7422 Val: 0.9273 Test: 0.8220\n",
      "Epoch: 040, Loss: 0.7377 Val: 0.1724 Test: 1.0293\n",
      "Epoch: 041, Loss: 0.7189 Val: 1.0235 Test: 0.5677\n",
      "Epoch: 042, Loss: 0.7378 Val: 0.2543 Test: 1.2584\n",
      "Epoch: 043, Loss: 0.8115 Val: 0.6926 Test: 1.1037\n",
      "Epoch: 044, Loss: 0.6518 Val: 0.4079 Test: 1.2909\n",
      "Epoch: 045, Loss: 0.8509 Val: 0.5198 Test: 0.5309\n",
      "Epoch: 046, Loss: 0.7090 Val: 0.3835 Test: 0.9256\n",
      "Epoch: 047, Loss: 0.5771 Val: 0.4740 Test: 1.2509\n",
      "Epoch: 048, Loss: 0.4979 Val: 0.5129 Test: 1.0177\n",
      "Epoch: 049, Loss: 0.6375 Val: 0.7585 Test: 1.2138\n",
      "eaat3\n",
      "Epoch: 001, Loss: 4.9483 Val: 15.6865 Test: 12.5379\n",
      "Epoch: 002, Loss: 10.2348 Val: 2.7397 Test: 2.6608\n",
      "Epoch: 003, Loss: 3.4998 Val: 4.1402 Test: 3.8874\n",
      "Epoch: 004, Loss: 4.1112 Val: 3.3622 Test: 3.1822\n",
      "Epoch: 005, Loss: 3.0795 Val: 2.0300 Test: 1.6570\n",
      "Epoch: 006, Loss: 1.2720 Val: 1.6236 Test: 2.8769\n",
      "Epoch: 007, Loss: 1.4555 Val: 2.2912 Test: 2.7237\n",
      "Epoch: 008, Loss: 1.8185 Val: 0.6530 Test: 1.4656\n",
      "Epoch: 009, Loss: 1.1821 Val: 1.3164 Test: 2.0186\n",
      "Epoch: 010, Loss: 1.3640 Val: 1.1306 Test: 1.5335\n",
      "Epoch: 011, Loss: 1.3567 Val: 0.9961 Test: 1.5056\n",
      "Epoch: 012, Loss: 1.0571 Val: 0.6310 Test: 1.2600\n",
      "Epoch: 013, Loss: 0.9351 Val: 1.0455 Test: 1.0386\n",
      "Epoch: 014, Loss: 1.0071 Val: 0.8298 Test: 1.2603\n",
      "Epoch: 015, Loss: 0.9852 Val: 0.8829 Test: 0.9903\n",
      "Epoch: 016, Loss: 1.0629 Val: 0.8271 Test: 1.2852\n",
      "Epoch: 017, Loss: 0.9353 Val: 0.7631 Test: 1.4646\n",
      "Epoch: 018, Loss: 0.9694 Val: 0.4274 Test: 1.4341\n",
      "Epoch: 019, Loss: 1.1008 Val: 0.7546 Test: 0.5615\n",
      "Epoch: 020, Loss: 0.9556 Val: 0.5009 Test: 0.9671\n",
      "Epoch: 021, Loss: 0.8067 Val: 0.4875 Test: 1.4647\n",
      "Epoch: 022, Loss: 1.0728 Val: 0.6101 Test: 1.6364\n",
      "Epoch: 023, Loss: 0.7518 Val: 0.7490 Test: 1.0831\n",
      "Epoch: 024, Loss: 1.0222 Val: 0.3728 Test: 1.2575\n",
      "Epoch: 025, Loss: 0.9481 Val: 0.1522 Test: 1.1084\n",
      "Epoch: 026, Loss: 1.1401 Val: 0.4993 Test: 1.5073\n",
      "Epoch: 027, Loss: 0.8057 Val: 0.4399 Test: 0.9458\n",
      "Epoch: 028, Loss: 0.9344 Val: 0.5703 Test: 1.5203\n",
      "Epoch: 029, Loss: 0.8035 Val: 0.7921 Test: 1.2020\n",
      "Epoch: 030, Loss: 0.8297 Val: 0.4433 Test: 0.6394\n",
      "Epoch: 031, Loss: 0.7794 Val: 0.4083 Test: 0.8890\n",
      "Epoch: 032, Loss: 0.7351 Val: 0.6242 Test: 1.4555\n",
      "Epoch: 033, Loss: 0.9009 Val: 0.3904 Test: 1.1641\n",
      "Epoch: 034, Loss: 0.7294 Val: 0.6045 Test: 0.8712\n",
      "Epoch: 035, Loss: 0.6483 Val: 0.4684 Test: 1.2571\n",
      "Epoch: 036, Loss: 0.7667 Val: 0.7948 Test: 1.2939\n",
      "Epoch: 037, Loss: 0.8791 Val: 0.8470 Test: 1.1579\n",
      "Epoch: 038, Loss: 0.6942 Val: 0.5687 Test: 0.7604\n",
      "Epoch: 039, Loss: 0.6630 Val: 0.3115 Test: 1.1376\n",
      "Epoch: 040, Loss: 0.7033 Val: 0.5701 Test: 1.2649\n",
      "Epoch: 041, Loss: 0.8330 Val: 0.9783 Test: 1.6590\n",
      "Epoch: 042, Loss: 0.8577 Val: 0.6664 Test: 1.2409\n",
      "Epoch: 043, Loss: 0.7405 Val: 0.5318 Test: 0.6665\n",
      "Epoch: 044, Loss: 0.4801 Val: 0.4414 Test: 1.0189\n",
      "Epoch: 045, Loss: 1.0462 Val: 0.6867 Test: 1.0170\n",
      "Epoch: 046, Loss: 0.7963 Val: 0.6533 Test: 1.1884\n",
      "Epoch: 047, Loss: 0.8002 Val: 1.1135 Test: 1.3096\n",
      "Epoch: 048, Loss: 1.0184 Val: 0.7840 Test: 1.3544\n",
      "Epoch: 049, Loss: 0.6115 Val: 0.7725 Test: 1.1532\n"
     ]
    }
   ],
   "source": [
    "from low_inhibitors import LSSInhibitor\n",
    "\n",
    "for dataset_name in LSSInhibitor.names.keys():\n",
    "    \n",
    "    print(dataset_name)\n",
    "    \n",
    "    path = '../../tmp/data'\n",
    "    \n",
    "    # use the attentiveFP node and edge features during the mol-2-graph transoformation\n",
    "    dataset = LSSInhibitor(path, name='mglur2', pre_transform=GenAttentiveFeatures()).shuffle()\n",
    "    #dataset = MoleculeNet(path, name='FreeSolv', pre_transform=GenFeatures()).shuffle()\n",
    "    \n",
    "    batch_size = 8\n",
    "    \n",
    "    # train, valid, test splitting\n",
    "    N = len(dataset) // 5\n",
    "    val_dataset = dataset[:N]\n",
    "    test_dataset = dataset[N:2 * N]\n",
    "    train_dataset = dataset[2 * N:]\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "                        edge_dim=10, num_layers=2, num_timesteps=2,\n",
    "                        dropout=0.2).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
    "                                 weight_decay=10**-5)\n",
    "\n",
    "    def train():\n",
    "        total_loss = total_examples = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "            total_examples += data.num_graphs\n",
    "        return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(loader):\n",
    "        mse = []\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "        return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "\n",
    "    for epoch in range(1, 50):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(val_loader)\n",
    "        test_rmse = test(test_loader)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "              f'Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cc1b4-1a51-429f-90d2-5f5f53959b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
