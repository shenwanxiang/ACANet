{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988ab225-f6f0-44b6-b24d-7d1374847765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "%matplotlib inline\n",
    "#A100 80GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f5cf08-0f69-4dc4-830d-ba271bd8ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "torch.cuda.set_device(gpuid)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a32cb66-5320-421b-9259-880db0f5aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/shenwanxiang/Research/bidd-clsar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde33631-dc5e-419f-b970-26e4b10535fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clsar.dataset import LSSNS, HSSMS\n",
    "from clsar.feature import Gen39AtomFeatures\n",
    "from clsar.model.model import ACANet_PNA, get_deg, _fix_reproducibility # model\n",
    "from clsar.model.loss import ACALoss, get_best_cliff, get_best_structure_batch\n",
    "_fix_reproducibility(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca7d324-4af1-4477-99c4-4b0a96157fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, aca_loss):\n",
    "\n",
    "    total_examples = 0\n",
    "    total_loss =  0    \n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0  \n",
    "    \n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index, \n",
    "                                        data.edge_attr, data.batch)\n",
    "\n",
    "        \n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                            fps_smiles = data.fp_smiles,\n",
    "                            fps_scaffold = data.fp_scaffold,                           \n",
    "                            smiles_list = data.smiles,                           \n",
    "                           )\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs        \n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs        \n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "    \n",
    "    train_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    return train_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_loader, model, aca_loss):\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    total_tsm_loss = 0\n",
    "    total_reg_loss = 0\n",
    "\n",
    "    n_label_triplets = []\n",
    "    n_structure_triplets = []\n",
    "    n_triplets = []\n",
    "    n_hv_triplets = []\n",
    "    \n",
    "    mse = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        predictions, embeddings = model(data.x.float(), data.edge_index,\n",
    "                                        data.edge_attr, data.batch)\n",
    "        \n",
    "        loss_out = aca_loss(labels = data.y, \n",
    "                            predictions = predictions,\n",
    "                            embeddings = embeddings,\n",
    "                            fps_smiles = data.fp_smiles,\n",
    "                            fps_scaffold = data.fp_scaffold,                           \n",
    "                            smiles_list = data.smiles,                           \n",
    "                           )\n",
    "\n",
    "        \n",
    "        loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs = loss_out\n",
    "\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_tsm_loss += float(tsm_loss) * data.num_graphs\n",
    "        total_reg_loss += float(reg_loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "\n",
    "        n_label_triplets.append(int(N_Y_ACTs))\n",
    "        n_structure_triplets.append(int(N_S_ACTs))\n",
    "        n_triplets.append(int(N_ACTs))\n",
    "        n_hv_triplets.append(int(N_HV_ACTs))\n",
    "\n",
    "        mse.append(F.mse_loss(predictions, data.y, reduction='none').cpu())\n",
    "\n",
    "    test_loss = total_loss / total_examples\n",
    "    total_tsm_loss = total_tsm_loss / total_examples\n",
    "    total_reg_loss = total_reg_loss / total_examples\n",
    "\n",
    "    n_label_triplets = int(sum(n_label_triplets) / (i+1))\n",
    "    n_structure_triplets = int(sum(n_structure_triplets) / (i+1))\n",
    "    n_triplets = int(sum(n_triplets) / (i+1))\n",
    "    n_hv_triplets = int(sum(n_hv_triplets) / (i+1))\n",
    "    \n",
    "    test_rmse = float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    \n",
    "    return test_loss, total_tsm_loss, total_reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets, test_rmse\n",
    "\n",
    "\n",
    "\n",
    "def Test_performance(alpha=1.0, similarity_gate = True, gate_type = 'OR'):\n",
    "    _fix_reproducibility(42)\n",
    "    model = ACANet_PNA(**pub_args, deg=deg).to(device)  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=10**-5)\n",
    "    aca_loss = ACALoss(alpha=alpha, \n",
    "                        cliff_lower = 1., \n",
    "                        cliff_upper = 1.,\n",
    "                        squared = False,\n",
    "                        similarity_gate = similarity_gate,\n",
    "                        similarity_neg = 0.9, #0.\n",
    "                        similarity_pos = 1, #1\n",
    "                        gate_type = gate_type,\n",
    "                        dev_mode = True,)\n",
    "    \n",
    "    history = []\n",
    "    #ls_his = []\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, tsm_loss, reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets = train(train_loader, model, optimizer, aca_loss)\n",
    "\n",
    "        _, _, _, _, _, _, train_n_hv_triplets, train_rmse = test(train_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, val_n_hv_triplets, val_rmse = test(val_loader, model, aca_loss)\n",
    "        _, _, _, _, _, _, test_n_hv_triplets, test_rmse = test(test_loader, model, aca_loss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f} tsm_loss: {tsm_loss:.4f} reg_loss: {reg_loss:.4f} '\n",
    "              f'N_Y: {n_label_triplets:03d} N_S: {n_structure_triplets:03d} N: {n_triplets:03d} N_HV: {n_hv_triplets:03d} '\n",
    "              f'Val: {val_rmse:.4f} Test: {test_rmse:.4f}')\n",
    "    \n",
    "        history.append({'Epoch':epoch, 'train_loss':train_loss, 'train_triplet_loss':tsm_loss,\n",
    "                        'train_reg_loss':reg_loss, 'val_rmse':val_rmse, \n",
    "                        'test_rmse':test_rmse, 'train_rmse':train_rmse,\n",
    "                        \n",
    "                        'n_label_triplets': n_label_triplets, \n",
    "                        'n_structure_triplets':n_structure_triplets,\n",
    "                        'n_triplets':n_triplets,\n",
    "                        'n_hv_triplets':n_hv_triplets,\n",
    "                        \n",
    "\n",
    "                        'train_n_hv_triplets':train_n_hv_triplets,\n",
    "                        'val_n_hv_triplets':val_n_hv_triplets,\n",
    "                        'test_n_hv_triplets':test_n_hv_triplets,\n",
    "                        'alpha':alpha, 'similarity_gate':similarity_gate,\n",
    "                        'gate_type':gate_type,\n",
    "                       \n",
    "                       })\n",
    "        #ls_his.append({'Epoch':epoch, 'mae_loss':float(mae_loss), 'triplet_loss':float(triplet_loss)})\n",
    "    dfh = pd.DataFrame(history)\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa1d63b-0567-406f-9f48-291959b18978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'CHEMBL3979_EC50'\n",
    "Dataset =  HSSMS #LSSNS \n",
    "epochs = 800\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "\n",
    "pre_transform = Gen39AtomFeatures()\n",
    "in_channels = pre_transform.in_channels\n",
    "path = './data/'\n",
    "\n",
    "## model HPs\n",
    "pub_args = {'in_channels':pre_transform.in_channels, \n",
    "            'edge_dim':pre_transform.edge_dim,\n",
    "            'convs_layers': [64, 128, 256, 512],   \n",
    "            'dense_layers': [256, 128, 32], \n",
    "            'out_channels':1, \n",
    "            'aggregators': ['mean', 'min', 'max', 'sum','std'],\n",
    "            'scalers':['identity', 'amplification', 'attenuation'] ,\n",
    "            'dropout_p': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7b5051-3408-450a-9901-79b1042719d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f6da88-ccba-4731-a70b-f53c5c006652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 6.5474 tsm_loss: 11.0853 reg_loss: 6.5474 N_Y: 407843 N_S: 1754772 N: 407843 N_HV: 217279 Val: 6.9647 Test: 6.9852\n",
      "Epoch: 002, Loss: 5.5485 tsm_loss: 15.4066 reg_loss: 5.5485 N_Y: 403186 N_S: 1754772 N: 403186 N_HV: 214984 Val: 6.9662 Test: 6.9867\n",
      "Epoch: 003, Loss: 4.3193 tsm_loss: 16.2928 reg_loss: 4.3193 N_Y: 405580 N_S: 1754772 N: 405580 N_HV: 213671 Val: 6.9648 Test: 6.9853\n",
      "Epoch: 004, Loss: 2.5980 tsm_loss: 15.5147 reg_loss: 2.5980 N_Y: 406000 N_S: 1754772 N: 406000 N_HV: 213953 Val: 6.9586 Test: 6.9791\n",
      "Epoch: 005, Loss: 0.9682 tsm_loss: 13.5867 reg_loss: 0.9682 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 201959 Val: 6.9493 Test: 6.9699\n",
      "Epoch: 006, Loss: 0.8057 tsm_loss: 11.5977 reg_loss: 0.8057 N_Y: 404414 N_S: 1754772 N: 404414 N_HV: 193080 Val: 6.1465 Test: 6.1602\n",
      "Epoch: 007, Loss: 0.6967 tsm_loss: 9.1759 reg_loss: 0.6967 N_Y: 405844 N_S: 1754772 N: 405844 N_HV: 190650 Val: 4.6541 Test: 4.6503\n",
      "Epoch: 008, Loss: 0.6485 tsm_loss: 6.2630 reg_loss: 0.6485 N_Y: 406738 N_S: 1754772 N: 406738 N_HV: 177821 Val: 3.7070 Test: 3.6961\n",
      "Epoch: 009, Loss: 0.6070 tsm_loss: 6.4980 reg_loss: 0.6070 N_Y: 405450 N_S: 1754772 N: 405450 N_HV: 172423 Val: 2.9967 Test: 2.9828\n",
      "Epoch: 010, Loss: 0.5872 tsm_loss: 5.6235 reg_loss: 0.5872 N_Y: 406218 N_S: 1754772 N: 406218 N_HV: 164719 Val: 2.6399 Test: 2.6352\n",
      "Epoch: 011, Loss: 0.5656 tsm_loss: 5.5726 reg_loss: 0.5656 N_Y: 405649 N_S: 1754772 N: 405649 N_HV: 167551 Val: 1.6299 Test: 1.5936\n",
      "Epoch: 012, Loss: 0.5341 tsm_loss: 5.6119 reg_loss: 0.5341 N_Y: 406217 N_S: 1754772 N: 406217 N_HV: 161879 Val: 1.6182 Test: 1.5928\n",
      "Epoch: 013, Loss: 0.5400 tsm_loss: 5.9563 reg_loss: 0.5400 N_Y: 406272 N_S: 1754772 N: 406272 N_HV: 160035 Val: 1.0034 Test: 0.9710\n",
      "Epoch: 014, Loss: 0.5613 tsm_loss: 5.4221 reg_loss: 0.5613 N_Y: 408158 N_S: 1754772 N: 408158 N_HV: 166582 Val: 0.8246 Test: 0.7522\n",
      "Epoch: 015, Loss: 0.5552 tsm_loss: 5.2747 reg_loss: 0.5552 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 159143 Val: 0.8218 Test: 0.7654\n",
      "Epoch: 016, Loss: 0.5360 tsm_loss: 5.0241 reg_loss: 0.5360 N_Y: 404438 N_S: 1754772 N: 404438 N_HV: 156073 Val: 0.7937 Test: 0.7328\n",
      "Epoch: 017, Loss: 0.5096 tsm_loss: 5.5580 reg_loss: 0.5096 N_Y: 406418 N_S: 1754772 N: 406418 N_HV: 163666 Val: 0.8406 Test: 0.7934\n",
      "Epoch: 018, Loss: 0.4919 tsm_loss: 5.5171 reg_loss: 0.4919 N_Y: 404922 N_S: 1754772 N: 404922 N_HV: 156953 Val: 0.7768 Test: 0.7361\n",
      "Epoch: 019, Loss: 0.4977 tsm_loss: 5.2566 reg_loss: 0.4977 N_Y: 405618 N_S: 1754772 N: 405618 N_HV: 160362 Val: 0.8270 Test: 0.7422\n",
      "Epoch: 020, Loss: 0.5024 tsm_loss: 5.4050 reg_loss: 0.5024 N_Y: 407960 N_S: 1754772 N: 407960 N_HV: 160057 Val: 0.7874 Test: 0.7423\n",
      "Epoch: 021, Loss: 0.4859 tsm_loss: 5.5984 reg_loss: 0.4859 N_Y: 405658 N_S: 1754772 N: 405658 N_HV: 165856 Val: 0.7881 Test: 0.7276\n",
      "Epoch: 022, Loss: 0.4645 tsm_loss: 5.3129 reg_loss: 0.4645 N_Y: 405801 N_S: 1754772 N: 405801 N_HV: 158368 Val: 0.8014 Test: 0.7740\n",
      "Epoch: 023, Loss: 0.4712 tsm_loss: 5.3897 reg_loss: 0.4712 N_Y: 406015 N_S: 1754772 N: 406015 N_HV: 158225 Val: 0.8879 Test: 0.8213\n",
      "Epoch: 024, Loss: 0.4897 tsm_loss: 4.9524 reg_loss: 0.4897 N_Y: 407094 N_S: 1754772 N: 407094 N_HV: 155563 Val: 0.8035 Test: 0.7786\n",
      "Epoch: 025, Loss: 0.4875 tsm_loss: 5.2704 reg_loss: 0.4875 N_Y: 406662 N_S: 1754772 N: 406662 N_HV: 161833 Val: 0.8060 Test: 0.7542\n",
      "Epoch: 026, Loss: 0.4501 tsm_loss: 5.2943 reg_loss: 0.4501 N_Y: 407087 N_S: 1754772 N: 407087 N_HV: 160324 Val: 0.8057 Test: 0.7255\n",
      "Epoch: 027, Loss: 0.4698 tsm_loss: 5.0473 reg_loss: 0.4698 N_Y: 407243 N_S: 1754772 N: 407243 N_HV: 158753 Val: 0.8285 Test: 0.7258\n",
      "Epoch: 028, Loss: 0.4397 tsm_loss: 4.9940 reg_loss: 0.4397 N_Y: 406722 N_S: 1754772 N: 406722 N_HV: 162672 Val: 0.7574 Test: 0.7151\n",
      "Epoch: 029, Loss: 0.4237 tsm_loss: 4.8685 reg_loss: 0.4237 N_Y: 403434 N_S: 1754772 N: 403434 N_HV: 156359 Val: 0.7669 Test: 0.6909\n",
      "Epoch: 030, Loss: 0.4647 tsm_loss: 4.9672 reg_loss: 0.4647 N_Y: 405728 N_S: 1754772 N: 405728 N_HV: 161161 Val: 0.9747 Test: 0.9209\n",
      "Epoch: 031, Loss: 0.4372 tsm_loss: 4.8731 reg_loss: 0.4372 N_Y: 405494 N_S: 1754772 N: 405494 N_HV: 155218 Val: 0.7763 Test: 0.6919\n",
      "Epoch: 032, Loss: 0.4274 tsm_loss: 5.0186 reg_loss: 0.4274 N_Y: 407319 N_S: 1754772 N: 407319 N_HV: 161425 Val: 0.8164 Test: 0.7021\n",
      "Epoch: 033, Loss: 0.4113 tsm_loss: 4.9494 reg_loss: 0.4113 N_Y: 404776 N_S: 1754772 N: 404776 N_HV: 157174 Val: 0.8149 Test: 0.6919\n",
      "Epoch: 034, Loss: 0.4189 tsm_loss: 5.1832 reg_loss: 0.4189 N_Y: 406471 N_S: 1754772 N: 406471 N_HV: 159673 Val: 0.8349 Test: 0.7066\n",
      "Epoch: 035, Loss: 0.4192 tsm_loss: 4.8898 reg_loss: 0.4192 N_Y: 405258 N_S: 1754772 N: 405258 N_HV: 153844 Val: 0.8117 Test: 0.7543\n",
      "Epoch: 036, Loss: 0.3953 tsm_loss: 4.6827 reg_loss: 0.3953 N_Y: 403624 N_S: 1754772 N: 403624 N_HV: 157186 Val: 0.8392 Test: 0.7958\n",
      "Epoch: 037, Loss: 0.4153 tsm_loss: 4.7348 reg_loss: 0.4153 N_Y: 406362 N_S: 1754772 N: 406362 N_HV: 161334 Val: 0.9038 Test: 0.7975\n",
      "Epoch: 038, Loss: 0.4149 tsm_loss: 4.4062 reg_loss: 0.4149 N_Y: 406074 N_S: 1754772 N: 406074 N_HV: 152333 Val: 0.8004 Test: 0.7476\n",
      "Epoch: 039, Loss: 0.3988 tsm_loss: 4.5333 reg_loss: 0.3988 N_Y: 406788 N_S: 1754772 N: 406788 N_HV: 157134 Val: 0.8852 Test: 0.7781\n",
      "Epoch: 040, Loss: 0.3968 tsm_loss: 4.5579 reg_loss: 0.3968 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 154274 Val: 0.7811 Test: 0.6848\n",
      "Epoch: 041, Loss: 0.3909 tsm_loss: 4.3413 reg_loss: 0.3909 N_Y: 405878 N_S: 1754772 N: 405878 N_HV: 153542 Val: 0.9527 Test: 0.8503\n",
      "Epoch: 042, Loss: 0.4118 tsm_loss: 4.3603 reg_loss: 0.4118 N_Y: 406512 N_S: 1754772 N: 406512 N_HV: 153048 Val: 0.8169 Test: 0.7498\n",
      "Epoch: 043, Loss: 0.4102 tsm_loss: 4.4149 reg_loss: 0.4102 N_Y: 405416 N_S: 1754772 N: 405416 N_HV: 156123 Val: 0.8386 Test: 0.6978\n",
      "Epoch: 044, Loss: 0.4684 tsm_loss: 4.5210 reg_loss: 0.4684 N_Y: 407704 N_S: 1754772 N: 407704 N_HV: 149893 Val: 0.8651 Test: 0.8089\n",
      "Epoch: 045, Loss: 0.4206 tsm_loss: 4.4004 reg_loss: 0.4206 N_Y: 406899 N_S: 1754772 N: 406899 N_HV: 155149 Val: 0.9486 Test: 0.8767\n",
      "Epoch: 046, Loss: 0.3929 tsm_loss: 4.5142 reg_loss: 0.3929 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 152681 Val: 0.8536 Test: 0.7387\n",
      "Epoch: 047, Loss: 0.3946 tsm_loss: 4.8118 reg_loss: 0.3946 N_Y: 407002 N_S: 1754772 N: 407002 N_HV: 160819 Val: 0.8034 Test: 0.7077\n",
      "Epoch: 048, Loss: 0.3690 tsm_loss: 4.8468 reg_loss: 0.3690 N_Y: 406284 N_S: 1754772 N: 406284 N_HV: 157541 Val: 0.7643 Test: 0.7161\n",
      "Epoch: 049, Loss: 0.4104 tsm_loss: 4.7877 reg_loss: 0.4104 N_Y: 406758 N_S: 1754772 N: 406758 N_HV: 161097 Val: 0.7871 Test: 0.6806\n",
      "Epoch: 050, Loss: 0.3966 tsm_loss: 4.5560 reg_loss: 0.3966 N_Y: 405055 N_S: 1754772 N: 405055 N_HV: 155579 Val: 0.9037 Test: 0.8764\n",
      "Epoch: 051, Loss: 0.3892 tsm_loss: 4.7586 reg_loss: 0.3892 N_Y: 405849 N_S: 1754772 N: 405849 N_HV: 163131 Val: 0.8171 Test: 0.7132\n",
      "Epoch: 052, Loss: 0.3790 tsm_loss: 4.7542 reg_loss: 0.3790 N_Y: 407089 N_S: 1754772 N: 407089 N_HV: 158377 Val: 0.9909 Test: 0.9581\n",
      "Epoch: 053, Loss: 0.3718 tsm_loss: 4.7231 reg_loss: 0.3718 N_Y: 407550 N_S: 1754772 N: 407550 N_HV: 158926 Val: 0.8564 Test: 0.7539\n",
      "Epoch: 054, Loss: 0.4006 tsm_loss: 4.5920 reg_loss: 0.4006 N_Y: 403778 N_S: 1754772 N: 403778 N_HV: 157212 Val: 0.7527 Test: 0.6521\n",
      "Epoch: 055, Loss: 0.3521 tsm_loss: 4.4509 reg_loss: 0.3521 N_Y: 404629 N_S: 1754772 N: 404629 N_HV: 156723 Val: 0.7755 Test: 0.6725\n",
      "Epoch: 056, Loss: 0.4036 tsm_loss: 4.5445 reg_loss: 0.4036 N_Y: 404189 N_S: 1754772 N: 404189 N_HV: 154967 Val: 0.7957 Test: 0.6780\n",
      "Epoch: 057, Loss: 0.3979 tsm_loss: 4.3817 reg_loss: 0.3979 N_Y: 406270 N_S: 1754772 N: 406270 N_HV: 153491 Val: 0.7548 Test: 0.6709\n",
      "Epoch: 058, Loss: 0.3640 tsm_loss: 4.3162 reg_loss: 0.3640 N_Y: 407048 N_S: 1754772 N: 407048 N_HV: 160809 Val: 0.8595 Test: 0.7461\n",
      "Epoch: 059, Loss: 0.3618 tsm_loss: 4.2085 reg_loss: 0.3618 N_Y: 405896 N_S: 1754772 N: 405896 N_HV: 153535 Val: 0.8610 Test: 0.7592\n",
      "Epoch: 060, Loss: 0.3678 tsm_loss: 4.4553 reg_loss: 0.3678 N_Y: 405965 N_S: 1754772 N: 405965 N_HV: 156113 Val: 0.7817 Test: 0.6740\n",
      "Epoch: 061, Loss: 0.3584 tsm_loss: 4.6701 reg_loss: 0.3584 N_Y: 404825 N_S: 1754772 N: 404825 N_HV: 157370 Val: 0.7656 Test: 0.6548\n",
      "Epoch: 062, Loss: 0.3719 tsm_loss: 4.7137 reg_loss: 0.3719 N_Y: 407083 N_S: 1754772 N: 407083 N_HV: 158491 Val: 0.7602 Test: 0.6599\n",
      "Epoch: 063, Loss: 0.3692 tsm_loss: 4.3865 reg_loss: 0.3692 N_Y: 406740 N_S: 1754772 N: 406740 N_HV: 157387 Val: 0.9825 Test: 0.9175\n",
      "Epoch: 064, Loss: 0.3906 tsm_loss: 4.5004 reg_loss: 0.3906 N_Y: 403919 N_S: 1754772 N: 403919 N_HV: 154708 Val: 0.8172 Test: 0.6930\n",
      "Epoch: 065, Loss: 0.3598 tsm_loss: 4.3989 reg_loss: 0.3598 N_Y: 405852 N_S: 1754772 N: 405852 N_HV: 154824 Val: 0.8026 Test: 0.7001\n",
      "Epoch: 066, Loss: 0.3395 tsm_loss: 4.2890 reg_loss: 0.3395 N_Y: 407418 N_S: 1754772 N: 407418 N_HV: 157931 Val: 0.8325 Test: 0.7412\n",
      "Epoch: 067, Loss: 0.3503 tsm_loss: 4.4186 reg_loss: 0.3503 N_Y: 406228 N_S: 1754772 N: 406228 N_HV: 156411 Val: 0.7872 Test: 0.6685\n",
      "Epoch: 068, Loss: 0.3315 tsm_loss: 4.3551 reg_loss: 0.3315 N_Y: 405977 N_S: 1754772 N: 405977 N_HV: 154593 Val: 0.7688 Test: 0.6504\n",
      "Epoch: 069, Loss: 0.3414 tsm_loss: 4.4766 reg_loss: 0.3414 N_Y: 407387 N_S: 1754772 N: 407387 N_HV: 162743 Val: 0.9156 Test: 0.7796\n",
      "Epoch: 070, Loss: 0.3523 tsm_loss: 4.4833 reg_loss: 0.3523 N_Y: 406059 N_S: 1754772 N: 406059 N_HV: 159908 Val: 0.8366 Test: 0.7989\n",
      "Epoch: 071, Loss: 0.3779 tsm_loss: 4.5843 reg_loss: 0.3779 N_Y: 405856 N_S: 1754772 N: 405856 N_HV: 166548 Val: 0.8352 Test: 0.7170\n",
      "Epoch: 072, Loss: 0.3463 tsm_loss: 4.6506 reg_loss: 0.3463 N_Y: 405569 N_S: 1754772 N: 405569 N_HV: 157719 Val: 0.7838 Test: 0.7183\n",
      "Epoch: 073, Loss: 0.3485 tsm_loss: 4.9812 reg_loss: 0.3485 N_Y: 406456 N_S: 1754772 N: 406456 N_HV: 164403 Val: 0.7932 Test: 0.7058\n",
      "Epoch: 074, Loss: 0.3364 tsm_loss: 4.7081 reg_loss: 0.3364 N_Y: 406829 N_S: 1754772 N: 406829 N_HV: 156973 Val: 0.9440 Test: 0.8195\n",
      "Epoch: 075, Loss: 0.3424 tsm_loss: 4.5921 reg_loss: 0.3424 N_Y: 405118 N_S: 1754772 N: 405118 N_HV: 158571 Val: 0.8308 Test: 0.7346\n",
      "Epoch: 076, Loss: 0.3715 tsm_loss: 4.7208 reg_loss: 0.3715 N_Y: 406922 N_S: 1754772 N: 406922 N_HV: 160697 Val: 0.8934 Test: 0.7607\n",
      "Epoch: 077, Loss: 0.3489 tsm_loss: 4.8062 reg_loss: 0.3489 N_Y: 406491 N_S: 1754772 N: 406491 N_HV: 160014 Val: 0.7692 Test: 0.6629\n",
      "Epoch: 078, Loss: 0.3492 tsm_loss: 4.6617 reg_loss: 0.3492 N_Y: 404984 N_S: 1754772 N: 404984 N_HV: 157622 Val: 0.7535 Test: 0.6456\n",
      "Epoch: 079, Loss: 0.3255 tsm_loss: 4.6562 reg_loss: 0.3255 N_Y: 404318 N_S: 1754772 N: 404318 N_HV: 157828 Val: 0.7807 Test: 0.6519\n",
      "Epoch: 080, Loss: 0.3062 tsm_loss: 4.7823 reg_loss: 0.3062 N_Y: 407036 N_S: 1754772 N: 407036 N_HV: 160258 Val: 0.7782 Test: 0.6767\n",
      "Epoch: 081, Loss: 0.3394 tsm_loss: 4.6766 reg_loss: 0.3394 N_Y: 406158 N_S: 1754772 N: 406158 N_HV: 155607 Val: 0.8671 Test: 0.7950\n",
      "Epoch: 082, Loss: 0.3327 tsm_loss: 4.6763 reg_loss: 0.3327 N_Y: 405766 N_S: 1754772 N: 405766 N_HV: 161974 Val: 0.8147 Test: 0.6781\n",
      "Epoch: 083, Loss: 0.3529 tsm_loss: 4.8413 reg_loss: 0.3529 N_Y: 405193 N_S: 1754772 N: 405193 N_HV: 159749 Val: 0.8184 Test: 0.6873\n",
      "Epoch: 084, Loss: 0.2976 tsm_loss: 4.8008 reg_loss: 0.2976 N_Y: 406090 N_S: 1754772 N: 406090 N_HV: 155678 Val: 0.7575 Test: 0.6470\n",
      "Epoch: 085, Loss: 0.3092 tsm_loss: 4.9287 reg_loss: 0.3092 N_Y: 406434 N_S: 1754772 N: 406434 N_HV: 160107 Val: 0.7731 Test: 0.6476\n",
      "Epoch: 086, Loss: 0.3117 tsm_loss: 4.8563 reg_loss: 0.3117 N_Y: 407006 N_S: 1754772 N: 407006 N_HV: 158840 Val: 0.7581 Test: 0.6545\n",
      "Epoch: 087, Loss: 0.3050 tsm_loss: 4.6024 reg_loss: 0.3050 N_Y: 405222 N_S: 1754772 N: 405222 N_HV: 158255 Val: 0.7887 Test: 0.6939\n",
      "Epoch: 088, Loss: 0.3152 tsm_loss: 4.7062 reg_loss: 0.3152 N_Y: 404675 N_S: 1754772 N: 404675 N_HV: 159993 Val: 0.8399 Test: 0.7101\n",
      "Epoch: 089, Loss: 0.3226 tsm_loss: 4.5509 reg_loss: 0.3226 N_Y: 406870 N_S: 1754772 N: 406870 N_HV: 160716 Val: 0.8292 Test: 0.7552\n",
      "Epoch: 090, Loss: 0.3055 tsm_loss: 4.6386 reg_loss: 0.3055 N_Y: 407519 N_S: 1754772 N: 407519 N_HV: 158913 Val: 0.7817 Test: 0.6609\n",
      "Epoch: 091, Loss: 0.2846 tsm_loss: 4.6771 reg_loss: 0.2846 N_Y: 404021 N_S: 1754772 N: 404021 N_HV: 157198 Val: 0.7918 Test: 0.6712\n",
      "Epoch: 092, Loss: 0.2921 tsm_loss: 4.7685 reg_loss: 0.2921 N_Y: 405637 N_S: 1754772 N: 405637 N_HV: 157424 Val: 0.8977 Test: 0.8592\n",
      "Epoch: 093, Loss: 0.3849 tsm_loss: 4.8734 reg_loss: 0.3849 N_Y: 404801 N_S: 1754772 N: 404801 N_HV: 161419 Val: 0.8167 Test: 0.7228\n",
      "Epoch: 094, Loss: 0.3300 tsm_loss: 4.3984 reg_loss: 0.3300 N_Y: 407779 N_S: 1754772 N: 407779 N_HV: 155543 Val: 0.8239 Test: 0.7009\n",
      "Epoch: 095, Loss: 0.3195 tsm_loss: 4.5434 reg_loss: 0.3195 N_Y: 406181 N_S: 1754772 N: 406181 N_HV: 157760 Val: 0.8805 Test: 0.8160\n",
      "Epoch: 096, Loss: 0.3213 tsm_loss: 4.6615 reg_loss: 0.3213 N_Y: 406479 N_S: 1754772 N: 406479 N_HV: 160057 Val: 0.7967 Test: 0.6959\n",
      "Epoch: 097, Loss: 0.3084 tsm_loss: 4.5388 reg_loss: 0.3084 N_Y: 407406 N_S: 1754772 N: 407406 N_HV: 153946 Val: 0.7884 Test: 0.6894\n",
      "Epoch: 098, Loss: 0.3226 tsm_loss: 4.3020 reg_loss: 0.3226 N_Y: 406066 N_S: 1754772 N: 406066 N_HV: 159447 Val: 0.8476 Test: 0.7319\n",
      "Epoch: 099, Loss: 0.3588 tsm_loss: 4.4574 reg_loss: 0.3588 N_Y: 404909 N_S: 1754772 N: 404909 N_HV: 157793 Val: 0.9221 Test: 0.7945\n",
      "Epoch: 100, Loss: 0.3421 tsm_loss: 4.2119 reg_loss: 0.3421 N_Y: 405644 N_S: 1754772 N: 405644 N_HV: 156846 Val: 0.7742 Test: 0.6479\n",
      "Epoch: 101, Loss: 0.2999 tsm_loss: 4.2006 reg_loss: 0.2999 N_Y: 406463 N_S: 1754772 N: 406463 N_HV: 157310 Val: 0.8627 Test: 0.7145\n",
      "Epoch: 102, Loss: 0.2924 tsm_loss: 4.4168 reg_loss: 0.2924 N_Y: 406729 N_S: 1754772 N: 406729 N_HV: 155756 Val: 0.8961 Test: 0.8266\n",
      "Epoch: 103, Loss: 0.2988 tsm_loss: 4.5391 reg_loss: 0.2988 N_Y: 406258 N_S: 1754772 N: 406258 N_HV: 160074 Val: 0.8088 Test: 0.6674\n",
      "Epoch: 104, Loss: 0.3194 tsm_loss: 4.3046 reg_loss: 0.3194 N_Y: 406326 N_S: 1754772 N: 406326 N_HV: 155180 Val: 0.7684 Test: 0.6556\n",
      "Epoch: 105, Loss: 0.3034 tsm_loss: 4.4175 reg_loss: 0.3034 N_Y: 406876 N_S: 1754772 N: 406876 N_HV: 159627 Val: 0.8116 Test: 0.7053\n",
      "Epoch: 106, Loss: 0.3003 tsm_loss: 4.5391 reg_loss: 0.3003 N_Y: 406131 N_S: 1754772 N: 406131 N_HV: 160498 Val: 0.7518 Test: 0.6417\n",
      "Epoch: 107, Loss: 0.2673 tsm_loss: 4.3418 reg_loss: 0.2673 N_Y: 406707 N_S: 1754772 N: 406707 N_HV: 157493 Val: 0.7703 Test: 0.6487\n",
      "Epoch: 108, Loss: 0.2965 tsm_loss: 4.5150 reg_loss: 0.2965 N_Y: 406186 N_S: 1754772 N: 406186 N_HV: 161870 Val: 0.8046 Test: 0.6940\n",
      "Epoch: 109, Loss: 0.2635 tsm_loss: 4.5287 reg_loss: 0.2635 N_Y: 405316 N_S: 1754772 N: 405316 N_HV: 159743 Val: 0.7881 Test: 0.6771\n",
      "Epoch: 110, Loss: 0.2510 tsm_loss: 4.3157 reg_loss: 0.2510 N_Y: 404106 N_S: 1754772 N: 404106 N_HV: 156573 Val: 0.8086 Test: 0.6736\n",
      "Epoch: 111, Loss: 0.2857 tsm_loss: 4.4569 reg_loss: 0.2857 N_Y: 406924 N_S: 1754772 N: 406924 N_HV: 164745 Val: 0.7921 Test: 0.6947\n",
      "Epoch: 112, Loss: 0.2664 tsm_loss: 4.4977 reg_loss: 0.2664 N_Y: 405292 N_S: 1754772 N: 405292 N_HV: 158896 Val: 0.9090 Test: 0.8140\n",
      "Epoch: 113, Loss: 0.2758 tsm_loss: 4.2534 reg_loss: 0.2758 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 159147 Val: 0.7900 Test: 0.6717\n",
      "Epoch: 114, Loss: 0.2784 tsm_loss: 4.3984 reg_loss: 0.2784 N_Y: 407354 N_S: 1754772 N: 407354 N_HV: 162529 Val: 0.8259 Test: 0.6952\n",
      "Epoch: 115, Loss: 0.2781 tsm_loss: 4.4489 reg_loss: 0.2781 N_Y: 404537 N_S: 1754772 N: 404537 N_HV: 155750 Val: 0.8044 Test: 0.7117\n",
      "Epoch: 116, Loss: 0.2982 tsm_loss: 4.7349 reg_loss: 0.2982 N_Y: 405245 N_S: 1754772 N: 405245 N_HV: 161496 Val: 0.9667 Test: 0.8378\n",
      "Epoch: 117, Loss: 0.2851 tsm_loss: 4.1127 reg_loss: 0.2851 N_Y: 403369 N_S: 1754772 N: 403369 N_HV: 153895 Val: 0.8265 Test: 0.6920\n",
      "Epoch: 118, Loss: 0.2641 tsm_loss: 4.1375 reg_loss: 0.2641 N_Y: 406939 N_S: 1754772 N: 406939 N_HV: 161152 Val: 0.7985 Test: 0.6868\n",
      "Epoch: 119, Loss: 0.2551 tsm_loss: 4.1724 reg_loss: 0.2551 N_Y: 405775 N_S: 1754772 N: 405775 N_HV: 155694 Val: 0.7786 Test: 0.6608\n",
      "Epoch: 120, Loss: 0.2748 tsm_loss: 4.4268 reg_loss: 0.2748 N_Y: 405718 N_S: 1754772 N: 405718 N_HV: 163526 Val: 0.7887 Test: 0.6494\n",
      "Epoch: 121, Loss: 0.2574 tsm_loss: 4.4795 reg_loss: 0.2574 N_Y: 407439 N_S: 1754772 N: 407439 N_HV: 159734 Val: 0.8324 Test: 0.7500\n",
      "Epoch: 122, Loss: 0.2857 tsm_loss: 4.4185 reg_loss: 0.2857 N_Y: 406141 N_S: 1754772 N: 406141 N_HV: 163456 Val: 0.7767 Test: 0.6433\n",
      "Epoch: 123, Loss: 0.2653 tsm_loss: 4.4409 reg_loss: 0.2653 N_Y: 406872 N_S: 1754772 N: 406872 N_HV: 160330 Val: 0.7709 Test: 0.6458\n",
      "Epoch: 124, Loss: 0.2668 tsm_loss: 4.3802 reg_loss: 0.2668 N_Y: 407102 N_S: 1754772 N: 407102 N_HV: 157817 Val: 0.7857 Test: 0.6637\n",
      "Epoch: 125, Loss: 0.2781 tsm_loss: 4.5855 reg_loss: 0.2781 N_Y: 406330 N_S: 1754772 N: 406330 N_HV: 163980 Val: 0.8386 Test: 0.6987\n",
      "Epoch: 126, Loss: 0.2623 tsm_loss: 4.4961 reg_loss: 0.2623 N_Y: 404927 N_S: 1754772 N: 404927 N_HV: 158699 Val: 0.7569 Test: 0.6400\n",
      "Epoch: 127, Loss: 0.2430 tsm_loss: 4.5226 reg_loss: 0.2430 N_Y: 406536 N_S: 1754772 N: 406536 N_HV: 157566 Val: 0.7687 Test: 0.6422\n",
      "Epoch: 128, Loss: 0.2594 tsm_loss: 4.3517 reg_loss: 0.2594 N_Y: 406174 N_S: 1754772 N: 406174 N_HV: 159134 Val: 0.7365 Test: 0.6651\n",
      "Epoch: 129, Loss: 0.2621 tsm_loss: 4.2712 reg_loss: 0.2621 N_Y: 405806 N_S: 1754772 N: 405806 N_HV: 155296 Val: 0.7506 Test: 0.6584\n",
      "Epoch: 130, Loss: 0.2506 tsm_loss: 4.1626 reg_loss: 0.2506 N_Y: 406162 N_S: 1754772 N: 406162 N_HV: 158068 Val: 0.7834 Test: 0.6773\n",
      "Epoch: 131, Loss: 0.2892 tsm_loss: 4.3758 reg_loss: 0.2892 N_Y: 406333 N_S: 1754772 N: 406333 N_HV: 159131 Val: 0.8281 Test: 0.7419\n",
      "Epoch: 132, Loss: 0.2301 tsm_loss: 4.4763 reg_loss: 0.2301 N_Y: 406173 N_S: 1754772 N: 406173 N_HV: 157804 Val: 0.7648 Test: 0.6411\n",
      "Epoch: 133, Loss: 0.2865 tsm_loss: 4.4110 reg_loss: 0.2865 N_Y: 406469 N_S: 1754772 N: 406469 N_HV: 160314 Val: 0.9160 Test: 0.7844\n",
      "Epoch: 134, Loss: 0.3128 tsm_loss: 4.4902 reg_loss: 0.3128 N_Y: 407921 N_S: 1754772 N: 407921 N_HV: 162309 Val: 0.7794 Test: 0.6701\n",
      "Epoch: 135, Loss: 0.2492 tsm_loss: 4.4371 reg_loss: 0.2492 N_Y: 406910 N_S: 1754772 N: 406910 N_HV: 161337 Val: 0.7771 Test: 0.6457\n",
      "Epoch: 136, Loss: 0.3030 tsm_loss: 4.2378 reg_loss: 0.3030 N_Y: 405676 N_S: 1754772 N: 405676 N_HV: 162406 Val: 0.7765 Test: 0.6479\n",
      "Epoch: 137, Loss: 0.2513 tsm_loss: 4.2298 reg_loss: 0.2513 N_Y: 405759 N_S: 1754772 N: 405759 N_HV: 163108 Val: 0.7885 Test: 0.6668\n",
      "Epoch: 138, Loss: 0.2449 tsm_loss: 4.2530 reg_loss: 0.2449 N_Y: 405236 N_S: 1754772 N: 405236 N_HV: 157140 Val: 0.7724 Test: 0.6467\n",
      "Epoch: 139, Loss: 0.2475 tsm_loss: 4.3340 reg_loss: 0.2475 N_Y: 406129 N_S: 1754772 N: 406129 N_HV: 161229 Val: 0.8082 Test: 0.6653\n",
      "Epoch: 140, Loss: 0.3146 tsm_loss: 4.0869 reg_loss: 0.3146 N_Y: 404875 N_S: 1754772 N: 404875 N_HV: 155467 Val: 0.8502 Test: 0.7567\n",
      "Epoch: 141, Loss: 0.2438 tsm_loss: 4.0846 reg_loss: 0.2438 N_Y: 405828 N_S: 1754772 N: 405828 N_HV: 160149 Val: 0.7810 Test: 0.6536\n",
      "Epoch: 142, Loss: 0.2910 tsm_loss: 4.0758 reg_loss: 0.2910 N_Y: 404139 N_S: 1754772 N: 404139 N_HV: 161141 Val: 0.7645 Test: 0.6538\n",
      "Epoch: 143, Loss: 0.2297 tsm_loss: 4.2108 reg_loss: 0.2297 N_Y: 406562 N_S: 1754772 N: 406562 N_HV: 160012 Val: 0.7581 Test: 0.6822\n",
      "Epoch: 144, Loss: 0.2336 tsm_loss: 4.0716 reg_loss: 0.2336 N_Y: 406577 N_S: 1754772 N: 406577 N_HV: 154841 Val: 0.7683 Test: 0.6514\n",
      "Epoch: 145, Loss: 0.2302 tsm_loss: 4.0949 reg_loss: 0.2302 N_Y: 406681 N_S: 1754772 N: 406681 N_HV: 159677 Val: 0.7954 Test: 0.6712\n",
      "Epoch: 146, Loss: 0.2328 tsm_loss: 3.9445 reg_loss: 0.2328 N_Y: 406289 N_S: 1754772 N: 406289 N_HV: 156307 Val: 0.7528 Test: 0.6351\n",
      "Epoch: 147, Loss: 0.2169 tsm_loss: 4.2092 reg_loss: 0.2169 N_Y: 406057 N_S: 1754772 N: 406057 N_HV: 164539 Val: 0.7823 Test: 0.6610\n",
      "Epoch: 148, Loss: 0.2302 tsm_loss: 4.2065 reg_loss: 0.2302 N_Y: 405415 N_S: 1754772 N: 405415 N_HV: 161712 Val: 0.7685 Test: 0.6524\n",
      "Epoch: 149, Loss: 0.2295 tsm_loss: 4.3883 reg_loss: 0.2295 N_Y: 405036 N_S: 1754772 N: 405036 N_HV: 159595 Val: 0.7932 Test: 0.7024\n",
      "Epoch: 150, Loss: 0.2231 tsm_loss: 4.5512 reg_loss: 0.2231 N_Y: 407155 N_S: 1754772 N: 407155 N_HV: 166529 Val: 0.7608 Test: 0.6431\n",
      "Epoch: 151, Loss: 0.2238 tsm_loss: 4.5172 reg_loss: 0.2238 N_Y: 406000 N_S: 1754772 N: 406000 N_HV: 164650 Val: 0.7735 Test: 0.6500\n",
      "Epoch: 152, Loss: 0.2307 tsm_loss: 4.5767 reg_loss: 0.2307 N_Y: 407945 N_S: 1754772 N: 407945 N_HV: 164967 Val: 0.8641 Test: 0.7926\n",
      "Epoch: 153, Loss: 0.2371 tsm_loss: 4.4503 reg_loss: 0.2371 N_Y: 406630 N_S: 1754772 N: 406630 N_HV: 161059 Val: 0.8583 Test: 0.7345\n",
      "Epoch: 154, Loss: 0.3107 tsm_loss: 4.4234 reg_loss: 0.3107 N_Y: 406408 N_S: 1754772 N: 406408 N_HV: 164169 Val: 0.7581 Test: 0.6374\n",
      "Epoch: 155, Loss: 0.2887 tsm_loss: 4.2029 reg_loss: 0.2887 N_Y: 407058 N_S: 1754772 N: 407058 N_HV: 162369 Val: 0.7810 Test: 0.6629\n",
      "Epoch: 156, Loss: 0.2575 tsm_loss: 4.4100 reg_loss: 0.2575 N_Y: 403853 N_S: 1754772 N: 403853 N_HV: 165831 Val: 0.7504 Test: 0.6259\n",
      "Epoch: 157, Loss: 0.2583 tsm_loss: 3.9979 reg_loss: 0.2583 N_Y: 406257 N_S: 1754772 N: 406257 N_HV: 159721 Val: 0.7636 Test: 0.6544\n",
      "Epoch: 158, Loss: 0.2182 tsm_loss: 4.1742 reg_loss: 0.2182 N_Y: 405295 N_S: 1754772 N: 405295 N_HV: 159413 Val: 0.7775 Test: 0.6619\n",
      "Epoch: 159, Loss: 0.2201 tsm_loss: 4.1802 reg_loss: 0.2201 N_Y: 405908 N_S: 1754772 N: 405908 N_HV: 158389 Val: 0.7927 Test: 0.7217\n",
      "Epoch: 160, Loss: 0.2521 tsm_loss: 3.9953 reg_loss: 0.2521 N_Y: 406271 N_S: 1754772 N: 406271 N_HV: 161183 Val: 0.7914 Test: 0.6625\n",
      "Epoch: 161, Loss: 0.2352 tsm_loss: 4.0717 reg_loss: 0.2352 N_Y: 407799 N_S: 1754772 N: 407799 N_HV: 160701 Val: 0.7765 Test: 0.6403\n",
      "Epoch: 162, Loss: 0.2425 tsm_loss: 4.2182 reg_loss: 0.2425 N_Y: 406273 N_S: 1754772 N: 406273 N_HV: 165027 Val: 0.8707 Test: 0.7763\n",
      "Epoch: 163, Loss: 0.3108 tsm_loss: 4.2981 reg_loss: 0.3108 N_Y: 407078 N_S: 1754772 N: 407078 N_HV: 162504 Val: 0.9808 Test: 0.8715\n",
      "Epoch: 164, Loss: 0.3225 tsm_loss: 4.6118 reg_loss: 0.3225 N_Y: 407295 N_S: 1754772 N: 407295 N_HV: 168968 Val: 0.7463 Test: 0.6320\n",
      "Epoch: 165, Loss: 0.2553 tsm_loss: 4.0941 reg_loss: 0.2553 N_Y: 406269 N_S: 1754772 N: 406269 N_HV: 156038 Val: 0.7541 Test: 0.6302\n",
      "Epoch: 166, Loss: 0.2192 tsm_loss: 4.1681 reg_loss: 0.2192 N_Y: 406299 N_S: 1754772 N: 406299 N_HV: 159782 Val: 0.8628 Test: 0.7896\n",
      "Epoch: 167, Loss: 0.2532 tsm_loss: 4.1667 reg_loss: 0.2532 N_Y: 405672 N_S: 1754772 N: 405672 N_HV: 159797 Val: 0.7947 Test: 0.6690\n",
      "Epoch: 168, Loss: 0.2363 tsm_loss: 4.0767 reg_loss: 0.2363 N_Y: 407344 N_S: 1754772 N: 407344 N_HV: 159579 Val: 0.7480 Test: 0.6506\n",
      "Epoch: 169, Loss: 0.2121 tsm_loss: 4.3134 reg_loss: 0.2121 N_Y: 406736 N_S: 1754772 N: 406736 N_HV: 160791 Val: 0.7817 Test: 0.6387\n",
      "Epoch: 170, Loss: 0.2109 tsm_loss: 4.0749 reg_loss: 0.2109 N_Y: 405863 N_S: 1754772 N: 405863 N_HV: 159257 Val: 0.7595 Test: 0.6409\n",
      "Epoch: 171, Loss: 0.2108 tsm_loss: 4.0837 reg_loss: 0.2108 N_Y: 407109 N_S: 1754772 N: 407109 N_HV: 164457 Val: 0.8202 Test: 0.7394\n",
      "Epoch: 172, Loss: 0.2316 tsm_loss: 4.1254 reg_loss: 0.2316 N_Y: 406730 N_S: 1754772 N: 406730 N_HV: 158827 Val: 0.7447 Test: 0.6424\n",
      "Epoch: 173, Loss: 0.2288 tsm_loss: 4.1129 reg_loss: 0.2288 N_Y: 406542 N_S: 1754772 N: 406542 N_HV: 163606 Val: 0.7824 Test: 0.6797\n",
      "Epoch: 174, Loss: 0.2107 tsm_loss: 3.9167 reg_loss: 0.2107 N_Y: 403669 N_S: 1754772 N: 403669 N_HV: 160886 Val: 0.7545 Test: 0.6320\n",
      "Epoch: 175, Loss: 0.2092 tsm_loss: 4.0720 reg_loss: 0.2092 N_Y: 407396 N_S: 1754772 N: 407396 N_HV: 164511 Val: 0.8136 Test: 0.6940\n",
      "Epoch: 176, Loss: 0.2835 tsm_loss: 4.1470 reg_loss: 0.2835 N_Y: 406142 N_S: 1754772 N: 406142 N_HV: 164052 Val: 0.7819 Test: 0.6949\n",
      "Epoch: 177, Loss: 0.2133 tsm_loss: 4.1366 reg_loss: 0.2133 N_Y: 405266 N_S: 1754772 N: 405266 N_HV: 164708 Val: 0.7823 Test: 0.6398\n",
      "Epoch: 178, Loss: 0.1982 tsm_loss: 4.0720 reg_loss: 0.1982 N_Y: 406765 N_S: 1754772 N: 406765 N_HV: 161290 Val: 0.7866 Test: 0.6562\n",
      "Epoch: 179, Loss: 0.2043 tsm_loss: 3.9497 reg_loss: 0.2043 N_Y: 406957 N_S: 1754772 N: 406957 N_HV: 162183 Val: 0.7681 Test: 0.6588\n",
      "Epoch: 180, Loss: 0.2111 tsm_loss: 4.0433 reg_loss: 0.2111 N_Y: 404782 N_S: 1754772 N: 404782 N_HV: 162235 Val: 0.7589 Test: 0.6471\n",
      "Epoch: 181, Loss: 0.2173 tsm_loss: 4.0922 reg_loss: 0.2173 N_Y: 405579 N_S: 1754772 N: 405579 N_HV: 162562 Val: 0.8204 Test: 0.7657\n",
      "Epoch: 182, Loss: 0.2656 tsm_loss: 4.0948 reg_loss: 0.2656 N_Y: 406194 N_S: 1754772 N: 406194 N_HV: 166882 Val: 0.7399 Test: 0.6333\n",
      "Epoch: 183, Loss: 0.2459 tsm_loss: 4.1301 reg_loss: 0.2459 N_Y: 405616 N_S: 1754772 N: 405616 N_HV: 167290 Val: 0.7458 Test: 0.6266\n",
      "Epoch: 184, Loss: 0.2323 tsm_loss: 4.2550 reg_loss: 0.2323 N_Y: 407099 N_S: 1754772 N: 407099 N_HV: 166451 Val: 0.7556 Test: 0.6363\n",
      "Epoch: 185, Loss: 0.2421 tsm_loss: 3.9794 reg_loss: 0.2421 N_Y: 405148 N_S: 1754772 N: 405148 N_HV: 160626 Val: 0.8275 Test: 0.6981\n",
      "Epoch: 186, Loss: 0.2397 tsm_loss: 4.0366 reg_loss: 0.2397 N_Y: 405871 N_S: 1754772 N: 405871 N_HV: 165799 Val: 0.7273 Test: 0.6315\n",
      "Epoch: 187, Loss: 0.2363 tsm_loss: 3.8030 reg_loss: 0.2363 N_Y: 406539 N_S: 1754772 N: 406539 N_HV: 161730 Val: 0.7840 Test: 0.6761\n",
      "Epoch: 188, Loss: 0.2121 tsm_loss: 3.9904 reg_loss: 0.2121 N_Y: 404984 N_S: 1754772 N: 404984 N_HV: 165178 Val: 0.7733 Test: 0.6823\n",
      "Epoch: 189, Loss: 0.2090 tsm_loss: 4.0814 reg_loss: 0.2090 N_Y: 406673 N_S: 1754772 N: 406673 N_HV: 164219 Val: 0.7502 Test: 0.6556\n",
      "Epoch: 190, Loss: 0.1968 tsm_loss: 4.1492 reg_loss: 0.1968 N_Y: 405937 N_S: 1754772 N: 405937 N_HV: 163524 Val: 0.7729 Test: 0.6625\n",
      "Epoch: 191, Loss: 0.2006 tsm_loss: 4.2340 reg_loss: 0.2006 N_Y: 405366 N_S: 1754772 N: 405366 N_HV: 164065 Val: 0.7819 Test: 0.6833\n",
      "Epoch: 192, Loss: 0.1929 tsm_loss: 4.1597 reg_loss: 0.1929 N_Y: 406747 N_S: 1754772 N: 406747 N_HV: 167759 Val: 0.7594 Test: 0.6440\n",
      "Epoch: 193, Loss: 0.1927 tsm_loss: 4.1490 reg_loss: 0.1927 N_Y: 405199 N_S: 1754772 N: 405199 N_HV: 163414 Val: 0.7466 Test: 0.6378\n",
      "Epoch: 194, Loss: 0.1988 tsm_loss: 4.0460 reg_loss: 0.1988 N_Y: 407539 N_S: 1754772 N: 407539 N_HV: 162629 Val: 0.7780 Test: 0.6422\n",
      "Epoch: 195, Loss: 0.2017 tsm_loss: 3.9760 reg_loss: 0.2017 N_Y: 406072 N_S: 1754772 N: 406072 N_HV: 167808 Val: 0.7476 Test: 0.6370\n",
      "Epoch: 196, Loss: 0.1844 tsm_loss: 3.9843 reg_loss: 0.1844 N_Y: 406493 N_S: 1754772 N: 406493 N_HV: 162690 Val: 0.7455 Test: 0.6346\n",
      "Epoch: 197, Loss: 0.1978 tsm_loss: 4.0733 reg_loss: 0.1978 N_Y: 406052 N_S: 1754772 N: 406052 N_HV: 168477 Val: 0.7597 Test: 0.6448\n",
      "Epoch: 198, Loss: 0.2244 tsm_loss: 4.1008 reg_loss: 0.2244 N_Y: 405360 N_S: 1754772 N: 405360 N_HV: 164263 Val: 0.7569 Test: 0.6863\n",
      "Epoch: 199, Loss: 0.2109 tsm_loss: 4.0125 reg_loss: 0.2109 N_Y: 405635 N_S: 1754772 N: 405635 N_HV: 166940 Val: 0.7658 Test: 0.6577\n",
      "Epoch: 200, Loss: 0.2189 tsm_loss: 3.9760 reg_loss: 0.2189 N_Y: 407098 N_S: 1754772 N: 407098 N_HV: 164365 Val: 0.7739 Test: 0.6532\n",
      "Epoch: 201, Loss: 0.2041 tsm_loss: 4.0947 reg_loss: 0.2041 N_Y: 405911 N_S: 1754772 N: 405911 N_HV: 165691 Val: 0.7517 Test: 0.6648\n",
      "Epoch: 202, Loss: 0.2167 tsm_loss: 4.1438 reg_loss: 0.2167 N_Y: 404144 N_S: 1754772 N: 404144 N_HV: 164362 Val: 0.7484 Test: 0.6409\n",
      "Epoch: 203, Loss: 0.2175 tsm_loss: 4.3012 reg_loss: 0.2175 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 172161 Val: 0.7582 Test: 0.6502\n",
      "Epoch: 204, Loss: 0.2061 tsm_loss: 4.0744 reg_loss: 0.2061 N_Y: 405318 N_S: 1754772 N: 405318 N_HV: 163946 Val: 0.7455 Test: 0.6487\n",
      "Epoch: 205, Loss: 0.1869 tsm_loss: 4.1057 reg_loss: 0.1869 N_Y: 406470 N_S: 1754772 N: 406470 N_HV: 168822 Val: 0.7590 Test: 0.6443\n",
      "Epoch: 206, Loss: 0.1857 tsm_loss: 4.0203 reg_loss: 0.1857 N_Y: 405566 N_S: 1754772 N: 405566 N_HV: 168939 Val: 0.7391 Test: 0.6315\n",
      "Epoch: 207, Loss: 0.1834 tsm_loss: 4.0176 reg_loss: 0.1834 N_Y: 405253 N_S: 1754772 N: 405253 N_HV: 168779 Val: 0.7711 Test: 0.6945\n",
      "Epoch: 208, Loss: 0.1814 tsm_loss: 4.1311 reg_loss: 0.1814 N_Y: 405789 N_S: 1754772 N: 405789 N_HV: 169573 Val: 0.7599 Test: 0.6539\n",
      "Epoch: 209, Loss: 0.2110 tsm_loss: 4.1532 reg_loss: 0.2110 N_Y: 406144 N_S: 1754772 N: 406144 N_HV: 168799 Val: 0.8120 Test: 0.6896\n",
      "Epoch: 210, Loss: 0.2153 tsm_loss: 4.0214 reg_loss: 0.2153 N_Y: 403127 N_S: 1754772 N: 403127 N_HV: 167425 Val: 0.7307 Test: 0.6321\n",
      "Epoch: 211, Loss: 0.2308 tsm_loss: 4.0780 reg_loss: 0.2308 N_Y: 407135 N_S: 1754772 N: 407135 N_HV: 170668 Val: 0.7749 Test: 0.6728\n",
      "Epoch: 212, Loss: 0.2211 tsm_loss: 4.0772 reg_loss: 0.2211 N_Y: 406901 N_S: 1754772 N: 406901 N_HV: 169649 Val: 0.7516 Test: 0.6445\n",
      "Epoch: 213, Loss: 0.1779 tsm_loss: 3.9759 reg_loss: 0.1779 N_Y: 405890 N_S: 1754772 N: 405890 N_HV: 167121 Val: 0.7456 Test: 0.6764\n",
      "Epoch: 214, Loss: 0.1778 tsm_loss: 4.1281 reg_loss: 0.1778 N_Y: 406296 N_S: 1754772 N: 406296 N_HV: 169919 Val: 0.7629 Test: 0.6636\n",
      "Epoch: 215, Loss: 0.1595 tsm_loss: 3.9638 reg_loss: 0.1595 N_Y: 403586 N_S: 1754772 N: 403586 N_HV: 165009 Val: 0.7435 Test: 0.6374\n",
      "Epoch: 216, Loss: 0.1825 tsm_loss: 3.7949 reg_loss: 0.1825 N_Y: 406639 N_S: 1754772 N: 406639 N_HV: 165690 Val: 0.7663 Test: 0.6658\n",
      "Epoch: 217, Loss: 0.1842 tsm_loss: 3.8190 reg_loss: 0.1842 N_Y: 406361 N_S: 1754772 N: 406361 N_HV: 166075 Val: 0.7441 Test: 0.6394\n",
      "Epoch: 218, Loss: 0.1756 tsm_loss: 3.8771 reg_loss: 0.1756 N_Y: 406864 N_S: 1754772 N: 406864 N_HV: 168397 Val: 0.8194 Test: 0.7571\n",
      "Epoch: 219, Loss: 0.2142 tsm_loss: 3.8554 reg_loss: 0.2142 N_Y: 406069 N_S: 1754772 N: 406069 N_HV: 166937 Val: 0.7659 Test: 0.6593\n",
      "Epoch: 220, Loss: 0.2331 tsm_loss: 3.8452 reg_loss: 0.2331 N_Y: 406819 N_S: 1754772 N: 406819 N_HV: 169952 Val: 0.8952 Test: 0.8021\n",
      "Epoch: 221, Loss: 0.1877 tsm_loss: 3.8032 reg_loss: 0.1877 N_Y: 406537 N_S: 1754772 N: 406537 N_HV: 166868 Val: 0.7318 Test: 0.6409\n",
      "Epoch: 222, Loss: 0.2223 tsm_loss: 3.9133 reg_loss: 0.2223 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 167400 Val: 0.7506 Test: 0.6520\n",
      "Epoch: 223, Loss: 0.2171 tsm_loss: 4.0029 reg_loss: 0.2171 N_Y: 405589 N_S: 1754772 N: 405589 N_HV: 167190 Val: 0.7734 Test: 0.6445\n",
      "Epoch: 224, Loss: 0.2363 tsm_loss: 3.8346 reg_loss: 0.2363 N_Y: 406410 N_S: 1754772 N: 406410 N_HV: 164711 Val: 0.7409 Test: 0.6345\n",
      "Epoch: 225, Loss: 0.1909 tsm_loss: 3.9647 reg_loss: 0.1909 N_Y: 405524 N_S: 1754772 N: 405524 N_HV: 167546 Val: 0.7648 Test: 0.6701\n",
      "Epoch: 226, Loss: 0.1748 tsm_loss: 3.9320 reg_loss: 0.1748 N_Y: 406650 N_S: 1754772 N: 406650 N_HV: 167945 Val: 0.7550 Test: 0.6959\n",
      "Epoch: 227, Loss: 0.1617 tsm_loss: 3.7915 reg_loss: 0.1617 N_Y: 405631 N_S: 1754772 N: 405631 N_HV: 164692 Val: 0.8059 Test: 0.6952\n",
      "Epoch: 228, Loss: 0.1657 tsm_loss: 3.9416 reg_loss: 0.1657 N_Y: 406719 N_S: 1754772 N: 406719 N_HV: 168194 Val: 0.7376 Test: 0.6412\n",
      "Epoch: 229, Loss: 0.1669 tsm_loss: 3.8828 reg_loss: 0.1669 N_Y: 406502 N_S: 1754772 N: 406502 N_HV: 166417 Val: 0.7316 Test: 0.6322\n",
      "Epoch: 230, Loss: 0.1839 tsm_loss: 3.7904 reg_loss: 0.1839 N_Y: 406822 N_S: 1754772 N: 406822 N_HV: 164025 Val: 0.7437 Test: 0.6369\n",
      "Epoch: 231, Loss: 0.1502 tsm_loss: 3.7502 reg_loss: 0.1502 N_Y: 406432 N_S: 1754772 N: 406432 N_HV: 164047 Val: 0.7474 Test: 0.6656\n",
      "Epoch: 232, Loss: 0.1715 tsm_loss: 3.8693 reg_loss: 0.1715 N_Y: 406902 N_S: 1754772 N: 406902 N_HV: 166004 Val: 0.7438 Test: 0.6502\n",
      "Epoch: 233, Loss: 0.1619 tsm_loss: 4.0879 reg_loss: 0.1619 N_Y: 406744 N_S: 1754772 N: 406744 N_HV: 168339 Val: 0.7572 Test: 0.6571\n",
      "Epoch: 234, Loss: 0.1890 tsm_loss: 3.9911 reg_loss: 0.1890 N_Y: 405061 N_S: 1754772 N: 405061 N_HV: 165917 Val: 0.7446 Test: 0.6500\n",
      "Epoch: 235, Loss: 0.1876 tsm_loss: 3.8037 reg_loss: 0.1876 N_Y: 406032 N_S: 1754772 N: 406032 N_HV: 166183 Val: 0.7342 Test: 0.6353\n",
      "Epoch: 236, Loss: 0.1779 tsm_loss: 3.7940 reg_loss: 0.1779 N_Y: 405734 N_S: 1754772 N: 405734 N_HV: 169496 Val: 0.7618 Test: 0.6360\n",
      "Epoch: 237, Loss: 0.1745 tsm_loss: 3.8041 reg_loss: 0.1745 N_Y: 406021 N_S: 1754772 N: 406021 N_HV: 164770 Val: 0.7542 Test: 0.6506\n",
      "Epoch: 238, Loss: 0.1904 tsm_loss: 4.0360 reg_loss: 0.1904 N_Y: 404844 N_S: 1754772 N: 404844 N_HV: 169690 Val: 0.7694 Test: 0.6710\n",
      "Epoch: 239, Loss: 0.1756 tsm_loss: 3.9326 reg_loss: 0.1756 N_Y: 404242 N_S: 1754772 N: 404242 N_HV: 165612 Val: 0.7910 Test: 0.6648\n",
      "Epoch: 240, Loss: 0.1775 tsm_loss: 3.8895 reg_loss: 0.1775 N_Y: 405252 N_S: 1754772 N: 405252 N_HV: 169855 Val: 0.7580 Test: 0.6628\n",
      "Epoch: 241, Loss: 0.1633 tsm_loss: 4.0700 reg_loss: 0.1633 N_Y: 406443 N_S: 1754772 N: 406443 N_HV: 173744 Val: 0.7276 Test: 0.6349\n",
      "Epoch: 242, Loss: 0.1517 tsm_loss: 3.9072 reg_loss: 0.1517 N_Y: 406082 N_S: 1754772 N: 406082 N_HV: 169115 Val: 0.7436 Test: 0.6484\n",
      "Epoch: 243, Loss: 0.1778 tsm_loss: 3.8952 reg_loss: 0.1778 N_Y: 407247 N_S: 1754772 N: 407247 N_HV: 167703 Val: 0.7389 Test: 0.6674\n",
      "Epoch: 244, Loss: 0.1998 tsm_loss: 3.7986 reg_loss: 0.1998 N_Y: 406834 N_S: 1754772 N: 406834 N_HV: 168117 Val: 0.7529 Test: 0.6464\n",
      "Epoch: 245, Loss: 0.2400 tsm_loss: 3.8109 reg_loss: 0.2400 N_Y: 405576 N_S: 1754772 N: 405576 N_HV: 168610 Val: 0.7728 Test: 0.6991\n",
      "Epoch: 246, Loss: 0.2304 tsm_loss: 3.8048 reg_loss: 0.2304 N_Y: 404776 N_S: 1754772 N: 404776 N_HV: 163261 Val: 0.7397 Test: 0.6391\n",
      "Epoch: 247, Loss: 0.2122 tsm_loss: 3.8866 reg_loss: 0.2122 N_Y: 406891 N_S: 1754772 N: 406891 N_HV: 171633 Val: 0.7598 Test: 0.6484\n",
      "Epoch: 248, Loss: 0.1724 tsm_loss: 3.8049 reg_loss: 0.1724 N_Y: 405142 N_S: 1754772 N: 405142 N_HV: 168012 Val: 0.7634 Test: 0.6480\n",
      "Epoch: 249, Loss: 0.1734 tsm_loss: 3.7431 reg_loss: 0.1734 N_Y: 406625 N_S: 1754772 N: 406625 N_HV: 169385 Val: 0.7422 Test: 0.6387\n",
      "Epoch: 250, Loss: 0.1786 tsm_loss: 3.7829 reg_loss: 0.1786 N_Y: 405641 N_S: 1754772 N: 405641 N_HV: 168321 Val: 0.7402 Test: 0.6525\n",
      "Epoch: 251, Loss: 0.2111 tsm_loss: 3.7594 reg_loss: 0.2111 N_Y: 404299 N_S: 1754772 N: 404299 N_HV: 171977 Val: 0.7679 Test: 0.6948\n",
      "Epoch: 252, Loss: 0.1775 tsm_loss: 3.5564 reg_loss: 0.1775 N_Y: 406095 N_S: 1754772 N: 406095 N_HV: 167968 Val: 0.7252 Test: 0.6147\n",
      "Epoch: 253, Loss: 0.1706 tsm_loss: 3.7387 reg_loss: 0.1706 N_Y: 407035 N_S: 1754772 N: 407035 N_HV: 171447 Val: 0.7239 Test: 0.6429\n",
      "Epoch: 254, Loss: 0.1697 tsm_loss: 3.7791 reg_loss: 0.1697 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 169944 Val: 0.7576 Test: 0.6810\n",
      "Epoch: 255, Loss: 0.1977 tsm_loss: 3.6696 reg_loss: 0.1977 N_Y: 408029 N_S: 1754772 N: 408029 N_HV: 165316 Val: 0.7849 Test: 0.6564\n",
      "Epoch: 256, Loss: 0.2018 tsm_loss: 3.7062 reg_loss: 0.2018 N_Y: 405826 N_S: 1754772 N: 405826 N_HV: 168765 Val: 0.7357 Test: 0.6185\n",
      "Epoch: 257, Loss: 0.1470 tsm_loss: 3.6446 reg_loss: 0.1470 N_Y: 405439 N_S: 1754772 N: 405439 N_HV: 167369 Val: 0.7439 Test: 0.6592\n",
      "Epoch: 258, Loss: 0.1451 tsm_loss: 3.7163 reg_loss: 0.1451 N_Y: 406304 N_S: 1754772 N: 406304 N_HV: 167069 Val: 0.7352 Test: 0.6382\n",
      "Epoch: 259, Loss: 0.1748 tsm_loss: 3.8788 reg_loss: 0.1748 N_Y: 406878 N_S: 1754772 N: 406878 N_HV: 171690 Val: 0.7748 Test: 0.6588\n",
      "Epoch: 260, Loss: 0.2363 tsm_loss: 3.8136 reg_loss: 0.2363 N_Y: 406689 N_S: 1754772 N: 406689 N_HV: 166828 Val: 0.7761 Test: 0.7044\n",
      "Epoch: 261, Loss: 0.1764 tsm_loss: 3.9593 reg_loss: 0.1764 N_Y: 406121 N_S: 1754772 N: 406121 N_HV: 166740 Val: 0.7366 Test: 0.6376\n",
      "Epoch: 262, Loss: 0.1486 tsm_loss: 4.0192 reg_loss: 0.1486 N_Y: 407349 N_S: 1754772 N: 407349 N_HV: 171810 Val: 0.7434 Test: 0.6322\n",
      "Epoch: 263, Loss: 0.1288 tsm_loss: 3.9727 reg_loss: 0.1288 N_Y: 406115 N_S: 1754772 N: 406115 N_HV: 170808 Val: 0.7509 Test: 0.6326\n",
      "Epoch: 264, Loss: 0.1438 tsm_loss: 4.0397 reg_loss: 0.1438 N_Y: 407381 N_S: 1754772 N: 407381 N_HV: 173678 Val: 0.7523 Test: 0.6387\n",
      "Epoch: 265, Loss: 0.1563 tsm_loss: 3.9242 reg_loss: 0.1563 N_Y: 404742 N_S: 1754772 N: 404742 N_HV: 170421 Val: 0.7403 Test: 0.6416\n",
      "Epoch: 266, Loss: 0.1810 tsm_loss: 3.8131 reg_loss: 0.1810 N_Y: 406645 N_S: 1754772 N: 406645 N_HV: 170157 Val: 0.7591 Test: 0.6424\n",
      "Epoch: 267, Loss: 0.1619 tsm_loss: 3.8780 reg_loss: 0.1619 N_Y: 405541 N_S: 1754772 N: 405541 N_HV: 171257 Val: 0.7680 Test: 0.6405\n",
      "Epoch: 268, Loss: 0.1498 tsm_loss: 3.8326 reg_loss: 0.1498 N_Y: 406361 N_S: 1754772 N: 406361 N_HV: 167299 Val: 0.7609 Test: 0.6695\n",
      "Epoch: 269, Loss: 0.1915 tsm_loss: 3.9327 reg_loss: 0.1915 N_Y: 405062 N_S: 1754772 N: 405062 N_HV: 170697 Val: 0.7731 Test: 0.6781\n",
      "Epoch: 270, Loss: 0.2232 tsm_loss: 3.7364 reg_loss: 0.2232 N_Y: 405297 N_S: 1754772 N: 405297 N_HV: 170517 Val: 0.7520 Test: 0.6390\n",
      "Epoch: 271, Loss: 0.1691 tsm_loss: 4.0191 reg_loss: 0.1691 N_Y: 407073 N_S: 1754772 N: 407073 N_HV: 171114 Val: 0.8208 Test: 0.6926\n",
      "Epoch: 272, Loss: 0.1916 tsm_loss: 3.9846 reg_loss: 0.1916 N_Y: 406788 N_S: 1754772 N: 406788 N_HV: 168821 Val: 0.7452 Test: 0.6301\n",
      "Epoch: 273, Loss: 0.1672 tsm_loss: 3.9988 reg_loss: 0.1672 N_Y: 403583 N_S: 1754772 N: 403583 N_HV: 171157 Val: 0.7573 Test: 0.6631\n",
      "Epoch: 274, Loss: 0.2036 tsm_loss: 3.8276 reg_loss: 0.2036 N_Y: 406181 N_S: 1754772 N: 406181 N_HV: 169498 Val: 0.7509 Test: 0.6440\n",
      "Epoch: 275, Loss: 0.1929 tsm_loss: 3.8790 reg_loss: 0.1929 N_Y: 404184 N_S: 1754772 N: 404184 N_HV: 170811 Val: 0.7437 Test: 0.6369\n",
      "Epoch: 276, Loss: 0.1670 tsm_loss: 3.7877 reg_loss: 0.1670 N_Y: 405246 N_S: 1754772 N: 405246 N_HV: 168501 Val: 0.7534 Test: 0.6650\n",
      "Epoch: 277, Loss: 0.1949 tsm_loss: 3.6937 reg_loss: 0.1949 N_Y: 406328 N_S: 1754772 N: 406328 N_HV: 168210 Val: 0.7329 Test: 0.6213\n",
      "Epoch: 278, Loss: 0.1598 tsm_loss: 3.6574 reg_loss: 0.1598 N_Y: 406011 N_S: 1754772 N: 406011 N_HV: 168601 Val: 0.7390 Test: 0.6390\n",
      "Epoch: 279, Loss: 0.1473 tsm_loss: 3.8418 reg_loss: 0.1473 N_Y: 402922 N_S: 1754772 N: 402922 N_HV: 168411 Val: 0.7539 Test: 0.6583\n",
      "Epoch: 280, Loss: 0.1363 tsm_loss: 3.9100 reg_loss: 0.1363 N_Y: 405462 N_S: 1754772 N: 405462 N_HV: 168982 Val: 0.7366 Test: 0.6385\n",
      "Epoch: 281, Loss: 0.1522 tsm_loss: 3.9005 reg_loss: 0.1522 N_Y: 405230 N_S: 1754772 N: 405230 N_HV: 169449 Val: 0.7379 Test: 0.6235\n",
      "Epoch: 282, Loss: 0.1441 tsm_loss: 3.8150 reg_loss: 0.1441 N_Y: 406514 N_S: 1754772 N: 406514 N_HV: 170105 Val: 0.7394 Test: 0.6413\n",
      "Epoch: 283, Loss: 0.1444 tsm_loss: 3.8459 reg_loss: 0.1444 N_Y: 406814 N_S: 1754772 N: 406814 N_HV: 171315 Val: 0.7651 Test: 0.6368\n",
      "Epoch: 284, Loss: 0.1662 tsm_loss: 3.8725 reg_loss: 0.1662 N_Y: 407713 N_S: 1754772 N: 407713 N_HV: 171771 Val: 0.7223 Test: 0.6366\n",
      "Epoch: 285, Loss: 0.1519 tsm_loss: 3.8346 reg_loss: 0.1519 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 172607 Val: 0.7489 Test: 0.6595\n",
      "Epoch: 286, Loss: 0.1713 tsm_loss: 3.9779 reg_loss: 0.1713 N_Y: 405746 N_S: 1754772 N: 405746 N_HV: 172159 Val: 0.7379 Test: 0.6395\n",
      "Epoch: 287, Loss: 0.1561 tsm_loss: 3.7991 reg_loss: 0.1561 N_Y: 404873 N_S: 1754772 N: 404873 N_HV: 170212 Val: 0.7244 Test: 0.6312\n",
      "Epoch: 288, Loss: 0.1547 tsm_loss: 3.7329 reg_loss: 0.1547 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 170405 Val: 0.7244 Test: 0.6265\n",
      "Epoch: 289, Loss: 0.1450 tsm_loss: 3.8392 reg_loss: 0.1450 N_Y: 404390 N_S: 1754772 N: 404390 N_HV: 170871 Val: 0.7588 Test: 0.6395\n",
      "Epoch: 290, Loss: 0.1511 tsm_loss: 3.9020 reg_loss: 0.1511 N_Y: 405676 N_S: 1754772 N: 405676 N_HV: 170667 Val: 0.7287 Test: 0.6241\n",
      "Epoch: 291, Loss: 0.1471 tsm_loss: 3.7689 reg_loss: 0.1471 N_Y: 407135 N_S: 1754772 N: 407135 N_HV: 171858 Val: 0.7428 Test: 0.6634\n",
      "Epoch: 292, Loss: 0.1663 tsm_loss: 3.7800 reg_loss: 0.1663 N_Y: 405760 N_S: 1754772 N: 405760 N_HV: 172956 Val: 0.7298 Test: 0.6409\n",
      "Epoch: 293, Loss: 0.1526 tsm_loss: 3.8298 reg_loss: 0.1526 N_Y: 405159 N_S: 1754772 N: 405159 N_HV: 173494 Val: 0.7288 Test: 0.6371\n",
      "Epoch: 294, Loss: 0.1259 tsm_loss: 3.7268 reg_loss: 0.1259 N_Y: 405869 N_S: 1754772 N: 405869 N_HV: 172509 Val: 0.7401 Test: 0.6396\n",
      "Epoch: 295, Loss: 0.1394 tsm_loss: 3.6868 reg_loss: 0.1394 N_Y: 406339 N_S: 1754772 N: 406339 N_HV: 171507 Val: 0.7685 Test: 0.6547\n",
      "Epoch: 296, Loss: 0.1529 tsm_loss: 3.7285 reg_loss: 0.1529 N_Y: 406727 N_S: 1754772 N: 406727 N_HV: 174336 Val: 0.7486 Test: 0.6205\n",
      "Epoch: 297, Loss: 0.1258 tsm_loss: 3.8140 reg_loss: 0.1258 N_Y: 406440 N_S: 1754772 N: 406440 N_HV: 172360 Val: 0.7450 Test: 0.6323\n",
      "Epoch: 298, Loss: 0.1327 tsm_loss: 3.6501 reg_loss: 0.1327 N_Y: 406206 N_S: 1754772 N: 406206 N_HV: 172273 Val: 0.7357 Test: 0.6338\n",
      "Epoch: 299, Loss: 0.1513 tsm_loss: 3.6594 reg_loss: 0.1513 N_Y: 405469 N_S: 1754772 N: 405469 N_HV: 174420 Val: 0.7595 Test: 0.6321\n",
      "Epoch: 300, Loss: 0.1709 tsm_loss: 3.6420 reg_loss: 0.1709 N_Y: 404108 N_S: 1754772 N: 404108 N_HV: 169923 Val: 0.7358 Test: 0.6495\n",
      "Epoch: 301, Loss: 0.1714 tsm_loss: 3.8795 reg_loss: 0.1714 N_Y: 404449 N_S: 1754772 N: 404449 N_HV: 172879 Val: 0.7662 Test: 0.6477\n",
      "Epoch: 302, Loss: 0.1658 tsm_loss: 3.6520 reg_loss: 0.1658 N_Y: 406549 N_S: 1754772 N: 406549 N_HV: 172177 Val: 0.7335 Test: 0.6166\n",
      "Epoch: 303, Loss: 0.1386 tsm_loss: 3.6881 reg_loss: 0.1386 N_Y: 404805 N_S: 1754772 N: 404805 N_HV: 174290 Val: 0.7523 Test: 0.6256\n",
      "Epoch: 304, Loss: 0.1337 tsm_loss: 3.7211 reg_loss: 0.1337 N_Y: 405193 N_S: 1754772 N: 405193 N_HV: 168230 Val: 0.7564 Test: 0.6484\n",
      "Epoch: 305, Loss: 0.1310 tsm_loss: 3.6853 reg_loss: 0.1310 N_Y: 405731 N_S: 1754772 N: 405731 N_HV: 172765 Val: 0.7420 Test: 0.6460\n",
      "Epoch: 306, Loss: 0.1506 tsm_loss: 3.6773 reg_loss: 0.1506 N_Y: 406022 N_S: 1754772 N: 406022 N_HV: 169424 Val: 0.7477 Test: 0.6248\n",
      "Epoch: 307, Loss: 0.1785 tsm_loss: 3.7722 reg_loss: 0.1785 N_Y: 405041 N_S: 1754772 N: 405041 N_HV: 171800 Val: 0.7402 Test: 0.6383\n",
      "Epoch: 308, Loss: 0.1323 tsm_loss: 3.6972 reg_loss: 0.1323 N_Y: 405739 N_S: 1754772 N: 405739 N_HV: 171068 Val: 0.7498 Test: 0.6275\n",
      "Epoch: 309, Loss: 0.1220 tsm_loss: 3.6711 reg_loss: 0.1220 N_Y: 406710 N_S: 1754772 N: 406710 N_HV: 173814 Val: 0.7580 Test: 0.6321\n",
      "Epoch: 310, Loss: 0.1392 tsm_loss: 3.6614 reg_loss: 0.1392 N_Y: 406561 N_S: 1754772 N: 406561 N_HV: 171356 Val: 0.7727 Test: 0.6501\n",
      "Epoch: 311, Loss: 0.1519 tsm_loss: 3.6853 reg_loss: 0.1519 N_Y: 406066 N_S: 1754772 N: 406066 N_HV: 170337 Val: 0.7357 Test: 0.6353\n",
      "Epoch: 312, Loss: 0.1643 tsm_loss: 3.7934 reg_loss: 0.1643 N_Y: 405356 N_S: 1754772 N: 405356 N_HV: 172547 Val: 0.7309 Test: 0.6425\n",
      "Epoch: 313, Loss: 0.1709 tsm_loss: 3.7117 reg_loss: 0.1709 N_Y: 407217 N_S: 1754772 N: 407217 N_HV: 170022 Val: 0.7511 Test: 0.6477\n",
      "Epoch: 314, Loss: 0.1379 tsm_loss: 3.8116 reg_loss: 0.1379 N_Y: 405885 N_S: 1754772 N: 405885 N_HV: 174507 Val: 0.8316 Test: 0.6862\n",
      "Epoch: 315, Loss: 0.1809 tsm_loss: 3.8911 reg_loss: 0.1809 N_Y: 406199 N_S: 1754772 N: 406199 N_HV: 170398 Val: 0.7785 Test: 0.7064\n",
      "Epoch: 316, Loss: 0.2002 tsm_loss: 3.7801 reg_loss: 0.2002 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 173100 Val: 0.8273 Test: 0.6988\n",
      "Epoch: 317, Loss: 0.2045 tsm_loss: 3.7383 reg_loss: 0.2045 N_Y: 406856 N_S: 1754772 N: 406856 N_HV: 167312 Val: 0.7490 Test: 0.6664\n",
      "Epoch: 318, Loss: 0.1976 tsm_loss: 3.9899 reg_loss: 0.1976 N_Y: 406986 N_S: 1754772 N: 406986 N_HV: 173867 Val: 0.8183 Test: 0.7642\n",
      "Epoch: 319, Loss: 0.1877 tsm_loss: 4.0864 reg_loss: 0.1877 N_Y: 405159 N_S: 1754772 N: 405159 N_HV: 172163 Val: 0.7634 Test: 0.6527\n",
      "Epoch: 320, Loss: 0.1797 tsm_loss: 3.8794 reg_loss: 0.1797 N_Y: 407001 N_S: 1754772 N: 407001 N_HV: 172889 Val: 0.7570 Test: 0.6425\n",
      "Epoch: 321, Loss: 0.1580 tsm_loss: 3.8804 reg_loss: 0.1580 N_Y: 406065 N_S: 1754772 N: 406065 N_HV: 171276 Val: 0.7486 Test: 0.6452\n",
      "Epoch: 322, Loss: 0.1552 tsm_loss: 3.8290 reg_loss: 0.1552 N_Y: 405940 N_S: 1754772 N: 405940 N_HV: 169711 Val: 0.7531 Test: 0.6353\n",
      "Epoch: 323, Loss: 0.1537 tsm_loss: 3.7394 reg_loss: 0.1537 N_Y: 404848 N_S: 1754772 N: 404848 N_HV: 169196 Val: 0.7795 Test: 0.6464\n",
      "Epoch: 324, Loss: 0.2317 tsm_loss: 3.8545 reg_loss: 0.2317 N_Y: 406716 N_S: 1754772 N: 406716 N_HV: 175486 Val: 0.8018 Test: 0.7374\n",
      "Epoch: 325, Loss: 0.1518 tsm_loss: 3.7567 reg_loss: 0.1518 N_Y: 404574 N_S: 1754772 N: 404574 N_HV: 171150 Val: 0.7546 Test: 0.6301\n",
      "Epoch: 326, Loss: 0.1652 tsm_loss: 3.7679 reg_loss: 0.1652 N_Y: 406556 N_S: 1754772 N: 406556 N_HV: 171391 Val: 0.7542 Test: 0.6559\n",
      "Epoch: 327, Loss: 0.1300 tsm_loss: 3.8797 reg_loss: 0.1300 N_Y: 405558 N_S: 1754772 N: 405558 N_HV: 171835 Val: 0.7343 Test: 0.6650\n",
      "Epoch: 328, Loss: 0.1767 tsm_loss: 3.8967 reg_loss: 0.1767 N_Y: 406371 N_S: 1754772 N: 406371 N_HV: 174465 Val: 0.7444 Test: 0.6418\n",
      "Epoch: 329, Loss: 0.1624 tsm_loss: 3.9357 reg_loss: 0.1624 N_Y: 406013 N_S: 1754772 N: 406013 N_HV: 173878 Val: 0.7580 Test: 0.6641\n",
      "Epoch: 330, Loss: 0.1305 tsm_loss: 3.8056 reg_loss: 0.1305 N_Y: 405517 N_S: 1754772 N: 405517 N_HV: 170860 Val: 0.7457 Test: 0.6416\n",
      "Epoch: 331, Loss: 0.1791 tsm_loss: 3.7420 reg_loss: 0.1791 N_Y: 405623 N_S: 1754772 N: 405623 N_HV: 169957 Val: 0.7430 Test: 0.6407\n",
      "Epoch: 332, Loss: 0.1398 tsm_loss: 3.7217 reg_loss: 0.1398 N_Y: 404253 N_S: 1754772 N: 404253 N_HV: 167777 Val: 0.7551 Test: 0.6713\n",
      "Epoch: 333, Loss: 0.1493 tsm_loss: 3.8003 reg_loss: 0.1493 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 174909 Val: 0.7415 Test: 0.6331\n",
      "Epoch: 334, Loss: 0.1267 tsm_loss: 3.7754 reg_loss: 0.1267 N_Y: 407152 N_S: 1754772 N: 407152 N_HV: 170859 Val: 0.7534 Test: 0.6272\n",
      "Epoch: 335, Loss: 0.1523 tsm_loss: 3.8676 reg_loss: 0.1523 N_Y: 406655 N_S: 1754772 N: 406655 N_HV: 173823 Val: 0.7529 Test: 0.6416\n",
      "Epoch: 336, Loss: 0.1520 tsm_loss: 3.8650 reg_loss: 0.1520 N_Y: 405895 N_S: 1754772 N: 405895 N_HV: 173012 Val: 0.7646 Test: 0.6651\n",
      "Epoch: 337, Loss: 0.1366 tsm_loss: 3.8272 reg_loss: 0.1366 N_Y: 405636 N_S: 1754772 N: 405636 N_HV: 171527 Val: 0.7510 Test: 0.6333\n",
      "Epoch: 338, Loss: 0.1375 tsm_loss: 3.8490 reg_loss: 0.1375 N_Y: 406117 N_S: 1754772 N: 406117 N_HV: 174807 Val: 0.7409 Test: 0.6356\n",
      "Epoch: 339, Loss: 0.1177 tsm_loss: 3.6993 reg_loss: 0.1177 N_Y: 405840 N_S: 1754772 N: 405840 N_HV: 170298 Val: 0.7358 Test: 0.6436\n",
      "Epoch: 340, Loss: 0.1311 tsm_loss: 3.8810 reg_loss: 0.1311 N_Y: 407036 N_S: 1754772 N: 407036 N_HV: 174905 Val: 0.7380 Test: 0.6448\n",
      "Epoch: 341, Loss: 0.1376 tsm_loss: 3.7335 reg_loss: 0.1376 N_Y: 406529 N_S: 1754772 N: 406529 N_HV: 170607 Val: 0.7522 Test: 0.6629\n",
      "Epoch: 342, Loss: 0.1604 tsm_loss: 3.8940 reg_loss: 0.1604 N_Y: 406923 N_S: 1754772 N: 406923 N_HV: 173372 Val: 0.7342 Test: 0.6250\n",
      "Epoch: 343, Loss: 0.1444 tsm_loss: 3.8958 reg_loss: 0.1444 N_Y: 406316 N_S: 1754772 N: 406316 N_HV: 172795 Val: 0.7545 Test: 0.6439\n",
      "Epoch: 344, Loss: 0.1294 tsm_loss: 3.8599 reg_loss: 0.1294 N_Y: 407063 N_S: 1754772 N: 407063 N_HV: 172383 Val: 0.7581 Test: 0.6336\n",
      "Epoch: 345, Loss: 0.1242 tsm_loss: 3.7092 reg_loss: 0.1242 N_Y: 406412 N_S: 1754772 N: 406412 N_HV: 168612 Val: 0.7309 Test: 0.6356\n",
      "Epoch: 346, Loss: 0.1120 tsm_loss: 3.7541 reg_loss: 0.1120 N_Y: 406204 N_S: 1754772 N: 406204 N_HV: 171785 Val: 0.7354 Test: 0.6378\n",
      "Epoch: 347, Loss: 0.1286 tsm_loss: 3.7863 reg_loss: 0.1286 N_Y: 406305 N_S: 1754772 N: 406305 N_HV: 171160 Val: 0.7410 Test: 0.6267\n",
      "Epoch: 348, Loss: 0.1350 tsm_loss: 3.7585 reg_loss: 0.1350 N_Y: 405982 N_S: 1754772 N: 405982 N_HV: 173406 Val: 0.7448 Test: 0.6394\n",
      "Epoch: 349, Loss: 0.1739 tsm_loss: 3.6954 reg_loss: 0.1739 N_Y: 407161 N_S: 1754772 N: 407161 N_HV: 169868 Val: 0.7832 Test: 0.7241\n",
      "Epoch: 350, Loss: 0.1560 tsm_loss: 3.7422 reg_loss: 0.1560 N_Y: 406451 N_S: 1754772 N: 406451 N_HV: 173241 Val: 0.7672 Test: 0.6821\n",
      "Epoch: 351, Loss: 0.1534 tsm_loss: 3.7565 reg_loss: 0.1534 N_Y: 407895 N_S: 1754772 N: 407895 N_HV: 175272 Val: 0.7495 Test: 0.6361\n",
      "Epoch: 352, Loss: 0.1631 tsm_loss: 3.6751 reg_loss: 0.1631 N_Y: 406297 N_S: 1754772 N: 406297 N_HV: 172567 Val: 0.7230 Test: 0.6396\n",
      "Epoch: 353, Loss: 0.1158 tsm_loss: 3.7403 reg_loss: 0.1158 N_Y: 405620 N_S: 1754772 N: 405620 N_HV: 174161 Val: 0.7301 Test: 0.6445\n",
      "Epoch: 354, Loss: 0.1402 tsm_loss: 3.8868 reg_loss: 0.1402 N_Y: 405196 N_S: 1754772 N: 405196 N_HV: 176773 Val: 0.7806 Test: 0.7190\n",
      "Epoch: 355, Loss: 0.2037 tsm_loss: 3.8086 reg_loss: 0.2037 N_Y: 404082 N_S: 1754772 N: 404082 N_HV: 172645 Val: 0.7936 Test: 0.6760\n",
      "Epoch: 356, Loss: 0.1497 tsm_loss: 3.8851 reg_loss: 0.1497 N_Y: 405656 N_S: 1754772 N: 405656 N_HV: 173472 Val: 0.7378 Test: 0.6312\n",
      "Epoch: 357, Loss: 0.1650 tsm_loss: 3.8336 reg_loss: 0.1650 N_Y: 406342 N_S: 1754772 N: 406342 N_HV: 172078 Val: 0.7404 Test: 0.6360\n",
      "Epoch: 358, Loss: 0.1725 tsm_loss: 3.8276 reg_loss: 0.1725 N_Y: 405049 N_S: 1754772 N: 405049 N_HV: 172922 Val: 0.7496 Test: 0.6373\n",
      "Epoch: 359, Loss: 0.1514 tsm_loss: 3.8602 reg_loss: 0.1514 N_Y: 404673 N_S: 1754772 N: 404673 N_HV: 172463 Val: 0.7310 Test: 0.6395\n",
      "Epoch: 360, Loss: 0.1164 tsm_loss: 3.8947 reg_loss: 0.1164 N_Y: 406973 N_S: 1754772 N: 406973 N_HV: 172771 Val: 0.7473 Test: 0.6334\n",
      "Epoch: 361, Loss: 0.1253 tsm_loss: 3.9184 reg_loss: 0.1253 N_Y: 406607 N_S: 1754772 N: 406607 N_HV: 174474 Val: 0.7645 Test: 0.6431\n",
      "Epoch: 362, Loss: 0.1546 tsm_loss: 3.8094 reg_loss: 0.1546 N_Y: 406359 N_S: 1754772 N: 406359 N_HV: 170324 Val: 0.7566 Test: 0.6691\n",
      "Epoch: 363, Loss: 0.1462 tsm_loss: 3.8353 reg_loss: 0.1462 N_Y: 406123 N_S: 1754772 N: 406123 N_HV: 174001 Val: 0.7760 Test: 0.6382\n",
      "Epoch: 364, Loss: 0.1788 tsm_loss: 3.5683 reg_loss: 0.1788 N_Y: 407195 N_S: 1754772 N: 407195 N_HV: 167513 Val: 0.7764 Test: 0.6562\n",
      "Epoch: 365, Loss: 0.1630 tsm_loss: 3.8038 reg_loss: 0.1630 N_Y: 405720 N_S: 1754772 N: 405720 N_HV: 172763 Val: 0.7657 Test: 0.6877\n",
      "Epoch: 366, Loss: 0.1596 tsm_loss: 3.8739 reg_loss: 0.1596 N_Y: 406046 N_S: 1754772 N: 406046 N_HV: 173151 Val: 0.7521 Test: 0.6554\n",
      "Epoch: 367, Loss: 0.1399 tsm_loss: 3.9156 reg_loss: 0.1399 N_Y: 406423 N_S: 1754772 N: 406423 N_HV: 173212 Val: 0.7981 Test: 0.6724\n",
      "Epoch: 368, Loss: 0.1581 tsm_loss: 3.8018 reg_loss: 0.1581 N_Y: 405552 N_S: 1754772 N: 405552 N_HV: 172089 Val: 0.7357 Test: 0.6447\n",
      "Epoch: 369, Loss: 0.1876 tsm_loss: 3.9034 reg_loss: 0.1876 N_Y: 405840 N_S: 1754772 N: 405840 N_HV: 174628 Val: 0.7296 Test: 0.6234\n",
      "Epoch: 370, Loss: 0.1778 tsm_loss: 3.9482 reg_loss: 0.1778 N_Y: 405772 N_S: 1754772 N: 405772 N_HV: 173635 Val: 0.7293 Test: 0.6294\n",
      "Epoch: 371, Loss: 0.1544 tsm_loss: 3.9340 reg_loss: 0.1544 N_Y: 405417 N_S: 1754772 N: 405417 N_HV: 173702 Val: 0.7439 Test: 0.6337\n",
      "Epoch: 372, Loss: 0.1494 tsm_loss: 3.8077 reg_loss: 0.1494 N_Y: 406648 N_S: 1754772 N: 406648 N_HV: 173162 Val: 0.7343 Test: 0.6339\n",
      "Epoch: 373, Loss: 0.1312 tsm_loss: 4.1273 reg_loss: 0.1312 N_Y: 407996 N_S: 1754772 N: 407996 N_HV: 175543 Val: 0.7280 Test: 0.6174\n",
      "Epoch: 374, Loss: 0.1315 tsm_loss: 4.1676 reg_loss: 0.1315 N_Y: 405927 N_S: 1754772 N: 405927 N_HV: 178345 Val: 0.7344 Test: 0.6148\n",
      "Epoch: 375, Loss: 0.1384 tsm_loss: 4.1017 reg_loss: 0.1384 N_Y: 405277 N_S: 1754772 N: 405277 N_HV: 177061 Val: 0.7469 Test: 0.6712\n",
      "Epoch: 376, Loss: 0.1223 tsm_loss: 4.1137 reg_loss: 0.1223 N_Y: 405724 N_S: 1754772 N: 405724 N_HV: 178883 Val: 0.7405 Test: 0.6427\n",
      "Epoch: 377, Loss: 0.1106 tsm_loss: 4.1175 reg_loss: 0.1106 N_Y: 406675 N_S: 1754772 N: 406675 N_HV: 176804 Val: 0.7641 Test: 0.6399\n",
      "Epoch: 378, Loss: 0.1443 tsm_loss: 3.9832 reg_loss: 0.1443 N_Y: 406793 N_S: 1754772 N: 406793 N_HV: 176250 Val: 0.7880 Test: 0.6443\n",
      "Epoch: 379, Loss: 0.1165 tsm_loss: 4.0454 reg_loss: 0.1165 N_Y: 405124 N_S: 1754772 N: 405124 N_HV: 177225 Val: 0.7708 Test: 0.6391\n",
      "Epoch: 380, Loss: 0.1137 tsm_loss: 3.9964 reg_loss: 0.1137 N_Y: 405105 N_S: 1754772 N: 405105 N_HV: 171086 Val: 0.7385 Test: 0.6466\n",
      "Epoch: 381, Loss: 0.1321 tsm_loss: 4.0503 reg_loss: 0.1321 N_Y: 406296 N_S: 1754772 N: 406296 N_HV: 176377 Val: 0.7351 Test: 0.6369\n",
      "Epoch: 382, Loss: 0.1189 tsm_loss: 3.9759 reg_loss: 0.1189 N_Y: 405211 N_S: 1754772 N: 405211 N_HV: 173998 Val: 0.7730 Test: 0.7072\n",
      "Epoch: 383, Loss: 0.1410 tsm_loss: 3.8151 reg_loss: 0.1410 N_Y: 404147 N_S: 1754772 N: 404147 N_HV: 170966 Val: 0.7414 Test: 0.6351\n",
      "Epoch: 384, Loss: 0.1286 tsm_loss: 3.8825 reg_loss: 0.1286 N_Y: 406786 N_S: 1754772 N: 406786 N_HV: 175874 Val: 0.7524 Test: 0.6486\n",
      "Epoch: 385, Loss: 0.1257 tsm_loss: 3.8400 reg_loss: 0.1257 N_Y: 407053 N_S: 1754772 N: 407053 N_HV: 175472 Val: 0.7488 Test: 0.6301\n",
      "Epoch: 386, Loss: 0.1348 tsm_loss: 3.7647 reg_loss: 0.1348 N_Y: 405573 N_S: 1754772 N: 405573 N_HV: 170800 Val: 0.7482 Test: 0.6506\n",
      "Epoch: 387, Loss: 0.1142 tsm_loss: 3.9478 reg_loss: 0.1142 N_Y: 406838 N_S: 1754772 N: 406838 N_HV: 176241 Val: 0.7613 Test: 0.6804\n",
      "Epoch: 388, Loss: 0.1383 tsm_loss: 3.8392 reg_loss: 0.1383 N_Y: 407879 N_S: 1754772 N: 407879 N_HV: 173824 Val: 0.7414 Test: 0.6253\n",
      "Epoch: 389, Loss: 0.1324 tsm_loss: 4.0086 reg_loss: 0.1324 N_Y: 404104 N_S: 1754772 N: 404104 N_HV: 174607 Val: 0.7422 Test: 0.6400\n",
      "Epoch: 390, Loss: 0.1597 tsm_loss: 3.9451 reg_loss: 0.1597 N_Y: 405536 N_S: 1754772 N: 405536 N_HV: 174194 Val: 0.7437 Test: 0.6195\n",
      "Epoch: 391, Loss: 0.1037 tsm_loss: 3.7875 reg_loss: 0.1037 N_Y: 406087 N_S: 1754772 N: 406087 N_HV: 172066 Val: 0.7423 Test: 0.6265\n",
      "Epoch: 392, Loss: 0.1173 tsm_loss: 3.8552 reg_loss: 0.1173 N_Y: 406017 N_S: 1754772 N: 406017 N_HV: 174843 Val: 0.7416 Test: 0.6210\n",
      "Epoch: 393, Loss: 0.1222 tsm_loss: 3.8859 reg_loss: 0.1222 N_Y: 402317 N_S: 1754772 N: 402317 N_HV: 172196 Val: 0.7403 Test: 0.6287\n",
      "Epoch: 394, Loss: 0.1078 tsm_loss: 3.9089 reg_loss: 0.1078 N_Y: 406700 N_S: 1754772 N: 406700 N_HV: 176702 Val: 0.7504 Test: 0.6520\n",
      "Epoch: 395, Loss: 0.1255 tsm_loss: 3.8587 reg_loss: 0.1255 N_Y: 405928 N_S: 1754772 N: 405928 N_HV: 172675 Val: 0.7415 Test: 0.6395\n",
      "Epoch: 396, Loss: 0.1179 tsm_loss: 4.0174 reg_loss: 0.1179 N_Y: 406150 N_S: 1754772 N: 406150 N_HV: 175207 Val: 0.7552 Test: 0.6268\n",
      "Epoch: 397, Loss: 0.1274 tsm_loss: 3.9788 reg_loss: 0.1274 N_Y: 406108 N_S: 1754772 N: 406108 N_HV: 173921 Val: 0.7385 Test: 0.6260\n",
      "Epoch: 398, Loss: 0.1366 tsm_loss: 3.8504 reg_loss: 0.1366 N_Y: 406194 N_S: 1754772 N: 406194 N_HV: 173255 Val: 0.7550 Test: 0.6300\n",
      "Epoch: 399, Loss: 0.1341 tsm_loss: 3.9770 reg_loss: 0.1341 N_Y: 405980 N_S: 1754772 N: 405980 N_HV: 176013 Val: 0.7932 Test: 0.7195\n",
      "Epoch: 400, Loss: 0.1447 tsm_loss: 3.9833 reg_loss: 0.1447 N_Y: 406868 N_S: 1754772 N: 406868 N_HV: 173560 Val: 0.7327 Test: 0.6222\n",
      "Epoch: 401, Loss: 0.1332 tsm_loss: 3.9717 reg_loss: 0.1332 N_Y: 406012 N_S: 1754772 N: 406012 N_HV: 174294 Val: 0.7816 Test: 0.6505\n",
      "Epoch: 402, Loss: 0.1351 tsm_loss: 4.0617 reg_loss: 0.1351 N_Y: 405777 N_S: 1754772 N: 405777 N_HV: 177464 Val: 0.7503 Test: 0.6552\n",
      "Epoch: 403, Loss: 0.1263 tsm_loss: 3.9543 reg_loss: 0.1263 N_Y: 405863 N_S: 1754772 N: 405863 N_HV: 174159 Val: 0.7446 Test: 0.6350\n",
      "Epoch: 404, Loss: 0.1233 tsm_loss: 3.8924 reg_loss: 0.1233 N_Y: 406567 N_S: 1754772 N: 406567 N_HV: 176045 Val: 0.7501 Test: 0.6586\n",
      "Epoch: 405, Loss: 0.1188 tsm_loss: 3.9826 reg_loss: 0.1188 N_Y: 404835 N_S: 1754772 N: 404835 N_HV: 175211 Val: 0.7485 Test: 0.6345\n",
      "Epoch: 406, Loss: 0.1177 tsm_loss: 3.8961 reg_loss: 0.1177 N_Y: 406385 N_S: 1754772 N: 406385 N_HV: 174477 Val: 0.7350 Test: 0.6533\n",
      "Epoch: 407, Loss: 0.1252 tsm_loss: 3.8778 reg_loss: 0.1252 N_Y: 405890 N_S: 1754772 N: 405890 N_HV: 175112 Val: 0.7421 Test: 0.6287\n",
      "Epoch: 408, Loss: 0.1273 tsm_loss: 3.8825 reg_loss: 0.1273 N_Y: 404739 N_S: 1754772 N: 404739 N_HV: 174928 Val: 0.7882 Test: 0.6513\n",
      "Epoch: 409, Loss: 0.1626 tsm_loss: 3.8627 reg_loss: 0.1626 N_Y: 407854 N_S: 1754772 N: 407854 N_HV: 174855 Val: 0.7424 Test: 0.6374\n",
      "Epoch: 410, Loss: 0.1797 tsm_loss: 3.9447 reg_loss: 0.1797 N_Y: 405691 N_S: 1754772 N: 405691 N_HV: 172576 Val: 0.7400 Test: 0.6277\n",
      "Epoch: 411, Loss: 0.1225 tsm_loss: 3.9756 reg_loss: 0.1225 N_Y: 406351 N_S: 1754772 N: 406351 N_HV: 176473 Val: 0.7417 Test: 0.6265\n",
      "Epoch: 412, Loss: 0.1348 tsm_loss: 3.9759 reg_loss: 0.1348 N_Y: 405283 N_S: 1754772 N: 405283 N_HV: 175317 Val: 0.7339 Test: 0.6637\n",
      "Epoch: 413, Loss: 0.1380 tsm_loss: 3.8660 reg_loss: 0.1380 N_Y: 405618 N_S: 1754772 N: 405618 N_HV: 175607 Val: 0.7439 Test: 0.6460\n",
      "Epoch: 414, Loss: 0.1283 tsm_loss: 3.8867 reg_loss: 0.1283 N_Y: 405763 N_S: 1754772 N: 405763 N_HV: 175093 Val: 0.7357 Test: 0.6375\n",
      "Epoch: 415, Loss: 0.1186 tsm_loss: 3.7580 reg_loss: 0.1186 N_Y: 404405 N_S: 1754772 N: 404405 N_HV: 172617 Val: 0.7570 Test: 0.6306\n",
      "Epoch: 416, Loss: 0.1159 tsm_loss: 3.8327 reg_loss: 0.1159 N_Y: 406339 N_S: 1754772 N: 406339 N_HV: 173557 Val: 0.7621 Test: 0.6199\n",
      "Epoch: 417, Loss: 0.1019 tsm_loss: 3.9375 reg_loss: 0.1019 N_Y: 407131 N_S: 1754772 N: 407131 N_HV: 175465 Val: 0.7803 Test: 0.6357\n",
      "Epoch: 418, Loss: 0.1318 tsm_loss: 3.8773 reg_loss: 0.1318 N_Y: 406599 N_S: 1754772 N: 406599 N_HV: 175541 Val: 0.7631 Test: 0.6588\n",
      "Epoch: 419, Loss: 0.1048 tsm_loss: 3.8862 reg_loss: 0.1048 N_Y: 405727 N_S: 1754772 N: 405727 N_HV: 174252 Val: 0.7411 Test: 0.6283\n",
      "Epoch: 420, Loss: 0.1212 tsm_loss: 4.0164 reg_loss: 0.1212 N_Y: 406040 N_S: 1754772 N: 406040 N_HV: 176089 Val: 0.7472 Test: 0.6411\n",
      "Epoch: 421, Loss: 0.1149 tsm_loss: 3.7067 reg_loss: 0.1149 N_Y: 406017 N_S: 1754772 N: 406017 N_HV: 172669 Val: 0.7537 Test: 0.6349\n",
      "Epoch: 422, Loss: 0.1436 tsm_loss: 3.8304 reg_loss: 0.1436 N_Y: 405988 N_S: 1754772 N: 405988 N_HV: 175944 Val: 0.7549 Test: 0.6222\n",
      "Epoch: 423, Loss: 0.1309 tsm_loss: 3.7344 reg_loss: 0.1309 N_Y: 404829 N_S: 1754772 N: 404829 N_HV: 172188 Val: 0.7448 Test: 0.6178\n",
      "Epoch: 424, Loss: 0.1153 tsm_loss: 3.8275 reg_loss: 0.1153 N_Y: 406745 N_S: 1754772 N: 406745 N_HV: 176170 Val: 0.7513 Test: 0.6391\n",
      "Epoch: 425, Loss: 0.1140 tsm_loss: 3.9446 reg_loss: 0.1140 N_Y: 404851 N_S: 1754772 N: 404851 N_HV: 173340 Val: 0.7471 Test: 0.6338\n",
      "Epoch: 426, Loss: 0.1317 tsm_loss: 3.7296 reg_loss: 0.1317 N_Y: 404844 N_S: 1754772 N: 404844 N_HV: 173708 Val: 0.7522 Test: 0.6258\n",
      "Epoch: 427, Loss: 0.1033 tsm_loss: 3.6811 reg_loss: 0.1033 N_Y: 407267 N_S: 1754772 N: 407267 N_HV: 173345 Val: 0.7559 Test: 0.6165\n",
      "Epoch: 428, Loss: 0.1592 tsm_loss: 3.7042 reg_loss: 0.1592 N_Y: 406832 N_S: 1754772 N: 406832 N_HV: 175926 Val: 0.7513 Test: 0.6387\n",
      "Epoch: 429, Loss: 0.1213 tsm_loss: 3.7292 reg_loss: 0.1213 N_Y: 405685 N_S: 1754772 N: 405685 N_HV: 173010 Val: 0.7465 Test: 0.6354\n",
      "Epoch: 430, Loss: 0.1157 tsm_loss: 3.8746 reg_loss: 0.1157 N_Y: 406369 N_S: 1754772 N: 406369 N_HV: 177961 Val: 0.7531 Test: 0.6346\n",
      "Epoch: 431, Loss: 0.1093 tsm_loss: 3.8465 reg_loss: 0.1093 N_Y: 405507 N_S: 1754772 N: 405507 N_HV: 175916 Val: 0.7781 Test: 0.6449\n",
      "Epoch: 432, Loss: 0.1070 tsm_loss: 3.7705 reg_loss: 0.1070 N_Y: 405227 N_S: 1754772 N: 405227 N_HV: 174855 Val: 0.7383 Test: 0.6308\n",
      "Epoch: 433, Loss: 0.1082 tsm_loss: 3.8379 reg_loss: 0.1082 N_Y: 406365 N_S: 1754772 N: 406365 N_HV: 176381 Val: 0.7552 Test: 0.6638\n",
      "Epoch: 434, Loss: 0.1087 tsm_loss: 3.9590 reg_loss: 0.1087 N_Y: 406852 N_S: 1754772 N: 406852 N_HV: 175924 Val: 0.7451 Test: 0.6230\n",
      "Epoch: 435, Loss: 0.1016 tsm_loss: 4.0334 reg_loss: 0.1016 N_Y: 406311 N_S: 1754772 N: 406311 N_HV: 176116 Val: 0.7320 Test: 0.6251\n",
      "Epoch: 436, Loss: 0.1316 tsm_loss: 4.0777 reg_loss: 0.1316 N_Y: 407414 N_S: 1754772 N: 407414 N_HV: 177803 Val: 0.7408 Test: 0.6322\n",
      "Epoch: 437, Loss: 0.1723 tsm_loss: 3.9770 reg_loss: 0.1723 N_Y: 406175 N_S: 1754772 N: 406175 N_HV: 174574 Val: 0.7437 Test: 0.6405\n",
      "Epoch: 438, Loss: 0.1150 tsm_loss: 3.9884 reg_loss: 0.1150 N_Y: 405294 N_S: 1754772 N: 405294 N_HV: 176987 Val: 0.7399 Test: 0.6430\n",
      "Epoch: 439, Loss: 0.1597 tsm_loss: 3.8767 reg_loss: 0.1597 N_Y: 406234 N_S: 1754772 N: 406234 N_HV: 175123 Val: 0.7455 Test: 0.6398\n",
      "Epoch: 440, Loss: 0.1141 tsm_loss: 3.9647 reg_loss: 0.1141 N_Y: 406603 N_S: 1754772 N: 406603 N_HV: 179161 Val: 0.7404 Test: 0.6310\n",
      "Epoch: 441, Loss: 0.1024 tsm_loss: 3.8637 reg_loss: 0.1024 N_Y: 407237 N_S: 1754772 N: 407237 N_HV: 177458 Val: 0.7488 Test: 0.6311\n",
      "Epoch: 442, Loss: 0.0896 tsm_loss: 3.8894 reg_loss: 0.0896 N_Y: 405782 N_S: 1754772 N: 405782 N_HV: 173038 Val: 0.7620 Test: 0.6625\n",
      "Epoch: 443, Loss: 0.1217 tsm_loss: 3.9754 reg_loss: 0.1217 N_Y: 406100 N_S: 1754772 N: 406100 N_HV: 175709 Val: 0.7529 Test: 0.6416\n",
      "Epoch: 444, Loss: 0.1166 tsm_loss: 3.9342 reg_loss: 0.1166 N_Y: 406517 N_S: 1754772 N: 406517 N_HV: 176670 Val: 0.7623 Test: 0.6271\n",
      "Epoch: 445, Loss: 0.1192 tsm_loss: 3.9063 reg_loss: 0.1192 N_Y: 405435 N_S: 1754772 N: 405435 N_HV: 176241 Val: 0.7346 Test: 0.6125\n",
      "Epoch: 446, Loss: 0.1254 tsm_loss: 3.9555 reg_loss: 0.1254 N_Y: 403214 N_S: 1754772 N: 403214 N_HV: 174458 Val: 0.7396 Test: 0.6482\n",
      "Epoch: 447, Loss: 0.1208 tsm_loss: 3.9004 reg_loss: 0.1208 N_Y: 405965 N_S: 1754772 N: 405965 N_HV: 175560 Val: 0.7491 Test: 0.6218\n",
      "Epoch: 448, Loss: 0.1355 tsm_loss: 3.8773 reg_loss: 0.1355 N_Y: 407486 N_S: 1754772 N: 407486 N_HV: 177708 Val: 0.7436 Test: 0.6296\n",
      "Epoch: 449, Loss: 0.1154 tsm_loss: 3.8219 reg_loss: 0.1154 N_Y: 405937 N_S: 1754772 N: 405937 N_HV: 174223 Val: 0.7303 Test: 0.6256\n",
      "Epoch: 450, Loss: 0.1272 tsm_loss: 3.8282 reg_loss: 0.1272 N_Y: 404943 N_S: 1754772 N: 404943 N_HV: 176459 Val: 0.7454 Test: 0.6298\n",
      "Epoch: 451, Loss: 0.1012 tsm_loss: 3.8481 reg_loss: 0.1012 N_Y: 406571 N_S: 1754772 N: 406571 N_HV: 176783 Val: 0.7543 Test: 0.6660\n",
      "Epoch: 452, Loss: 0.1067 tsm_loss: 3.9370 reg_loss: 0.1067 N_Y: 404846 N_S: 1754772 N: 404846 N_HV: 176830 Val: 0.7385 Test: 0.6191\n",
      "Epoch: 453, Loss: 0.0997 tsm_loss: 4.0454 reg_loss: 0.0997 N_Y: 406886 N_S: 1754772 N: 406886 N_HV: 178740 Val: 0.7474 Test: 0.6213\n",
      "Epoch: 454, Loss: 0.1252 tsm_loss: 3.9515 reg_loss: 0.1252 N_Y: 405700 N_S: 1754772 N: 405700 N_HV: 174248 Val: 0.7359 Test: 0.6203\n",
      "Epoch: 455, Loss: 0.1136 tsm_loss: 4.0508 reg_loss: 0.1136 N_Y: 405408 N_S: 1754772 N: 405408 N_HV: 177543 Val: 0.7392 Test: 0.6175\n",
      "Epoch: 456, Loss: 0.0961 tsm_loss: 4.0370 reg_loss: 0.0961 N_Y: 406468 N_S: 1754772 N: 406468 N_HV: 178581 Val: 0.7391 Test: 0.6235\n",
      "Epoch: 457, Loss: 0.0964 tsm_loss: 4.0391 reg_loss: 0.0964 N_Y: 405611 N_S: 1754772 N: 405611 N_HV: 177481 Val: 0.7412 Test: 0.6341\n",
      "Epoch: 458, Loss: 0.1092 tsm_loss: 4.0019 reg_loss: 0.1092 N_Y: 405630 N_S: 1754772 N: 405630 N_HV: 174776 Val: 0.7326 Test: 0.6281\n",
      "Epoch: 459, Loss: 0.1201 tsm_loss: 4.1204 reg_loss: 0.1201 N_Y: 406251 N_S: 1754772 N: 406251 N_HV: 178326 Val: 0.7443 Test: 0.6191\n",
      "Epoch: 460, Loss: 0.1177 tsm_loss: 4.0057 reg_loss: 0.1177 N_Y: 406969 N_S: 1754772 N: 406969 N_HV: 177452 Val: 0.7613 Test: 0.6337\n",
      "Epoch: 461, Loss: 0.1198 tsm_loss: 4.0130 reg_loss: 0.1198 N_Y: 407199 N_S: 1754772 N: 407199 N_HV: 178036 Val: 0.7408 Test: 0.6265\n",
      "Epoch: 462, Loss: 0.1523 tsm_loss: 4.0372 reg_loss: 0.1523 N_Y: 403692 N_S: 1754772 N: 403692 N_HV: 175619 Val: 0.7948 Test: 0.7187\n",
      "Epoch: 463, Loss: 0.1871 tsm_loss: 3.9716 reg_loss: 0.1871 N_Y: 405858 N_S: 1754772 N: 405858 N_HV: 176773 Val: 0.7670 Test: 0.6472\n",
      "Epoch: 464, Loss: 0.1856 tsm_loss: 3.9457 reg_loss: 0.1856 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 177142 Val: 0.7236 Test: 0.6223\n",
      "Epoch: 465, Loss: 0.1307 tsm_loss: 3.9001 reg_loss: 0.1307 N_Y: 406021 N_S: 1754772 N: 406021 N_HV: 175774 Val: 0.7225 Test: 0.6126\n",
      "Epoch: 466, Loss: 0.1129 tsm_loss: 4.1788 reg_loss: 0.1129 N_Y: 406628 N_S: 1754772 N: 406628 N_HV: 180751 Val: 0.7279 Test: 0.6192\n",
      "Epoch: 467, Loss: 0.1008 tsm_loss: 3.9590 reg_loss: 0.1008 N_Y: 405791 N_S: 1754772 N: 405791 N_HV: 175398 Val: 0.7226 Test: 0.6208\n",
      "Epoch: 468, Loss: 0.1201 tsm_loss: 3.8673 reg_loss: 0.1201 N_Y: 406619 N_S: 1754772 N: 406619 N_HV: 175704 Val: 0.7363 Test: 0.6217\n",
      "Epoch: 469, Loss: 0.1079 tsm_loss: 3.8514 reg_loss: 0.1079 N_Y: 406236 N_S: 1754772 N: 406236 N_HV: 177233 Val: 0.7913 Test: 0.6641\n",
      "Epoch: 470, Loss: 0.1518 tsm_loss: 3.9723 reg_loss: 0.1518 N_Y: 405856 N_S: 1754772 N: 405856 N_HV: 178858 Val: 0.7319 Test: 0.6344\n",
      "Epoch: 471, Loss: 0.1348 tsm_loss: 3.8108 reg_loss: 0.1348 N_Y: 403921 N_S: 1754772 N: 403921 N_HV: 171140 Val: 0.7386 Test: 0.6637\n",
      "Epoch: 472, Loss: 0.1445 tsm_loss: 3.8569 reg_loss: 0.1445 N_Y: 404591 N_S: 1754772 N: 404591 N_HV: 175442 Val: 0.8068 Test: 0.6800\n",
      "Epoch: 473, Loss: 0.1475 tsm_loss: 3.9542 reg_loss: 0.1475 N_Y: 406504 N_S: 1754772 N: 406504 N_HV: 176410 Val: 0.7890 Test: 0.7075\n",
      "Epoch: 474, Loss: 0.1468 tsm_loss: 3.8659 reg_loss: 0.1468 N_Y: 405277 N_S: 1754772 N: 405277 N_HV: 172815 Val: 0.7338 Test: 0.6242\n",
      "Epoch: 475, Loss: 0.1382 tsm_loss: 4.0056 reg_loss: 0.1382 N_Y: 406328 N_S: 1754772 N: 406328 N_HV: 176439 Val: 0.7317 Test: 0.6311\n",
      "Epoch: 476, Loss: 0.1376 tsm_loss: 3.9578 reg_loss: 0.1376 N_Y: 405162 N_S: 1754772 N: 405162 N_HV: 175797 Val: 0.7323 Test: 0.6259\n",
      "Epoch: 477, Loss: 0.1276 tsm_loss: 3.9470 reg_loss: 0.1276 N_Y: 406568 N_S: 1754772 N: 406568 N_HV: 175218 Val: 0.7300 Test: 0.6238\n",
      "Epoch: 478, Loss: 0.1178 tsm_loss: 3.9978 reg_loss: 0.1178 N_Y: 405976 N_S: 1754772 N: 405976 N_HV: 174462 Val: 0.7316 Test: 0.6131\n",
      "Epoch: 479, Loss: 0.1150 tsm_loss: 4.1324 reg_loss: 0.1150 N_Y: 405606 N_S: 1754772 N: 405606 N_HV: 178725 Val: 0.7461 Test: 0.6263\n",
      "Epoch: 480, Loss: 0.1033 tsm_loss: 3.9951 reg_loss: 0.1033 N_Y: 406155 N_S: 1754772 N: 406155 N_HV: 176944 Val: 0.7315 Test: 0.6282\n",
      "Epoch: 481, Loss: 0.1392 tsm_loss: 3.9540 reg_loss: 0.1392 N_Y: 406099 N_S: 1754772 N: 406099 N_HV: 176296 Val: 0.7408 Test: 0.6295\n",
      "Epoch: 482, Loss: 0.0995 tsm_loss: 3.9685 reg_loss: 0.0995 N_Y: 406771 N_S: 1754772 N: 406771 N_HV: 176811 Val: 0.7365 Test: 0.6236\n",
      "Epoch: 483, Loss: 0.0994 tsm_loss: 3.9590 reg_loss: 0.0994 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 173941 Val: 0.7448 Test: 0.6338\n",
      "Epoch: 484, Loss: 0.0946 tsm_loss: 3.9997 reg_loss: 0.0946 N_Y: 406467 N_S: 1754772 N: 406467 N_HV: 176420 Val: 0.7378 Test: 0.6390\n",
      "Epoch: 485, Loss: 0.1008 tsm_loss: 4.0152 reg_loss: 0.1008 N_Y: 406237 N_S: 1754772 N: 406237 N_HV: 178286 Val: 0.7382 Test: 0.6327\n",
      "Epoch: 486, Loss: 0.1126 tsm_loss: 3.8967 reg_loss: 0.1126 N_Y: 406665 N_S: 1754772 N: 406665 N_HV: 172890 Val: 0.7169 Test: 0.6269\n",
      "Epoch: 487, Loss: 0.1388 tsm_loss: 4.1756 reg_loss: 0.1388 N_Y: 406167 N_S: 1754772 N: 406167 N_HV: 178609 Val: 0.7614 Test: 0.6796\n",
      "Epoch: 488, Loss: 0.1508 tsm_loss: 4.0171 reg_loss: 0.1508 N_Y: 405854 N_S: 1754772 N: 405854 N_HV: 175916 Val: 0.7497 Test: 0.6225\n",
      "Epoch: 489, Loss: 0.1347 tsm_loss: 4.0563 reg_loss: 0.1347 N_Y: 407073 N_S: 1754772 N: 407073 N_HV: 176832 Val: 0.7464 Test: 0.6149\n",
      "Epoch: 490, Loss: 0.1229 tsm_loss: 4.0113 reg_loss: 0.1229 N_Y: 406441 N_S: 1754772 N: 406441 N_HV: 178069 Val: 0.7641 Test: 0.6621\n",
      "Epoch: 491, Loss: 0.1469 tsm_loss: 3.9960 reg_loss: 0.1469 N_Y: 405362 N_S: 1754772 N: 405362 N_HV: 179321 Val: 0.7344 Test: 0.6168\n",
      "Epoch: 492, Loss: 0.1088 tsm_loss: 3.8888 reg_loss: 0.1088 N_Y: 406188 N_S: 1754772 N: 406188 N_HV: 175067 Val: 0.7366 Test: 0.6280\n",
      "Epoch: 493, Loss: 0.0980 tsm_loss: 4.0018 reg_loss: 0.0980 N_Y: 406424 N_S: 1754772 N: 406424 N_HV: 175905 Val: 0.7294 Test: 0.6229\n",
      "Epoch: 494, Loss: 0.1091 tsm_loss: 4.0612 reg_loss: 0.1091 N_Y: 405259 N_S: 1754772 N: 405259 N_HV: 175286 Val: 0.7225 Test: 0.6173\n",
      "Epoch: 495, Loss: 0.0992 tsm_loss: 4.0080 reg_loss: 0.0992 N_Y: 405920 N_S: 1754772 N: 405920 N_HV: 176225 Val: 0.7386 Test: 0.6249\n",
      "Epoch: 496, Loss: 0.0959 tsm_loss: 4.0777 reg_loss: 0.0959 N_Y: 405845 N_S: 1754772 N: 405845 N_HV: 175181 Val: 0.7280 Test: 0.6252\n",
      "Epoch: 497, Loss: 0.0982 tsm_loss: 4.0835 reg_loss: 0.0982 N_Y: 405544 N_S: 1754772 N: 405544 N_HV: 177027 Val: 0.7288 Test: 0.6455\n",
      "Epoch: 498, Loss: 0.1145 tsm_loss: 3.9071 reg_loss: 0.1145 N_Y: 407269 N_S: 1754772 N: 407269 N_HV: 175178 Val: 0.7654 Test: 0.6439\n",
      "Epoch: 499, Loss: 0.1250 tsm_loss: 4.1744 reg_loss: 0.1250 N_Y: 407045 N_S: 1754772 N: 407045 N_HV: 177768 Val: 0.7400 Test: 0.6334\n",
      "Epoch: 500, Loss: 0.1070 tsm_loss: 4.1611 reg_loss: 0.1070 N_Y: 405771 N_S: 1754772 N: 405771 N_HV: 176412 Val: 0.7300 Test: 0.6256\n",
      "Epoch: 501, Loss: 0.1027 tsm_loss: 4.2571 reg_loss: 0.1027 N_Y: 406070 N_S: 1754772 N: 406070 N_HV: 177291 Val: 0.7230 Test: 0.6183\n",
      "Epoch: 502, Loss: 0.0931 tsm_loss: 4.1134 reg_loss: 0.0931 N_Y: 405607 N_S: 1754772 N: 405607 N_HV: 174957 Val: 0.7386 Test: 0.6175\n",
      "Epoch: 503, Loss: 0.0998 tsm_loss: 4.1378 reg_loss: 0.0998 N_Y: 406377 N_S: 1754772 N: 406377 N_HV: 178135 Val: 0.7446 Test: 0.6258\n",
      "Epoch: 504, Loss: 0.0954 tsm_loss: 4.0151 reg_loss: 0.0954 N_Y: 406929 N_S: 1754772 N: 406929 N_HV: 176300 Val: 0.7382 Test: 0.6219\n",
      "Epoch: 505, Loss: 0.1225 tsm_loss: 3.9426 reg_loss: 0.1225 N_Y: 402495 N_S: 1754772 N: 402495 N_HV: 171273 Val: 0.7537 Test: 0.6619\n",
      "Epoch: 506, Loss: 0.1429 tsm_loss: 3.9711 reg_loss: 0.1429 N_Y: 407296 N_S: 1754772 N: 407296 N_HV: 176433 Val: 0.7318 Test: 0.6282\n",
      "Epoch: 507, Loss: 0.1098 tsm_loss: 3.9592 reg_loss: 0.1098 N_Y: 406614 N_S: 1754772 N: 406614 N_HV: 175067 Val: 0.7442 Test: 0.6370\n",
      "Epoch: 508, Loss: 0.1025 tsm_loss: 4.0340 reg_loss: 0.1025 N_Y: 404829 N_S: 1754772 N: 404829 N_HV: 173593 Val: 0.7458 Test: 0.6492\n",
      "Epoch: 509, Loss: 0.1198 tsm_loss: 4.1719 reg_loss: 0.1198 N_Y: 406439 N_S: 1754772 N: 406439 N_HV: 178592 Val: 0.7317 Test: 0.6358\n",
      "Epoch: 510, Loss: 0.1214 tsm_loss: 4.0953 reg_loss: 0.1214 N_Y: 406650 N_S: 1754772 N: 406650 N_HV: 174875 Val: 0.7262 Test: 0.6338\n",
      "Epoch: 511, Loss: 0.1106 tsm_loss: 3.8844 reg_loss: 0.1106 N_Y: 404975 N_S: 1754772 N: 404975 N_HV: 173705 Val: 0.7392 Test: 0.6222\n",
      "Epoch: 512, Loss: 0.1261 tsm_loss: 3.9317 reg_loss: 0.1261 N_Y: 405997 N_S: 1754772 N: 405997 N_HV: 174135 Val: 0.7311 Test: 0.6387\n",
      "Epoch: 513, Loss: 0.1507 tsm_loss: 3.8851 reg_loss: 0.1507 N_Y: 406467 N_S: 1754772 N: 406467 N_HV: 175164 Val: 0.7699 Test: 0.6822\n",
      "Epoch: 514, Loss: 0.1631 tsm_loss: 3.9371 reg_loss: 0.1631 N_Y: 404153 N_S: 1754772 N: 404153 N_HV: 173820 Val: 0.7582 Test: 0.6353\n",
      "Epoch: 515, Loss: 0.1392 tsm_loss: 3.9001 reg_loss: 0.1392 N_Y: 406675 N_S: 1754772 N: 406675 N_HV: 174033 Val: 0.7276 Test: 0.6195\n",
      "Epoch: 516, Loss: 0.1336 tsm_loss: 3.9324 reg_loss: 0.1336 N_Y: 407657 N_S: 1754772 N: 407657 N_HV: 174151 Val: 0.7256 Test: 0.6322\n",
      "Epoch: 517, Loss: 0.1640 tsm_loss: 4.1065 reg_loss: 0.1640 N_Y: 406657 N_S: 1754772 N: 406657 N_HV: 176961 Val: 0.7498 Test: 0.6291\n",
      "Epoch: 518, Loss: 0.1450 tsm_loss: 4.1207 reg_loss: 0.1450 N_Y: 407471 N_S: 1754772 N: 407471 N_HV: 176085 Val: 0.7448 Test: 0.6512\n",
      "Epoch: 519, Loss: 0.1361 tsm_loss: 4.0949 reg_loss: 0.1361 N_Y: 406447 N_S: 1754772 N: 406447 N_HV: 174330 Val: 0.7388 Test: 0.6482\n",
      "Epoch: 520, Loss: 0.1589 tsm_loss: 4.0711 reg_loss: 0.1589 N_Y: 407544 N_S: 1754772 N: 407544 N_HV: 174094 Val: 0.7659 Test: 0.6385\n",
      "Epoch: 521, Loss: 0.1448 tsm_loss: 4.1757 reg_loss: 0.1448 N_Y: 406764 N_S: 1754772 N: 406764 N_HV: 176855 Val: 0.7296 Test: 0.6225\n",
      "Epoch: 522, Loss: 0.1291 tsm_loss: 4.0628 reg_loss: 0.1291 N_Y: 406043 N_S: 1754772 N: 406043 N_HV: 174686 Val: 0.7472 Test: 0.6566\n",
      "Epoch: 523, Loss: 0.1276 tsm_loss: 4.1521 reg_loss: 0.1276 N_Y: 406728 N_S: 1754772 N: 406728 N_HV: 178416 Val: 0.7391 Test: 0.6191\n",
      "Epoch: 524, Loss: 0.1342 tsm_loss: 4.1504 reg_loss: 0.1342 N_Y: 404042 N_S: 1754772 N: 404042 N_HV: 176920 Val: 0.7392 Test: 0.6199\n",
      "Epoch: 525, Loss: 0.1362 tsm_loss: 4.0345 reg_loss: 0.1362 N_Y: 405073 N_S: 1754772 N: 405073 N_HV: 174639 Val: 0.7429 Test: 0.6486\n",
      "Epoch: 526, Loss: 0.1548 tsm_loss: 3.9570 reg_loss: 0.1548 N_Y: 406379 N_S: 1754772 N: 406379 N_HV: 176220 Val: 0.7752 Test: 0.6370\n",
      "Epoch: 527, Loss: 0.1230 tsm_loss: 4.0437 reg_loss: 0.1230 N_Y: 405510 N_S: 1754772 N: 405510 N_HV: 174010 Val: 0.7600 Test: 0.6888\n",
      "Epoch: 528, Loss: 0.1331 tsm_loss: 4.2085 reg_loss: 0.1331 N_Y: 407011 N_S: 1754772 N: 407011 N_HV: 178191 Val: 0.7235 Test: 0.6374\n",
      "Epoch: 529, Loss: 0.1123 tsm_loss: 4.2277 reg_loss: 0.1123 N_Y: 406255 N_S: 1754772 N: 406255 N_HV: 176696 Val: 0.7697 Test: 0.6561\n",
      "Epoch: 530, Loss: 0.1282 tsm_loss: 4.2436 reg_loss: 0.1282 N_Y: 401427 N_S: 1754772 N: 401427 N_HV: 173965 Val: 0.7418 Test: 0.6259\n",
      "Epoch: 531, Loss: 0.1010 tsm_loss: 4.1453 reg_loss: 0.1010 N_Y: 406278 N_S: 1754772 N: 406278 N_HV: 177634 Val: 0.7567 Test: 0.6698\n",
      "Epoch: 532, Loss: 0.1190 tsm_loss: 4.2430 reg_loss: 0.1190 N_Y: 407542 N_S: 1754772 N: 407542 N_HV: 178266 Val: 0.7230 Test: 0.6314\n",
      "Epoch: 533, Loss: 0.0897 tsm_loss: 4.2272 reg_loss: 0.0897 N_Y: 406210 N_S: 1754772 N: 406210 N_HV: 178092 Val: 0.7295 Test: 0.6433\n",
      "Epoch: 534, Loss: 0.1212 tsm_loss: 4.2642 reg_loss: 0.1212 N_Y: 405808 N_S: 1754772 N: 405808 N_HV: 177652 Val: 0.7566 Test: 0.6743\n",
      "Epoch: 535, Loss: 0.0975 tsm_loss: 4.2978 reg_loss: 0.0975 N_Y: 407052 N_S: 1754772 N: 407052 N_HV: 179472 Val: 0.7357 Test: 0.6345\n",
      "Epoch: 536, Loss: 0.1117 tsm_loss: 4.0722 reg_loss: 0.1117 N_Y: 407522 N_S: 1754772 N: 407522 N_HV: 174282 Val: 0.7431 Test: 0.6382\n",
      "Epoch: 537, Loss: 0.1262 tsm_loss: 4.3341 reg_loss: 0.1262 N_Y: 405855 N_S: 1754772 N: 405855 N_HV: 180118 Val: 0.7299 Test: 0.6440\n",
      "Epoch: 538, Loss: 0.1521 tsm_loss: 4.1190 reg_loss: 0.1521 N_Y: 406114 N_S: 1754772 N: 406114 N_HV: 177826 Val: 0.7511 Test: 0.6279\n",
      "Epoch: 539, Loss: 0.1591 tsm_loss: 4.2309 reg_loss: 0.1591 N_Y: 408260 N_S: 1754772 N: 408260 N_HV: 177691 Val: 0.7502 Test: 0.6410\n",
      "Epoch: 540, Loss: 0.1592 tsm_loss: 4.0694 reg_loss: 0.1592 N_Y: 404510 N_S: 1754772 N: 404510 N_HV: 176247 Val: 0.7573 Test: 0.6592\n",
      "Epoch: 541, Loss: 0.1067 tsm_loss: 4.0900 reg_loss: 0.1067 N_Y: 404980 N_S: 1754772 N: 404980 N_HV: 176972 Val: 0.7410 Test: 0.6308\n",
      "Epoch: 542, Loss: 0.0896 tsm_loss: 4.0728 reg_loss: 0.0896 N_Y: 403498 N_S: 1754772 N: 403498 N_HV: 176432 Val: 0.7441 Test: 0.6300\n",
      "Epoch: 543, Loss: 0.1222 tsm_loss: 4.0449 reg_loss: 0.1222 N_Y: 404571 N_S: 1754772 N: 404571 N_HV: 174101 Val: 0.7451 Test: 0.6509\n",
      "Epoch: 544, Loss: 0.1222 tsm_loss: 3.9712 reg_loss: 0.1222 N_Y: 406807 N_S: 1754772 N: 406807 N_HV: 175546 Val: 0.7399 Test: 0.6362\n",
      "Epoch: 545, Loss: 0.1444 tsm_loss: 4.0967 reg_loss: 0.1444 N_Y: 406688 N_S: 1754772 N: 406688 N_HV: 179625 Val: 0.7844 Test: 0.6471\n",
      "Epoch: 546, Loss: 0.1685 tsm_loss: 4.1286 reg_loss: 0.1685 N_Y: 406542 N_S: 1754772 N: 406542 N_HV: 177289 Val: 0.7381 Test: 0.6563\n",
      "Epoch: 547, Loss: 0.1467 tsm_loss: 4.1229 reg_loss: 0.1467 N_Y: 405923 N_S: 1754772 N: 405923 N_HV: 174886 Val: 0.7362 Test: 0.6252\n",
      "Epoch: 548, Loss: 0.1154 tsm_loss: 4.3006 reg_loss: 0.1154 N_Y: 406370 N_S: 1754772 N: 406370 N_HV: 176394 Val: 0.7422 Test: 0.6122\n",
      "Epoch: 549, Loss: 0.1273 tsm_loss: 4.2108 reg_loss: 0.1273 N_Y: 406498 N_S: 1754772 N: 406498 N_HV: 179072 Val: 0.7441 Test: 0.6249\n",
      "Epoch: 550, Loss: 0.1065 tsm_loss: 4.2659 reg_loss: 0.1065 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 176222 Val: 0.7454 Test: 0.6373\n",
      "Epoch: 551, Loss: 0.1212 tsm_loss: 4.2270 reg_loss: 0.1212 N_Y: 406314 N_S: 1754772 N: 406314 N_HV: 179170 Val: 0.7553 Test: 0.6261\n",
      "Epoch: 552, Loss: 0.0954 tsm_loss: 4.1788 reg_loss: 0.0954 N_Y: 406942 N_S: 1754772 N: 406942 N_HV: 177808 Val: 0.7347 Test: 0.6252\n",
      "Epoch: 553, Loss: 0.0907 tsm_loss: 4.1421 reg_loss: 0.0907 N_Y: 407059 N_S: 1754772 N: 407059 N_HV: 178989 Val: 0.7347 Test: 0.6274\n",
      "Epoch: 554, Loss: 0.1137 tsm_loss: 4.1652 reg_loss: 0.1137 N_Y: 406120 N_S: 1754772 N: 406120 N_HV: 181131 Val: 0.7339 Test: 0.6296\n",
      "Epoch: 555, Loss: 0.0972 tsm_loss: 4.0981 reg_loss: 0.0972 N_Y: 407853 N_S: 1754772 N: 407853 N_HV: 177940 Val: 0.7306 Test: 0.6214\n",
      "Epoch: 556, Loss: 0.1052 tsm_loss: 4.1633 reg_loss: 0.1052 N_Y: 404548 N_S: 1754772 N: 404548 N_HV: 178895 Val: 0.7524 Test: 0.6360\n",
      "Epoch: 557, Loss: 0.1438 tsm_loss: 4.0381 reg_loss: 0.1438 N_Y: 404957 N_S: 1754772 N: 404957 N_HV: 176163 Val: 0.7410 Test: 0.6419\n",
      "Epoch: 558, Loss: 0.1279 tsm_loss: 4.2564 reg_loss: 0.1279 N_Y: 404438 N_S: 1754772 N: 404438 N_HV: 179346 Val: 0.7312 Test: 0.6163\n",
      "Epoch: 559, Loss: 0.1179 tsm_loss: 4.1574 reg_loss: 0.1179 N_Y: 403862 N_S: 1754772 N: 403862 N_HV: 177549 Val: 0.7462 Test: 0.6324\n",
      "Epoch: 560, Loss: 0.1208 tsm_loss: 4.0328 reg_loss: 0.1208 N_Y: 407336 N_S: 1754772 N: 407336 N_HV: 176070 Val: 0.7302 Test: 0.6325\n",
      "Epoch: 561, Loss: 0.1169 tsm_loss: 4.1052 reg_loss: 0.1169 N_Y: 407049 N_S: 1754772 N: 407049 N_HV: 177705 Val: 0.7241 Test: 0.6207\n",
      "Epoch: 562, Loss: 0.1136 tsm_loss: 4.0967 reg_loss: 0.1136 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 177013 Val: 0.7305 Test: 0.6386\n",
      "Epoch: 563, Loss: 0.1150 tsm_loss: 4.2412 reg_loss: 0.1150 N_Y: 405960 N_S: 1754772 N: 405960 N_HV: 179236 Val: 0.7311 Test: 0.6545\n",
      "Epoch: 564, Loss: 0.1025 tsm_loss: 4.2158 reg_loss: 0.1025 N_Y: 406459 N_S: 1754772 N: 406459 N_HV: 176998 Val: 0.7287 Test: 0.6280\n",
      "Epoch: 565, Loss: 0.1321 tsm_loss: 4.2257 reg_loss: 0.1321 N_Y: 405547 N_S: 1754772 N: 405547 N_HV: 177120 Val: 0.7650 Test: 0.6413\n",
      "Epoch: 566, Loss: 0.1050 tsm_loss: 4.1366 reg_loss: 0.1050 N_Y: 406011 N_S: 1754772 N: 406011 N_HV: 176395 Val: 0.7504 Test: 0.6151\n",
      "Epoch: 567, Loss: 0.1146 tsm_loss: 4.1238 reg_loss: 0.1146 N_Y: 404139 N_S: 1754772 N: 404139 N_HV: 174301 Val: 0.7334 Test: 0.6414\n",
      "Epoch: 568, Loss: 0.0913 tsm_loss: 4.1850 reg_loss: 0.0913 N_Y: 405325 N_S: 1754772 N: 405325 N_HV: 174442 Val: 0.7328 Test: 0.6519\n",
      "Epoch: 569, Loss: 0.1274 tsm_loss: 4.0526 reg_loss: 0.1274 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 173587 Val: 0.7505 Test: 0.6458\n",
      "Epoch: 570, Loss: 0.1211 tsm_loss: 4.2478 reg_loss: 0.1211 N_Y: 406198 N_S: 1754772 N: 406198 N_HV: 178328 Val: 0.7711 Test: 0.6372\n",
      "Epoch: 571, Loss: 0.1426 tsm_loss: 4.0625 reg_loss: 0.1426 N_Y: 407358 N_S: 1754772 N: 407358 N_HV: 174954 Val: 0.7439 Test: 0.6191\n",
      "Epoch: 572, Loss: 0.1134 tsm_loss: 4.1322 reg_loss: 0.1134 N_Y: 406104 N_S: 1754772 N: 406104 N_HV: 176889 Val: 0.7329 Test: 0.6307\n",
      "Epoch: 573, Loss: 0.1295 tsm_loss: 4.1347 reg_loss: 0.1295 N_Y: 406228 N_S: 1754772 N: 406228 N_HV: 177730 Val: 0.7281 Test: 0.6155\n",
      "Epoch: 574, Loss: 0.0985 tsm_loss: 4.1752 reg_loss: 0.0985 N_Y: 406010 N_S: 1754772 N: 406010 N_HV: 180038 Val: 0.7432 Test: 0.6234\n",
      "Epoch: 575, Loss: 0.1291 tsm_loss: 4.1143 reg_loss: 0.1291 N_Y: 406774 N_S: 1754772 N: 406774 N_HV: 177417 Val: 0.7792 Test: 0.6943\n",
      "Epoch: 576, Loss: 0.1667 tsm_loss: 4.1154 reg_loss: 0.1667 N_Y: 401376 N_S: 1754772 N: 401376 N_HV: 175941 Val: 0.7333 Test: 0.6220\n",
      "Epoch: 577, Loss: 0.1391 tsm_loss: 4.1451 reg_loss: 0.1391 N_Y: 407498 N_S: 1754772 N: 407498 N_HV: 178208 Val: 0.7551 Test: 0.6292\n",
      "Epoch: 578, Loss: 0.1648 tsm_loss: 4.2388 reg_loss: 0.1648 N_Y: 405086 N_S: 1754772 N: 405086 N_HV: 179590 Val: 0.7461 Test: 0.6449\n",
      "Epoch: 579, Loss: 0.0979 tsm_loss: 4.1454 reg_loss: 0.0979 N_Y: 406584 N_S: 1754772 N: 406584 N_HV: 177984 Val: 0.7317 Test: 0.6162\n",
      "Epoch: 580, Loss: 0.1331 tsm_loss: 4.1501 reg_loss: 0.1331 N_Y: 407464 N_S: 1754772 N: 407464 N_HV: 179500 Val: 0.7474 Test: 0.6268\n",
      "Epoch: 581, Loss: 0.1588 tsm_loss: 4.1759 reg_loss: 0.1588 N_Y: 404296 N_S: 1754772 N: 404296 N_HV: 176233 Val: 0.7365 Test: 0.6371\n",
      "Epoch: 582, Loss: 0.1031 tsm_loss: 4.2072 reg_loss: 0.1031 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 176936 Val: 0.7388 Test: 0.6450\n",
      "Epoch: 583, Loss: 0.1307 tsm_loss: 4.0698 reg_loss: 0.1307 N_Y: 403768 N_S: 1754772 N: 403768 N_HV: 175819 Val: 0.7619 Test: 0.6352\n",
      "Epoch: 584, Loss: 0.1120 tsm_loss: 4.1186 reg_loss: 0.1120 N_Y: 405923 N_S: 1754772 N: 405923 N_HV: 175835 Val: 0.7384 Test: 0.6309\n",
      "Epoch: 585, Loss: 0.0997 tsm_loss: 4.0704 reg_loss: 0.0997 N_Y: 403995 N_S: 1754772 N: 403995 N_HV: 176466 Val: 0.7380 Test: 0.6356\n",
      "Epoch: 586, Loss: 0.0893 tsm_loss: 4.1511 reg_loss: 0.0893 N_Y: 406358 N_S: 1754772 N: 406358 N_HV: 178526 Val: 0.7413 Test: 0.6340\n",
      "Epoch: 587, Loss: 0.0869 tsm_loss: 4.0348 reg_loss: 0.0869 N_Y: 406756 N_S: 1754772 N: 406756 N_HV: 178588 Val: 0.7640 Test: 0.6387\n",
      "Epoch: 588, Loss: 0.1188 tsm_loss: 4.1323 reg_loss: 0.1188 N_Y: 405012 N_S: 1754772 N: 405012 N_HV: 176631 Val: 0.7342 Test: 0.6252\n",
      "Epoch: 589, Loss: 0.0956 tsm_loss: 4.2675 reg_loss: 0.0956 N_Y: 406660 N_S: 1754772 N: 406660 N_HV: 177077 Val: 0.7554 Test: 0.6436\n",
      "Epoch: 590, Loss: 0.1105 tsm_loss: 4.0869 reg_loss: 0.1105 N_Y: 406883 N_S: 1754772 N: 406883 N_HV: 175169 Val: 0.7342 Test: 0.6286\n",
      "Epoch: 591, Loss: 0.1313 tsm_loss: 4.1500 reg_loss: 0.1313 N_Y: 403602 N_S: 1754772 N: 403602 N_HV: 179550 Val: 0.7419 Test: 0.6359\n",
      "Epoch: 592, Loss: 0.1043 tsm_loss: 4.0547 reg_loss: 0.1043 N_Y: 406373 N_S: 1754772 N: 406373 N_HV: 175937 Val: 0.7319 Test: 0.6347\n",
      "Epoch: 593, Loss: 0.1128 tsm_loss: 4.2465 reg_loss: 0.1128 N_Y: 405501 N_S: 1754772 N: 405501 N_HV: 179917 Val: 0.7350 Test: 0.6393\n",
      "Epoch: 594, Loss: 0.1367 tsm_loss: 4.0812 reg_loss: 0.1367 N_Y: 406363 N_S: 1754772 N: 406363 N_HV: 176871 Val: 0.7452 Test: 0.6466\n",
      "Epoch: 595, Loss: 0.1138 tsm_loss: 4.0512 reg_loss: 0.1138 N_Y: 406534 N_S: 1754772 N: 406534 N_HV: 176485 Val: 0.7439 Test: 0.6443\n",
      "Epoch: 596, Loss: 0.1346 tsm_loss: 4.0851 reg_loss: 0.1346 N_Y: 407498 N_S: 1754772 N: 407498 N_HV: 178486 Val: 0.7494 Test: 0.6578\n",
      "Epoch: 597, Loss: 0.1240 tsm_loss: 4.1075 reg_loss: 0.1240 N_Y: 406967 N_S: 1754772 N: 406967 N_HV: 176747 Val: 0.7342 Test: 0.6492\n",
      "Epoch: 598, Loss: 0.1165 tsm_loss: 4.1922 reg_loss: 0.1165 N_Y: 408400 N_S: 1754772 N: 408400 N_HV: 178244 Val: 0.7233 Test: 0.6169\n",
      "Epoch: 599, Loss: 0.1111 tsm_loss: 4.2580 reg_loss: 0.1111 N_Y: 405530 N_S: 1754772 N: 405530 N_HV: 176425 Val: 0.7363 Test: 0.6228\n",
      "Epoch: 600, Loss: 0.1153 tsm_loss: 4.0752 reg_loss: 0.1153 N_Y: 406830 N_S: 1754772 N: 406830 N_HV: 174757 Val: 0.7341 Test: 0.6262\n",
      "Epoch: 601, Loss: 0.0932 tsm_loss: 4.2100 reg_loss: 0.0932 N_Y: 407442 N_S: 1754772 N: 407442 N_HV: 178273 Val: 0.7352 Test: 0.6258\n",
      "Epoch: 602, Loss: 0.0877 tsm_loss: 4.2569 reg_loss: 0.0877 N_Y: 405879 N_S: 1754772 N: 405879 N_HV: 177840 Val: 0.7391 Test: 0.6148\n",
      "Epoch: 603, Loss: 0.0971 tsm_loss: 4.1157 reg_loss: 0.0971 N_Y: 406124 N_S: 1754772 N: 406124 N_HV: 176997 Val: 0.7437 Test: 0.6378\n",
      "Epoch: 604, Loss: 0.0956 tsm_loss: 4.1656 reg_loss: 0.0956 N_Y: 403139 N_S: 1754772 N: 403139 N_HV: 175991 Val: 0.7394 Test: 0.6179\n",
      "Epoch: 605, Loss: 0.0907 tsm_loss: 4.0916 reg_loss: 0.0907 N_Y: 406477 N_S: 1754772 N: 406477 N_HV: 176170 Val: 0.7367 Test: 0.6260\n",
      "Epoch: 606, Loss: 0.1024 tsm_loss: 4.0846 reg_loss: 0.1024 N_Y: 403884 N_S: 1754772 N: 403884 N_HV: 175102 Val: 0.7364 Test: 0.6072\n",
      "Epoch: 607, Loss: 0.1079 tsm_loss: 4.0358 reg_loss: 0.1079 N_Y: 404210 N_S: 1754772 N: 404210 N_HV: 172848 Val: 0.7640 Test: 0.6336\n",
      "Epoch: 608, Loss: 0.1155 tsm_loss: 3.9790 reg_loss: 0.1155 N_Y: 405989 N_S: 1754772 N: 405989 N_HV: 176702 Val: 0.7329 Test: 0.6301\n",
      "Epoch: 609, Loss: 0.1518 tsm_loss: 4.1284 reg_loss: 0.1518 N_Y: 407490 N_S: 1754772 N: 407490 N_HV: 175473 Val: 0.7328 Test: 0.6239\n",
      "Epoch: 610, Loss: 0.1246 tsm_loss: 4.1646 reg_loss: 0.1246 N_Y: 404498 N_S: 1754772 N: 404498 N_HV: 176317 Val: 0.7241 Test: 0.6198\n",
      "Epoch: 611, Loss: 0.1141 tsm_loss: 4.1068 reg_loss: 0.1141 N_Y: 404254 N_S: 1754772 N: 404254 N_HV: 175555 Val: 0.7365 Test: 0.6453\n",
      "Epoch: 612, Loss: 0.1106 tsm_loss: 4.0402 reg_loss: 0.1106 N_Y: 405999 N_S: 1754772 N: 405999 N_HV: 173207 Val: 0.7219 Test: 0.6461\n",
      "Epoch: 613, Loss: 0.1657 tsm_loss: 4.0550 reg_loss: 0.1657 N_Y: 405264 N_S: 1754772 N: 405264 N_HV: 175033 Val: 0.7575 Test: 0.6460\n",
      "Epoch: 614, Loss: 0.1607 tsm_loss: 3.9922 reg_loss: 0.1607 N_Y: 406125 N_S: 1754772 N: 406125 N_HV: 174927 Val: 0.7440 Test: 0.6493\n",
      "Epoch: 615, Loss: 0.1475 tsm_loss: 3.9851 reg_loss: 0.1475 N_Y: 407499 N_S: 1754772 N: 407499 N_HV: 177299 Val: 0.7246 Test: 0.6355\n",
      "Epoch: 616, Loss: 0.1095 tsm_loss: 4.0120 reg_loss: 0.1095 N_Y: 405660 N_S: 1754772 N: 405660 N_HV: 176888 Val: 0.7392 Test: 0.6285\n",
      "Epoch: 617, Loss: 0.0921 tsm_loss: 4.1370 reg_loss: 0.0921 N_Y: 406227 N_S: 1754772 N: 406227 N_HV: 177475 Val: 0.7391 Test: 0.6301\n",
      "Epoch: 618, Loss: 0.1066 tsm_loss: 4.1069 reg_loss: 0.1066 N_Y: 406474 N_S: 1754772 N: 406474 N_HV: 176359 Val: 0.8000 Test: 0.7304\n",
      "Epoch: 619, Loss: 0.1282 tsm_loss: 4.0675 reg_loss: 0.1282 N_Y: 406337 N_S: 1754772 N: 406337 N_HV: 176640 Val: 0.7264 Test: 0.6122\n",
      "Epoch: 620, Loss: 0.0994 tsm_loss: 4.0855 reg_loss: 0.0994 N_Y: 407065 N_S: 1754772 N: 407065 N_HV: 177277 Val: 0.7462 Test: 0.6223\n",
      "Epoch: 621, Loss: 0.1337 tsm_loss: 3.8344 reg_loss: 0.1337 N_Y: 405576 N_S: 1754772 N: 405576 N_HV: 172921 Val: 0.7412 Test: 0.6418\n",
      "Epoch: 622, Loss: 0.1388 tsm_loss: 4.0349 reg_loss: 0.1388 N_Y: 406797 N_S: 1754772 N: 406797 N_HV: 177842 Val: 0.7299 Test: 0.6210\n",
      "Epoch: 623, Loss: 0.1083 tsm_loss: 4.1820 reg_loss: 0.1083 N_Y: 405930 N_S: 1754772 N: 405930 N_HV: 175884 Val: 0.7470 Test: 0.6353\n",
      "Epoch: 624, Loss: 0.0843 tsm_loss: 4.2456 reg_loss: 0.0843 N_Y: 405841 N_S: 1754772 N: 405841 N_HV: 178578 Val: 0.7299 Test: 0.6252\n",
      "Epoch: 625, Loss: 0.0964 tsm_loss: 3.9277 reg_loss: 0.0964 N_Y: 404498 N_S: 1754772 N: 404498 N_HV: 172286 Val: 0.7484 Test: 0.6209\n",
      "Epoch: 626, Loss: 0.1327 tsm_loss: 3.9716 reg_loss: 0.1327 N_Y: 403478 N_S: 1754772 N: 403478 N_HV: 173316 Val: 0.7320 Test: 0.6145\n",
      "Epoch: 627, Loss: 0.1165 tsm_loss: 3.8727 reg_loss: 0.1165 N_Y: 406227 N_S: 1754772 N: 406227 N_HV: 171880 Val: 0.7422 Test: 0.6152\n",
      "Epoch: 628, Loss: 0.1233 tsm_loss: 3.9012 reg_loss: 0.1233 N_Y: 405110 N_S: 1754772 N: 405110 N_HV: 172507 Val: 0.7681 Test: 0.6350\n",
      "Epoch: 629, Loss: 0.1729 tsm_loss: 3.9446 reg_loss: 0.1729 N_Y: 406977 N_S: 1754772 N: 406977 N_HV: 173649 Val: 0.7709 Test: 0.6677\n",
      "Epoch: 630, Loss: 0.1479 tsm_loss: 3.8564 reg_loss: 0.1479 N_Y: 405935 N_S: 1754772 N: 405935 N_HV: 172372 Val: 0.7599 Test: 0.6355\n",
      "Epoch: 631, Loss: 0.1261 tsm_loss: 3.9059 reg_loss: 0.1261 N_Y: 405788 N_S: 1754772 N: 405788 N_HV: 172675 Val: 0.7241 Test: 0.6113\n",
      "Epoch: 632, Loss: 0.0873 tsm_loss: 4.0151 reg_loss: 0.0873 N_Y: 405438 N_S: 1754772 N: 405438 N_HV: 175513 Val: 0.7372 Test: 0.6151\n",
      "Epoch: 633, Loss: 0.0820 tsm_loss: 3.9539 reg_loss: 0.0820 N_Y: 407049 N_S: 1754772 N: 407049 N_HV: 173889 Val: 0.7299 Test: 0.6133\n",
      "Epoch: 634, Loss: 0.0945 tsm_loss: 3.7823 reg_loss: 0.0945 N_Y: 404542 N_S: 1754772 N: 404542 N_HV: 171237 Val: 0.7280 Test: 0.6090\n",
      "Epoch: 635, Loss: 0.0803 tsm_loss: 3.8890 reg_loss: 0.0803 N_Y: 406641 N_S: 1754772 N: 406641 N_HV: 176353 Val: 0.7396 Test: 0.6125\n",
      "Epoch: 636, Loss: 0.0943 tsm_loss: 4.0492 reg_loss: 0.0943 N_Y: 405600 N_S: 1754772 N: 405600 N_HV: 175607 Val: 0.7425 Test: 0.6378\n",
      "Epoch: 637, Loss: 0.0945 tsm_loss: 4.0021 reg_loss: 0.0945 N_Y: 404783 N_S: 1754772 N: 404783 N_HV: 173431 Val: 0.7372 Test: 0.6205\n",
      "Epoch: 638, Loss: 0.1214 tsm_loss: 3.9223 reg_loss: 0.1214 N_Y: 405958 N_S: 1754772 N: 405958 N_HV: 175010 Val: 0.7497 Test: 0.6096\n",
      "Epoch: 639, Loss: 0.1257 tsm_loss: 4.0315 reg_loss: 0.1257 N_Y: 405033 N_S: 1754772 N: 405033 N_HV: 173586 Val: 0.7456 Test: 0.6216\n",
      "Epoch: 640, Loss: 0.1023 tsm_loss: 3.9490 reg_loss: 0.1023 N_Y: 407601 N_S: 1754772 N: 407601 N_HV: 177424 Val: 0.7480 Test: 0.6146\n",
      "Epoch: 641, Loss: 0.0841 tsm_loss: 4.0183 reg_loss: 0.0841 N_Y: 406065 N_S: 1754772 N: 406065 N_HV: 178571 Val: 0.7568 Test: 0.6662\n",
      "Epoch: 642, Loss: 0.1067 tsm_loss: 4.1189 reg_loss: 0.1067 N_Y: 405822 N_S: 1754772 N: 405822 N_HV: 177467 Val: 0.7462 Test: 0.6106\n",
      "Epoch: 643, Loss: 0.0853 tsm_loss: 3.9611 reg_loss: 0.0853 N_Y: 406561 N_S: 1754772 N: 406561 N_HV: 178222 Val: 0.7400 Test: 0.6196\n",
      "Epoch: 644, Loss: 0.0997 tsm_loss: 4.1241 reg_loss: 0.0997 N_Y: 406604 N_S: 1754772 N: 406604 N_HV: 179561 Val: 0.7404 Test: 0.6274\n",
      "Epoch: 645, Loss: 0.0864 tsm_loss: 3.9988 reg_loss: 0.0864 N_Y: 406204 N_S: 1754772 N: 406204 N_HV: 178214 Val: 0.7427 Test: 0.6098\n",
      "Epoch: 646, Loss: 0.0975 tsm_loss: 3.9443 reg_loss: 0.0975 N_Y: 406189 N_S: 1754772 N: 406189 N_HV: 175710 Val: 0.7407 Test: 0.6080\n",
      "Epoch: 647, Loss: 0.1000 tsm_loss: 3.9375 reg_loss: 0.1000 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 176044 Val: 0.7417 Test: 0.6315\n",
      "Epoch: 648, Loss: 0.1120 tsm_loss: 3.8803 reg_loss: 0.1120 N_Y: 406111 N_S: 1754772 N: 406111 N_HV: 174917 Val: 0.7319 Test: 0.6157\n",
      "Epoch: 649, Loss: 0.1012 tsm_loss: 3.9010 reg_loss: 0.1012 N_Y: 407431 N_S: 1754772 N: 407431 N_HV: 176017 Val: 0.7392 Test: 0.6449\n",
      "Epoch: 650, Loss: 0.1125 tsm_loss: 3.9006 reg_loss: 0.1125 N_Y: 407150 N_S: 1754772 N: 407150 N_HV: 176328 Val: 0.7231 Test: 0.6216\n",
      "Epoch: 651, Loss: 0.1423 tsm_loss: 4.0298 reg_loss: 0.1423 N_Y: 407395 N_S: 1754772 N: 407395 N_HV: 177570 Val: 0.7308 Test: 0.6230\n",
      "Epoch: 652, Loss: 0.1004 tsm_loss: 4.1028 reg_loss: 0.1004 N_Y: 404439 N_S: 1754772 N: 404439 N_HV: 175100 Val: 0.7410 Test: 0.6207\n",
      "Epoch: 653, Loss: 0.0821 tsm_loss: 4.0810 reg_loss: 0.0821 N_Y: 407272 N_S: 1754772 N: 407272 N_HV: 178827 Val: 0.7398 Test: 0.6244\n",
      "Epoch: 654, Loss: 0.0959 tsm_loss: 4.0807 reg_loss: 0.0959 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 178228 Val: 0.7357 Test: 0.6179\n",
      "Epoch: 655, Loss: 0.1326 tsm_loss: 4.1610 reg_loss: 0.1326 N_Y: 405488 N_S: 1754772 N: 405488 N_HV: 178716 Val: 0.7406 Test: 0.6270\n",
      "Epoch: 656, Loss: 0.1003 tsm_loss: 3.9602 reg_loss: 0.1003 N_Y: 407472 N_S: 1754772 N: 407472 N_HV: 176323 Val: 0.7263 Test: 0.6298\n",
      "Epoch: 657, Loss: 0.1076 tsm_loss: 4.0507 reg_loss: 0.1076 N_Y: 405240 N_S: 1754772 N: 405240 N_HV: 178626 Val: 0.7542 Test: 0.6318\n",
      "Epoch: 658, Loss: 0.1538 tsm_loss: 3.9332 reg_loss: 0.1538 N_Y: 406658 N_S: 1754772 N: 406658 N_HV: 173892 Val: 0.7420 Test: 0.6476\n",
      "Epoch: 659, Loss: 0.1310 tsm_loss: 3.8490 reg_loss: 0.1310 N_Y: 404714 N_S: 1754772 N: 404714 N_HV: 174139 Val: 0.7718 Test: 0.6541\n",
      "Epoch: 660, Loss: 0.1221 tsm_loss: 4.0077 reg_loss: 0.1221 N_Y: 406817 N_S: 1754772 N: 406817 N_HV: 177366 Val: 0.7275 Test: 0.6098\n",
      "Epoch: 661, Loss: 0.1000 tsm_loss: 3.9982 reg_loss: 0.1000 N_Y: 407446 N_S: 1754772 N: 407446 N_HV: 174541 Val: 0.7318 Test: 0.6343\n",
      "Epoch: 662, Loss: 0.1115 tsm_loss: 4.0397 reg_loss: 0.1115 N_Y: 407357 N_S: 1754772 N: 407357 N_HV: 176028 Val: 0.7500 Test: 0.6402\n",
      "Epoch: 663, Loss: 0.1090 tsm_loss: 4.2437 reg_loss: 0.1090 N_Y: 405126 N_S: 1754772 N: 405126 N_HV: 177727 Val: 0.7416 Test: 0.6171\n",
      "Epoch: 664, Loss: 0.0946 tsm_loss: 4.1434 reg_loss: 0.0946 N_Y: 406353 N_S: 1754772 N: 406353 N_HV: 176778 Val: 0.7375 Test: 0.6162\n",
      "Epoch: 665, Loss: 0.0895 tsm_loss: 4.1835 reg_loss: 0.0895 N_Y: 405289 N_S: 1754772 N: 405289 N_HV: 178122 Val: 0.7341 Test: 0.5995\n",
      "Epoch: 666, Loss: 0.0836 tsm_loss: 4.2011 reg_loss: 0.0836 N_Y: 405628 N_S: 1754772 N: 405628 N_HV: 179058 Val: 0.7495 Test: 0.6309\n",
      "Epoch: 667, Loss: 0.0841 tsm_loss: 4.1436 reg_loss: 0.0841 N_Y: 407745 N_S: 1754772 N: 407745 N_HV: 180861 Val: 0.7310 Test: 0.6032\n",
      "Epoch: 668, Loss: 0.0830 tsm_loss: 4.1269 reg_loss: 0.0830 N_Y: 406653 N_S: 1754772 N: 406653 N_HV: 179035 Val: 0.7330 Test: 0.6193\n",
      "Epoch: 669, Loss: 0.1057 tsm_loss: 4.1233 reg_loss: 0.1057 N_Y: 406248 N_S: 1754772 N: 406248 N_HV: 178811 Val: 0.7411 Test: 0.6430\n",
      "Epoch: 670, Loss: 0.1181 tsm_loss: 4.1793 reg_loss: 0.1181 N_Y: 404587 N_S: 1754772 N: 404587 N_HV: 179990 Val: 0.7587 Test: 0.6617\n",
      "Epoch: 671, Loss: 0.0862 tsm_loss: 3.9956 reg_loss: 0.0862 N_Y: 407779 N_S: 1754772 N: 407779 N_HV: 177037 Val: 0.7363 Test: 0.6099\n",
      "Epoch: 672, Loss: 0.1042 tsm_loss: 4.0512 reg_loss: 0.1042 N_Y: 407240 N_S: 1754772 N: 407240 N_HV: 181640 Val: 0.7548 Test: 0.6174\n",
      "Epoch: 673, Loss: 0.0948 tsm_loss: 3.9731 reg_loss: 0.0948 N_Y: 404779 N_S: 1754772 N: 404779 N_HV: 176947 Val: 0.7419 Test: 0.6121\n",
      "Epoch: 674, Loss: 0.1056 tsm_loss: 3.9890 reg_loss: 0.1056 N_Y: 406302 N_S: 1754772 N: 406302 N_HV: 180361 Val: 0.7471 Test: 0.6018\n",
      "Epoch: 675, Loss: 0.1192 tsm_loss: 3.8858 reg_loss: 0.1192 N_Y: 407440 N_S: 1754772 N: 407440 N_HV: 175499 Val: 0.7460 Test: 0.6052\n",
      "Epoch: 676, Loss: 0.1183 tsm_loss: 4.0557 reg_loss: 0.1183 N_Y: 405115 N_S: 1754772 N: 405115 N_HV: 180759 Val: 0.7426 Test: 0.6087\n",
      "Epoch: 677, Loss: 0.0923 tsm_loss: 3.8070 reg_loss: 0.0923 N_Y: 407288 N_S: 1754772 N: 407288 N_HV: 175662 Val: 0.7391 Test: 0.5908\n",
      "Epoch: 678, Loss: 0.0991 tsm_loss: 3.9886 reg_loss: 0.0991 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 175508 Val: 0.7448 Test: 0.5979\n",
      "Epoch: 679, Loss: 0.0841 tsm_loss: 3.9471 reg_loss: 0.0841 N_Y: 405471 N_S: 1754772 N: 405471 N_HV: 176231 Val: 0.7398 Test: 0.6134\n",
      "Epoch: 680, Loss: 0.0841 tsm_loss: 3.9895 reg_loss: 0.0841 N_Y: 405066 N_S: 1754772 N: 405066 N_HV: 176613 Val: 0.7634 Test: 0.6259\n",
      "Epoch: 681, Loss: 0.1048 tsm_loss: 4.0140 reg_loss: 0.1048 N_Y: 406034 N_S: 1754772 N: 406034 N_HV: 178699 Val: 0.7376 Test: 0.6216\n",
      "Epoch: 682, Loss: 0.0939 tsm_loss: 3.9957 reg_loss: 0.0939 N_Y: 403376 N_S: 1754772 N: 403376 N_HV: 177061 Val: 0.7536 Test: 0.6432\n",
      "Epoch: 683, Loss: 0.0974 tsm_loss: 4.1179 reg_loss: 0.0974 N_Y: 405731 N_S: 1754772 N: 405731 N_HV: 178417 Val: 0.7509 Test: 0.6157\n",
      "Epoch: 684, Loss: 0.0854 tsm_loss: 4.0519 reg_loss: 0.0854 N_Y: 406268 N_S: 1754772 N: 406268 N_HV: 178301 Val: 0.7391 Test: 0.6225\n",
      "Epoch: 685, Loss: 0.1006 tsm_loss: 4.0837 reg_loss: 0.1006 N_Y: 406911 N_S: 1754772 N: 406911 N_HV: 180784 Val: 0.7332 Test: 0.6122\n",
      "Epoch: 686, Loss: 0.0908 tsm_loss: 3.9511 reg_loss: 0.0908 N_Y: 405630 N_S: 1754772 N: 405630 N_HV: 176389 Val: 0.7249 Test: 0.6224\n",
      "Epoch: 687, Loss: 0.0940 tsm_loss: 4.0327 reg_loss: 0.0940 N_Y: 405527 N_S: 1754772 N: 405527 N_HV: 178428 Val: 0.7403 Test: 0.6259\n",
      "Epoch: 688, Loss: 0.0916 tsm_loss: 4.1204 reg_loss: 0.0916 N_Y: 404888 N_S: 1754772 N: 404888 N_HV: 178866 Val: 0.7293 Test: 0.6291\n",
      "Epoch: 689, Loss: 0.0990 tsm_loss: 3.9283 reg_loss: 0.0990 N_Y: 403462 N_S: 1754772 N: 403462 N_HV: 174588 Val: 0.7216 Test: 0.6116\n",
      "Epoch: 690, Loss: 0.0998 tsm_loss: 3.9328 reg_loss: 0.0998 N_Y: 407068 N_S: 1754772 N: 407068 N_HV: 176975 Val: 0.7399 Test: 0.6177\n",
      "Epoch: 691, Loss: 0.1392 tsm_loss: 3.8945 reg_loss: 0.1392 N_Y: 406168 N_S: 1754772 N: 406168 N_HV: 176137 Val: 0.7532 Test: 0.6532\n",
      "Epoch: 692, Loss: 0.1397 tsm_loss: 3.9456 reg_loss: 0.1397 N_Y: 407010 N_S: 1754772 N: 407010 N_HV: 175397 Val: 0.7228 Test: 0.6198\n",
      "Epoch: 693, Loss: 0.1046 tsm_loss: 4.0890 reg_loss: 0.1046 N_Y: 406498 N_S: 1754772 N: 406498 N_HV: 178108 Val: 0.7341 Test: 0.6248\n",
      "Epoch: 694, Loss: 0.0883 tsm_loss: 4.1069 reg_loss: 0.0883 N_Y: 404873 N_S: 1754772 N: 404873 N_HV: 175871 Val: 0.7216 Test: 0.6198\n",
      "Epoch: 695, Loss: 0.0982 tsm_loss: 4.0151 reg_loss: 0.0982 N_Y: 406057 N_S: 1754772 N: 406057 N_HV: 176236 Val: 0.7300 Test: 0.6192\n",
      "Epoch: 696, Loss: 0.0844 tsm_loss: 4.0138 reg_loss: 0.0844 N_Y: 407390 N_S: 1754772 N: 407390 N_HV: 177606 Val: 0.7455 Test: 0.6071\n",
      "Epoch: 697, Loss: 0.0811 tsm_loss: 4.0762 reg_loss: 0.0811 N_Y: 406793 N_S: 1754772 N: 406793 N_HV: 177558 Val: 0.7337 Test: 0.6090\n",
      "Epoch: 698, Loss: 0.0899 tsm_loss: 4.1140 reg_loss: 0.0899 N_Y: 406583 N_S: 1754772 N: 406583 N_HV: 178742 Val: 0.7308 Test: 0.6101\n",
      "Epoch: 699, Loss: 0.0882 tsm_loss: 4.0155 reg_loss: 0.0882 N_Y: 406948 N_S: 1754772 N: 406948 N_HV: 176404 Val: 0.7720 Test: 0.6334\n",
      "Epoch: 700, Loss: 0.1071 tsm_loss: 4.0347 reg_loss: 0.1071 N_Y: 405411 N_S: 1754772 N: 405411 N_HV: 175766 Val: 0.7296 Test: 0.6141\n",
      "Epoch: 701, Loss: 0.0862 tsm_loss: 4.0153 reg_loss: 0.0862 N_Y: 408080 N_S: 1754772 N: 408080 N_HV: 178444 Val: 0.7336 Test: 0.6228\n",
      "Epoch: 702, Loss: 0.0917 tsm_loss: 4.0540 reg_loss: 0.0917 N_Y: 405694 N_S: 1754772 N: 405694 N_HV: 176312 Val: 0.7383 Test: 0.6141\n",
      "Epoch: 703, Loss: 0.1207 tsm_loss: 4.0269 reg_loss: 0.1207 N_Y: 406248 N_S: 1754772 N: 406248 N_HV: 176878 Val: 0.7765 Test: 0.6365\n",
      "Epoch: 704, Loss: 0.1324 tsm_loss: 3.9402 reg_loss: 0.1324 N_Y: 407122 N_S: 1754772 N: 407122 N_HV: 178532 Val: 0.7470 Test: 0.6280\n",
      "Epoch: 705, Loss: 0.1034 tsm_loss: 3.9256 reg_loss: 0.1034 N_Y: 405983 N_S: 1754772 N: 405983 N_HV: 175457 Val: 0.7410 Test: 0.6200\n",
      "Epoch: 706, Loss: 0.1214 tsm_loss: 4.0305 reg_loss: 0.1214 N_Y: 407325 N_S: 1754772 N: 407325 N_HV: 177405 Val: 0.7361 Test: 0.6053\n",
      "Epoch: 707, Loss: 0.1383 tsm_loss: 4.0054 reg_loss: 0.1383 N_Y: 407436 N_S: 1754772 N: 407436 N_HV: 176947 Val: 0.7479 Test: 0.6464\n",
      "Epoch: 708, Loss: 0.1103 tsm_loss: 3.9591 reg_loss: 0.1103 N_Y: 407833 N_S: 1754772 N: 407833 N_HV: 173988 Val: 0.7197 Test: 0.6047\n",
      "Epoch: 709, Loss: 0.1152 tsm_loss: 4.0756 reg_loss: 0.1152 N_Y: 405810 N_S: 1754772 N: 405810 N_HV: 177643 Val: 0.7373 Test: 0.6102\n",
      "Epoch: 710, Loss: 0.0906 tsm_loss: 3.9348 reg_loss: 0.0906 N_Y: 405034 N_S: 1754772 N: 405034 N_HV: 173577 Val: 0.7397 Test: 0.6103\n",
      "Epoch: 711, Loss: 0.0997 tsm_loss: 3.9409 reg_loss: 0.0997 N_Y: 406360 N_S: 1754772 N: 406360 N_HV: 174824 Val: 0.7302 Test: 0.6092\n",
      "Epoch: 712, Loss: 0.0941 tsm_loss: 3.9541 reg_loss: 0.0941 N_Y: 405408 N_S: 1754772 N: 405408 N_HV: 173093 Val: 0.7262 Test: 0.6040\n",
      "Epoch: 713, Loss: 0.0921 tsm_loss: 4.0397 reg_loss: 0.0921 N_Y: 405743 N_S: 1754772 N: 405743 N_HV: 177492 Val: 0.7411 Test: 0.6033\n",
      "Epoch: 714, Loss: 0.0912 tsm_loss: 4.0302 reg_loss: 0.0912 N_Y: 405743 N_S: 1754772 N: 405743 N_HV: 174835 Val: 0.7416 Test: 0.6228\n",
      "Epoch: 715, Loss: 0.1015 tsm_loss: 3.9676 reg_loss: 0.1015 N_Y: 406704 N_S: 1754772 N: 406704 N_HV: 175784 Val: 0.7567 Test: 0.6275\n",
      "Epoch: 716, Loss: 0.0893 tsm_loss: 3.9707 reg_loss: 0.0893 N_Y: 405715 N_S: 1754772 N: 405715 N_HV: 175251 Val: 0.7370 Test: 0.6204\n",
      "Epoch: 717, Loss: 0.0771 tsm_loss: 3.9586 reg_loss: 0.0771 N_Y: 406163 N_S: 1754772 N: 406163 N_HV: 175596 Val: 0.7384 Test: 0.6189\n",
      "Epoch: 718, Loss: 0.0933 tsm_loss: 3.9432 reg_loss: 0.0933 N_Y: 407154 N_S: 1754772 N: 407154 N_HV: 176621 Val: 0.7255 Test: 0.6070\n",
      "Epoch: 719, Loss: 0.1425 tsm_loss: 3.9188 reg_loss: 0.1425 N_Y: 405944 N_S: 1754772 N: 405944 N_HV: 178648 Val: 0.7383 Test: 0.6066\n",
      "Epoch: 720, Loss: 0.1136 tsm_loss: 3.8884 reg_loss: 0.1136 N_Y: 406495 N_S: 1754772 N: 406495 N_HV: 175695 Val: 0.7576 Test: 0.6435\n",
      "Epoch: 721, Loss: 0.1060 tsm_loss: 3.9259 reg_loss: 0.1060 N_Y: 406417 N_S: 1754772 N: 406417 N_HV: 176875 Val: 0.7371 Test: 0.6190\n",
      "Epoch: 722, Loss: 0.0929 tsm_loss: 3.8714 reg_loss: 0.0929 N_Y: 405644 N_S: 1754772 N: 405644 N_HV: 174409 Val: 0.7411 Test: 0.6461\n",
      "Epoch: 723, Loss: 0.0998 tsm_loss: 3.7963 reg_loss: 0.0998 N_Y: 405242 N_S: 1754772 N: 405242 N_HV: 171929 Val: 0.7394 Test: 0.6164\n",
      "Epoch: 724, Loss: 0.1138 tsm_loss: 3.8327 reg_loss: 0.1138 N_Y: 406776 N_S: 1754772 N: 406776 N_HV: 176660 Val: 0.7415 Test: 0.6170\n",
      "Epoch: 725, Loss: 0.1657 tsm_loss: 3.9349 reg_loss: 0.1657 N_Y: 406388 N_S: 1754772 N: 406388 N_HV: 175818 Val: 0.7532 Test: 0.6802\n",
      "Epoch: 726, Loss: 0.1194 tsm_loss: 3.9687 reg_loss: 0.1194 N_Y: 404889 N_S: 1754772 N: 404889 N_HV: 174331 Val: 0.7228 Test: 0.6375\n",
      "Epoch: 727, Loss: 0.1040 tsm_loss: 3.8924 reg_loss: 0.1040 N_Y: 406033 N_S: 1754772 N: 406033 N_HV: 175475 Val: 0.7398 Test: 0.6234\n",
      "Epoch: 728, Loss: 0.1195 tsm_loss: 3.9164 reg_loss: 0.1195 N_Y: 405327 N_S: 1754772 N: 405327 N_HV: 174477 Val: 0.7343 Test: 0.6309\n",
      "Epoch: 729, Loss: 0.0732 tsm_loss: 4.0010 reg_loss: 0.0732 N_Y: 404687 N_S: 1754772 N: 404687 N_HV: 175384 Val: 0.7360 Test: 0.6136\n",
      "Epoch: 730, Loss: 0.0779 tsm_loss: 4.0043 reg_loss: 0.0779 N_Y: 405917 N_S: 1754772 N: 405917 N_HV: 178592 Val: 0.7494 Test: 0.6156\n",
      "Epoch: 731, Loss: 0.0761 tsm_loss: 3.8959 reg_loss: 0.0761 N_Y: 407023 N_S: 1754772 N: 407023 N_HV: 176445 Val: 0.7371 Test: 0.6026\n",
      "Epoch: 732, Loss: 0.0997 tsm_loss: 3.9696 reg_loss: 0.0997 N_Y: 407644 N_S: 1754772 N: 407644 N_HV: 177219 Val: 0.7380 Test: 0.6041\n",
      "Epoch: 733, Loss: 0.0981 tsm_loss: 3.9789 reg_loss: 0.0981 N_Y: 405568 N_S: 1754772 N: 405568 N_HV: 176956 Val: 0.7522 Test: 0.6156\n",
      "Epoch: 734, Loss: 0.1379 tsm_loss: 4.0039 reg_loss: 0.1379 N_Y: 406363 N_S: 1754772 N: 406363 N_HV: 178452 Val: 0.7771 Test: 0.6442\n",
      "Epoch: 735, Loss: 0.1272 tsm_loss: 3.9431 reg_loss: 0.1272 N_Y: 405804 N_S: 1754772 N: 405804 N_HV: 176214 Val: 0.7579 Test: 0.6885\n",
      "Epoch: 736, Loss: 0.1397 tsm_loss: 4.0173 reg_loss: 0.1397 N_Y: 406007 N_S: 1754772 N: 406007 N_HV: 177446 Val: 0.7080 Test: 0.6309\n",
      "Epoch: 737, Loss: 0.0999 tsm_loss: 3.9902 reg_loss: 0.0999 N_Y: 406315 N_S: 1754772 N: 406315 N_HV: 175414 Val: 0.7362 Test: 0.6637\n",
      "Epoch: 738, Loss: 0.1004 tsm_loss: 3.9811 reg_loss: 0.1004 N_Y: 405894 N_S: 1754772 N: 405894 N_HV: 177224 Val: 0.7367 Test: 0.6268\n",
      "Epoch: 739, Loss: 0.1100 tsm_loss: 3.9807 reg_loss: 0.1100 N_Y: 404599 N_S: 1754772 N: 404599 N_HV: 176090 Val: 0.7318 Test: 0.6375\n",
      "Epoch: 740, Loss: 0.1032 tsm_loss: 3.9050 reg_loss: 0.1032 N_Y: 405301 N_S: 1754772 N: 405301 N_HV: 175104 Val: 0.7452 Test: 0.6420\n",
      "Epoch: 741, Loss: 0.1136 tsm_loss: 3.8990 reg_loss: 0.1136 N_Y: 406428 N_S: 1754772 N: 406428 N_HV: 173144 Val: 0.7387 Test: 0.6109\n",
      "Epoch: 742, Loss: 0.0926 tsm_loss: 3.9957 reg_loss: 0.0926 N_Y: 407289 N_S: 1754772 N: 407289 N_HV: 176872 Val: 0.7515 Test: 0.6182\n",
      "Epoch: 743, Loss: 0.0759 tsm_loss: 3.8795 reg_loss: 0.0759 N_Y: 405857 N_S: 1754772 N: 405857 N_HV: 174938 Val: 0.7436 Test: 0.6155\n",
      "Epoch: 744, Loss: 0.0927 tsm_loss: 3.9218 reg_loss: 0.0927 N_Y: 406988 N_S: 1754772 N: 406988 N_HV: 173844 Val: 0.7454 Test: 0.6430\n",
      "Epoch: 745, Loss: 0.1171 tsm_loss: 3.9635 reg_loss: 0.1171 N_Y: 407010 N_S: 1754772 N: 407010 N_HV: 173523 Val: 0.7318 Test: 0.6406\n",
      "Epoch: 746, Loss: 0.1711 tsm_loss: 4.0163 reg_loss: 0.1711 N_Y: 408010 N_S: 1754772 N: 408010 N_HV: 178945 Val: 0.7364 Test: 0.6084\n",
      "Epoch: 747, Loss: 0.1254 tsm_loss: 3.9417 reg_loss: 0.1254 N_Y: 407316 N_S: 1754772 N: 407316 N_HV: 176146 Val: 0.7257 Test: 0.6114\n",
      "Epoch: 748, Loss: 0.0989 tsm_loss: 4.1697 reg_loss: 0.0989 N_Y: 405755 N_S: 1754772 N: 405755 N_HV: 180347 Val: 0.7377 Test: 0.6111\n",
      "Epoch: 749, Loss: 0.0800 tsm_loss: 4.0487 reg_loss: 0.0800 N_Y: 406518 N_S: 1754772 N: 406518 N_HV: 177324 Val: 0.7470 Test: 0.6404\n",
      "Epoch: 750, Loss: 0.0929 tsm_loss: 4.1171 reg_loss: 0.0929 N_Y: 404972 N_S: 1754772 N: 404972 N_HV: 178915 Val: 0.7448 Test: 0.6169\n",
      "Epoch: 751, Loss: 0.0920 tsm_loss: 4.1137 reg_loss: 0.0920 N_Y: 406586 N_S: 1754772 N: 406586 N_HV: 175977 Val: 0.7454 Test: 0.6184\n",
      "Epoch: 752, Loss: 0.0991 tsm_loss: 4.1725 reg_loss: 0.0991 N_Y: 406791 N_S: 1754772 N: 406791 N_HV: 178579 Val: 0.7292 Test: 0.6370\n",
      "Epoch: 753, Loss: 0.1044 tsm_loss: 4.2097 reg_loss: 0.1044 N_Y: 406385 N_S: 1754772 N: 406385 N_HV: 178641 Val: 0.7299 Test: 0.6183\n",
      "Epoch: 754, Loss: 0.0893 tsm_loss: 4.1519 reg_loss: 0.0893 N_Y: 405379 N_S: 1754772 N: 405379 N_HV: 177179 Val: 0.7450 Test: 0.6181\n",
      "Epoch: 755, Loss: 0.0742 tsm_loss: 4.0360 reg_loss: 0.0742 N_Y: 406239 N_S: 1754772 N: 406239 N_HV: 176948 Val: 0.7496 Test: 0.6234\n",
      "Epoch: 756, Loss: 0.0912 tsm_loss: 4.0913 reg_loss: 0.0912 N_Y: 405908 N_S: 1754772 N: 405908 N_HV: 174831 Val: 0.7533 Test: 0.6287\n",
      "Epoch: 757, Loss: 0.1084 tsm_loss: 4.0361 reg_loss: 0.1084 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 176745 Val: 0.7317 Test: 0.6166\n",
      "Epoch: 758, Loss: 0.1148 tsm_loss: 3.9675 reg_loss: 0.1148 N_Y: 406782 N_S: 1754772 N: 406782 N_HV: 176013 Val: 0.7587 Test: 0.6621\n",
      "Epoch: 759, Loss: 0.1158 tsm_loss: 4.0993 reg_loss: 0.1158 N_Y: 404233 N_S: 1754772 N: 404233 N_HV: 174618 Val: 0.7494 Test: 0.6661\n",
      "Epoch: 760, Loss: 0.1190 tsm_loss: 4.0120 reg_loss: 0.1190 N_Y: 406320 N_S: 1754772 N: 406320 N_HV: 175128 Val: 0.7396 Test: 0.6233\n",
      "Epoch: 761, Loss: 0.0880 tsm_loss: 3.9881 reg_loss: 0.0880 N_Y: 406419 N_S: 1754772 N: 406419 N_HV: 176441 Val: 0.7376 Test: 0.6329\n",
      "Epoch: 762, Loss: 0.1018 tsm_loss: 4.0935 reg_loss: 0.1018 N_Y: 406148 N_S: 1754772 N: 406148 N_HV: 176389 Val: 0.7414 Test: 0.6222\n",
      "Epoch: 763, Loss: 0.0776 tsm_loss: 4.0639 reg_loss: 0.0776 N_Y: 405820 N_S: 1754772 N: 405820 N_HV: 176434 Val: 0.7419 Test: 0.6384\n",
      "Epoch: 764, Loss: 0.0860 tsm_loss: 4.2321 reg_loss: 0.0860 N_Y: 404718 N_S: 1754772 N: 404718 N_HV: 177263 Val: 0.7332 Test: 0.6237\n",
      "Epoch: 765, Loss: 0.0942 tsm_loss: 4.2102 reg_loss: 0.0942 N_Y: 404039 N_S: 1754772 N: 404039 N_HV: 178082 Val: 0.7652 Test: 0.6349\n",
      "Epoch: 766, Loss: 0.1276 tsm_loss: 4.1477 reg_loss: 0.1276 N_Y: 404103 N_S: 1754772 N: 404103 N_HV: 176008 Val: 0.7384 Test: 0.6218\n",
      "Epoch: 767, Loss: 0.1191 tsm_loss: 4.0617 reg_loss: 0.1191 N_Y: 405488 N_S: 1754772 N: 405488 N_HV: 176920 Val: 0.7317 Test: 0.6085\n",
      "Epoch: 768, Loss: 0.0979 tsm_loss: 4.1463 reg_loss: 0.0979 N_Y: 406530 N_S: 1754772 N: 406530 N_HV: 178235 Val: 0.7471 Test: 0.6307\n",
      "Epoch: 769, Loss: 0.1461 tsm_loss: 4.2371 reg_loss: 0.1461 N_Y: 405330 N_S: 1754772 N: 405330 N_HV: 178680 Val: 0.7438 Test: 0.6570\n",
      "Epoch: 770, Loss: 0.1227 tsm_loss: 4.1137 reg_loss: 0.1227 N_Y: 406767 N_S: 1754772 N: 406767 N_HV: 176427 Val: 0.7253 Test: 0.6151\n",
      "Epoch: 771, Loss: 0.0882 tsm_loss: 4.0328 reg_loss: 0.0882 N_Y: 404764 N_S: 1754772 N: 404764 N_HV: 175822 Val: 0.7291 Test: 0.6120\n",
      "Epoch: 772, Loss: 0.0983 tsm_loss: 3.9428 reg_loss: 0.0983 N_Y: 406995 N_S: 1754772 N: 406995 N_HV: 175987 Val: 0.7320 Test: 0.6192\n",
      "Epoch: 773, Loss: 0.0951 tsm_loss: 4.1247 reg_loss: 0.0951 N_Y: 405347 N_S: 1754772 N: 405347 N_HV: 174173 Val: 0.7496 Test: 0.6576\n",
      "Epoch: 774, Loss: 0.0932 tsm_loss: 4.0331 reg_loss: 0.0932 N_Y: 406078 N_S: 1754772 N: 406078 N_HV: 175021 Val: 0.7418 Test: 0.6153\n",
      "Epoch: 775, Loss: 0.0960 tsm_loss: 4.0895 reg_loss: 0.0960 N_Y: 406581 N_S: 1754772 N: 406581 N_HV: 177561 Val: 0.7410 Test: 0.6221\n",
      "Epoch: 776, Loss: 0.0903 tsm_loss: 4.1359 reg_loss: 0.0903 N_Y: 405724 N_S: 1754772 N: 405724 N_HV: 177350 Val: 0.7328 Test: 0.6121\n",
      "Epoch: 777, Loss: 0.1272 tsm_loss: 4.2001 reg_loss: 0.1272 N_Y: 406548 N_S: 1754772 N: 406548 N_HV: 178789 Val: 0.7553 Test: 0.6300\n",
      "Epoch: 778, Loss: 0.1060 tsm_loss: 4.1317 reg_loss: 0.1060 N_Y: 406564 N_S: 1754772 N: 406564 N_HV: 177802 Val: 0.7352 Test: 0.6273\n",
      "Epoch: 779, Loss: 0.0889 tsm_loss: 4.0845 reg_loss: 0.0889 N_Y: 405015 N_S: 1754772 N: 405015 N_HV: 177700 Val: 0.7348 Test: 0.6125\n",
      "Epoch: 780, Loss: 0.0807 tsm_loss: 4.0973 reg_loss: 0.0807 N_Y: 406247 N_S: 1754772 N: 406247 N_HV: 177327 Val: 0.7548 Test: 0.6281\n",
      "Epoch: 781, Loss: 0.0993 tsm_loss: 4.0955 reg_loss: 0.0993 N_Y: 406364 N_S: 1754772 N: 406364 N_HV: 177528 Val: 0.7565 Test: 0.6580\n",
      "Epoch: 782, Loss: 0.0955 tsm_loss: 4.0702 reg_loss: 0.0955 N_Y: 405882 N_S: 1754772 N: 405882 N_HV: 176757 Val: 0.7415 Test: 0.6171\n",
      "Epoch: 783, Loss: 0.1087 tsm_loss: 4.0586 reg_loss: 0.1087 N_Y: 404477 N_S: 1754772 N: 404477 N_HV: 174745 Val: 0.7479 Test: 0.6206\n",
      "Epoch: 784, Loss: 0.0904 tsm_loss: 4.0280 reg_loss: 0.0904 N_Y: 405077 N_S: 1754772 N: 405077 N_HV: 175817 Val: 0.7342 Test: 0.6288\n",
      "Epoch: 785, Loss: 0.1037 tsm_loss: 4.1002 reg_loss: 0.1037 N_Y: 406505 N_S: 1754772 N: 406505 N_HV: 180279 Val: 0.7431 Test: 0.6413\n",
      "Epoch: 786, Loss: 0.0919 tsm_loss: 3.9720 reg_loss: 0.0919 N_Y: 403163 N_S: 1754772 N: 403163 N_HV: 174717 Val: 0.7394 Test: 0.6206\n",
      "Epoch: 787, Loss: 0.0909 tsm_loss: 3.9604 reg_loss: 0.0909 N_Y: 406462 N_S: 1754772 N: 406462 N_HV: 177538 Val: 0.7410 Test: 0.6185\n",
      "Epoch: 788, Loss: 0.0814 tsm_loss: 4.0664 reg_loss: 0.0814 N_Y: 404240 N_S: 1754772 N: 404240 N_HV: 178023 Val: 0.7397 Test: 0.6340\n",
      "Epoch: 789, Loss: 0.0697 tsm_loss: 4.1143 reg_loss: 0.0697 N_Y: 405003 N_S: 1754772 N: 405003 N_HV: 178040 Val: 0.7344 Test: 0.6232\n",
      "Epoch: 790, Loss: 0.0910 tsm_loss: 3.9568 reg_loss: 0.0910 N_Y: 407386 N_S: 1754772 N: 407386 N_HV: 176170 Val: 0.7290 Test: 0.6312\n",
      "Epoch: 791, Loss: 0.0866 tsm_loss: 3.9099 reg_loss: 0.0866 N_Y: 406084 N_S: 1754772 N: 406084 N_HV: 176058 Val: 0.7555 Test: 0.6795\n",
      "Epoch: 792, Loss: 0.1085 tsm_loss: 3.8889 reg_loss: 0.1085 N_Y: 405982 N_S: 1754772 N: 405982 N_HV: 173410 Val: 0.7455 Test: 0.6260\n",
      "Epoch: 793, Loss: 0.0859 tsm_loss: 3.8568 reg_loss: 0.0859 N_Y: 405124 N_S: 1754772 N: 405124 N_HV: 176052 Val: 0.7364 Test: 0.6250\n",
      "Epoch: 794, Loss: 0.0894 tsm_loss: 3.8982 reg_loss: 0.0894 N_Y: 405599 N_S: 1754772 N: 405599 N_HV: 173682 Val: 0.7405 Test: 0.6364\n",
      "Epoch: 795, Loss: 0.0933 tsm_loss: 4.1191 reg_loss: 0.0933 N_Y: 406121 N_S: 1754772 N: 406121 N_HV: 177901 Val: 0.7432 Test: 0.6152\n",
      "Epoch: 796, Loss: 0.0892 tsm_loss: 4.0052 reg_loss: 0.0892 N_Y: 406706 N_S: 1754772 N: 406706 N_HV: 176539 Val: 0.7454 Test: 0.6175\n",
      "Epoch: 797, Loss: 0.0963 tsm_loss: 3.9628 reg_loss: 0.0963 N_Y: 407016 N_S: 1754772 N: 407016 N_HV: 175215 Val: 0.7756 Test: 0.6794\n",
      "Epoch: 798, Loss: 0.1056 tsm_loss: 3.9596 reg_loss: 0.1056 N_Y: 405813 N_S: 1754772 N: 405813 N_HV: 175319 Val: 0.7458 Test: 0.6161\n",
      "Epoch: 799, Loss: 0.0851 tsm_loss: 4.0400 reg_loss: 0.0851 N_Y: 407625 N_S: 1754772 N: 407625 N_HV: 179660 Val: 0.7722 Test: 0.6291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 10.7865 tsm_loss: 4.0381 reg_loss: 6.7484 N_Y: 407843 N_S: 1754772 N: 407843 N_HV: 214331 Val: 6.9623 Test: 6.9829\n",
      "Epoch: 002, Loss: 8.0703 tsm_loss: 1.8523 reg_loss: 6.2180 N_Y: 403186 N_S: 1754772 N: 403186 N_HV: 201470 Val: 6.9560 Test: 6.9767\n",
      "Epoch: 003, Loss: 7.0487 tsm_loss: 1.5025 reg_loss: 5.5462 N_Y: 405580 N_S: 1754772 N: 405580 N_HV: 208808 Val: 6.9394 Test: 6.9604\n",
      "Epoch: 004, Loss: 5.9260 tsm_loss: 1.3178 reg_loss: 4.6082 N_Y: 406000 N_S: 1754772 N: 406000 N_HV: 185513 Val: 6.8928 Test: 6.9144\n",
      "Epoch: 005, Loss: 4.4620 tsm_loss: 1.2371 reg_loss: 3.2250 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 181447 Val: 6.7496 Test: 6.7720\n",
      "Epoch: 006, Loss: 2.6427 tsm_loss: 1.2889 reg_loss: 1.3538 N_Y: 404414 N_S: 1754772 N: 404414 N_HV: 176963 Val: 6.2749 Test: 6.2974\n",
      "Epoch: 007, Loss: 2.0209 tsm_loss: 1.1914 reg_loss: 0.8295 N_Y: 405844 N_S: 1754772 N: 405844 N_HV: 158905 Val: 5.2881 Test: 5.3105\n",
      "Epoch: 008, Loss: 1.8235 tsm_loss: 1.1288 reg_loss: 0.6947 N_Y: 406738 N_S: 1754772 N: 406738 N_HV: 167451 Val: 4.1108 Test: 4.1290\n",
      "Epoch: 009, Loss: 1.6684 tsm_loss: 0.9985 reg_loss: 0.6699 N_Y: 405450 N_S: 1754772 N: 405450 N_HV: 160035 Val: 2.8973 Test: 2.9006\n",
      "Epoch: 010, Loss: 1.5990 tsm_loss: 0.9719 reg_loss: 0.6270 N_Y: 406218 N_S: 1754772 N: 406218 N_HV: 148818 Val: 2.0247 Test: 2.0262\n",
      "Epoch: 011, Loss: 1.4631 tsm_loss: 0.8547 reg_loss: 0.6084 N_Y: 405649 N_S: 1754772 N: 405649 N_HV: 150679 Val: 1.4262 Test: 1.4106\n",
      "Epoch: 012, Loss: 1.4140 tsm_loss: 0.8441 reg_loss: 0.5699 N_Y: 406217 N_S: 1754772 N: 406217 N_HV: 141433 Val: 1.0314 Test: 1.0149\n",
      "Epoch: 013, Loss: 1.4103 tsm_loss: 0.8373 reg_loss: 0.5730 N_Y: 406272 N_S: 1754772 N: 406272 N_HV: 146194 Val: 0.8668 Test: 0.8486\n",
      "Epoch: 014, Loss: 1.2998 tsm_loss: 0.7404 reg_loss: 0.5594 N_Y: 408158 N_S: 1754772 N: 408158 N_HV: 144700 Val: 0.8736 Test: 0.8518\n",
      "Epoch: 015, Loss: 1.2748 tsm_loss: 0.7591 reg_loss: 0.5156 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 134586 Val: 0.7093 Test: 0.7008\n",
      "Epoch: 016, Loss: 1.2459 tsm_loss: 0.7495 reg_loss: 0.4964 N_Y: 404438 N_S: 1754772 N: 404438 N_HV: 125460 Val: 0.7257 Test: 0.7042\n",
      "Epoch: 017, Loss: 1.2548 tsm_loss: 0.7309 reg_loss: 0.5239 N_Y: 406418 N_S: 1754772 N: 406418 N_HV: 143884 Val: 0.7170 Test: 0.6941\n",
      "Epoch: 018, Loss: 1.2016 tsm_loss: 0.7134 reg_loss: 0.4882 N_Y: 404922 N_S: 1754772 N: 404922 N_HV: 128142 Val: 0.6922 Test: 0.6986\n",
      "Epoch: 019, Loss: 1.1796 tsm_loss: 0.6910 reg_loss: 0.4886 N_Y: 405618 N_S: 1754772 N: 405618 N_HV: 133809 Val: 0.7301 Test: 0.7121\n",
      "Epoch: 020, Loss: 1.1104 tsm_loss: 0.6378 reg_loss: 0.4726 N_Y: 407960 N_S: 1754772 N: 407960 N_HV: 123855 Val: 0.6924 Test: 0.6858\n",
      "Epoch: 021, Loss: 1.1028 tsm_loss: 0.6421 reg_loss: 0.4607 N_Y: 405658 N_S: 1754772 N: 405658 N_HV: 122632 Val: 0.7092 Test: 0.7106\n",
      "Epoch: 022, Loss: 1.0267 tsm_loss: 0.5700 reg_loss: 0.4567 N_Y: 405801 N_S: 1754772 N: 405801 N_HV: 114280 Val: 0.6852 Test: 0.6726\n",
      "Epoch: 023, Loss: 1.0256 tsm_loss: 0.5757 reg_loss: 0.4499 N_Y: 406015 N_S: 1754772 N: 406015 N_HV: 112767 Val: 0.6910 Test: 0.7010\n",
      "Epoch: 024, Loss: 1.0189 tsm_loss: 0.5722 reg_loss: 0.4466 N_Y: 407094 N_S: 1754772 N: 407094 N_HV: 116354 Val: 0.6929 Test: 0.7054\n",
      "Epoch: 025, Loss: 0.9529 tsm_loss: 0.5298 reg_loss: 0.4230 N_Y: 406662 N_S: 1754772 N: 406662 N_HV: 112552 Val: 0.6737 Test: 0.6505\n",
      "Epoch: 026, Loss: 0.9172 tsm_loss: 0.5176 reg_loss: 0.3997 N_Y: 407087 N_S: 1754772 N: 407087 N_HV: 108021 Val: 0.7282 Test: 0.7440\n",
      "Epoch: 027, Loss: 0.9862 tsm_loss: 0.5463 reg_loss: 0.4399 N_Y: 407243 N_S: 1754772 N: 407243 N_HV: 106659 Val: 0.7192 Test: 0.6966\n",
      "Epoch: 028, Loss: 0.9389 tsm_loss: 0.5275 reg_loss: 0.4114 N_Y: 406722 N_S: 1754772 N: 406722 N_HV: 110764 Val: 0.7014 Test: 0.6903\n",
      "Epoch: 029, Loss: 0.9079 tsm_loss: 0.5086 reg_loss: 0.3993 N_Y: 403434 N_S: 1754772 N: 403434 N_HV: 111238 Val: 0.7316 Test: 0.6995\n",
      "Epoch: 030, Loss: 0.9385 tsm_loss: 0.5149 reg_loss: 0.4236 N_Y: 405728 N_S: 1754772 N: 405728 N_HV: 109946 Val: 0.6636 Test: 0.6540\n",
      "Epoch: 031, Loss: 0.8828 tsm_loss: 0.4894 reg_loss: 0.3935 N_Y: 405494 N_S: 1754772 N: 405494 N_HV: 103396 Val: 0.7091 Test: 0.6728\n",
      "Epoch: 032, Loss: 0.8328 tsm_loss: 0.4562 reg_loss: 0.3766 N_Y: 407319 N_S: 1754772 N: 407319 N_HV: 100261 Val: 0.6717 Test: 0.6458\n",
      "Epoch: 033, Loss: 0.8243 tsm_loss: 0.4506 reg_loss: 0.3737 N_Y: 404776 N_S: 1754772 N: 404776 N_HV: 101998 Val: 0.7078 Test: 0.7282\n",
      "Epoch: 034, Loss: 0.7771 tsm_loss: 0.4198 reg_loss: 0.3573 N_Y: 406471 N_S: 1754772 N: 406471 N_HV: 97351 Val: 0.6854 Test: 0.6628\n",
      "Epoch: 035, Loss: 0.8102 tsm_loss: 0.4233 reg_loss: 0.3869 N_Y: 405258 N_S: 1754772 N: 405258 N_HV: 98311 Val: 0.6465 Test: 0.6329\n",
      "Epoch: 036, Loss: 0.7710 tsm_loss: 0.4071 reg_loss: 0.3640 N_Y: 403624 N_S: 1754772 N: 403624 N_HV: 95846 Val: 0.6732 Test: 0.6310\n",
      "Epoch: 037, Loss: 0.8231 tsm_loss: 0.4404 reg_loss: 0.3827 N_Y: 406362 N_S: 1754772 N: 406362 N_HV: 103616 Val: 0.6927 Test: 0.6445\n",
      "Epoch: 038, Loss: 0.7625 tsm_loss: 0.4014 reg_loss: 0.3612 N_Y: 406074 N_S: 1754772 N: 406074 N_HV: 93682 Val: 0.6586 Test: 0.6592\n",
      "Epoch: 039, Loss: 0.7327 tsm_loss: 0.3961 reg_loss: 0.3366 N_Y: 406788 N_S: 1754772 N: 406788 N_HV: 92412 Val: 0.6678 Test: 0.6433\n",
      "Epoch: 040, Loss: 0.6995 tsm_loss: 0.3661 reg_loss: 0.3334 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 92770 Val: 0.6750 Test: 0.6319\n",
      "Epoch: 041, Loss: 0.7155 tsm_loss: 0.3600 reg_loss: 0.3556 N_Y: 405878 N_S: 1754772 N: 405878 N_HV: 88713 Val: 0.6764 Test: 0.6489\n",
      "Epoch: 042, Loss: 0.7048 tsm_loss: 0.3474 reg_loss: 0.3573 N_Y: 406512 N_S: 1754772 N: 406512 N_HV: 86736 Val: 0.6655 Test: 0.6761\n",
      "Epoch: 043, Loss: 0.7269 tsm_loss: 0.3421 reg_loss: 0.3848 N_Y: 405416 N_S: 1754772 N: 405416 N_HV: 88037 Val: 0.6710 Test: 0.6417\n",
      "Epoch: 044, Loss: 0.6660 tsm_loss: 0.3301 reg_loss: 0.3360 N_Y: 407704 N_S: 1754772 N: 407704 N_HV: 83391 Val: 0.6486 Test: 0.6300\n",
      "Epoch: 045, Loss: 0.6506 tsm_loss: 0.3039 reg_loss: 0.3467 N_Y: 406899 N_S: 1754772 N: 406899 N_HV: 79780 Val: 0.6407 Test: 0.6284\n",
      "Epoch: 046, Loss: 0.6252 tsm_loss: 0.3145 reg_loss: 0.3107 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 83203 Val: 0.6622 Test: 0.6464\n",
      "Epoch: 047, Loss: 0.5973 tsm_loss: 0.2889 reg_loss: 0.3085 N_Y: 407002 N_S: 1754772 N: 407002 N_HV: 75181 Val: 0.6430 Test: 0.6154\n",
      "Epoch: 048, Loss: 0.6027 tsm_loss: 0.3038 reg_loss: 0.2989 N_Y: 406284 N_S: 1754772 N: 406284 N_HV: 81891 Val: 0.6846 Test: 0.6738\n",
      "Epoch: 049, Loss: 0.5818 tsm_loss: 0.2874 reg_loss: 0.2944 N_Y: 406758 N_S: 1754772 N: 406758 N_HV: 78196 Val: 0.6486 Test: 0.6103\n",
      "Epoch: 050, Loss: 0.6197 tsm_loss: 0.3066 reg_loss: 0.3131 N_Y: 405055 N_S: 1754772 N: 405055 N_HV: 78994 Val: 0.6596 Test: 0.6395\n",
      "Epoch: 051, Loss: 0.6312 tsm_loss: 0.3120 reg_loss: 0.3192 N_Y: 405849 N_S: 1754772 N: 405849 N_HV: 83651 Val: 0.6696 Test: 0.6182\n",
      "Epoch: 052, Loss: 0.5824 tsm_loss: 0.2694 reg_loss: 0.3130 N_Y: 407089 N_S: 1754772 N: 407089 N_HV: 76330 Val: 0.6505 Test: 0.6358\n",
      "Epoch: 053, Loss: 0.5604 tsm_loss: 0.2710 reg_loss: 0.2895 N_Y: 407550 N_S: 1754772 N: 407550 N_HV: 75264 Val: 0.7394 Test: 0.6807\n",
      "Epoch: 054, Loss: 0.5573 tsm_loss: 0.2778 reg_loss: 0.2795 N_Y: 403778 N_S: 1754772 N: 403778 N_HV: 75520 Val: 0.6715 Test: 0.6577\n",
      "Epoch: 055, Loss: 0.6146 tsm_loss: 0.2751 reg_loss: 0.3395 N_Y: 404629 N_S: 1754772 N: 404629 N_HV: 75092 Val: 0.6648 Test: 0.6173\n",
      "Epoch: 056, Loss: 0.5149 tsm_loss: 0.2442 reg_loss: 0.2707 N_Y: 404189 N_S: 1754772 N: 404189 N_HV: 71387 Val: 0.6344 Test: 0.6260\n",
      "Epoch: 057, Loss: 0.5572 tsm_loss: 0.2643 reg_loss: 0.2929 N_Y: 406270 N_S: 1754772 N: 406270 N_HV: 73759 Val: 0.6495 Test: 0.6152\n",
      "Epoch: 058, Loss: 0.5327 tsm_loss: 0.2515 reg_loss: 0.2812 N_Y: 407048 N_S: 1754772 N: 407048 N_HV: 74919 Val: 0.6659 Test: 0.6225\n",
      "Epoch: 059, Loss: 0.5260 tsm_loss: 0.2571 reg_loss: 0.2689 N_Y: 405896 N_S: 1754772 N: 405896 N_HV: 74074 Val: 0.6556 Test: 0.6255\n",
      "Epoch: 060, Loss: 0.5403 tsm_loss: 0.2681 reg_loss: 0.2722 N_Y: 405965 N_S: 1754772 N: 405965 N_HV: 71542 Val: 0.6807 Test: 0.6104\n",
      "Epoch: 061, Loss: 0.5469 tsm_loss: 0.2764 reg_loss: 0.2706 N_Y: 404825 N_S: 1754772 N: 404825 N_HV: 75255 Val: 0.6694 Test: 0.6389\n",
      "Epoch: 062, Loss: 0.5581 tsm_loss: 0.2709 reg_loss: 0.2872 N_Y: 407083 N_S: 1754772 N: 407083 N_HV: 77211 Val: 0.7480 Test: 0.6736\n",
      "Epoch: 063, Loss: 0.5304 tsm_loss: 0.2539 reg_loss: 0.2764 N_Y: 406740 N_S: 1754772 N: 406740 N_HV: 71107 Val: 0.6414 Test: 0.5995\n",
      "Epoch: 064, Loss: 0.5489 tsm_loss: 0.2700 reg_loss: 0.2789 N_Y: 403919 N_S: 1754772 N: 403919 N_HV: 72962 Val: 0.6687 Test: 0.6067\n",
      "Epoch: 065, Loss: 0.5274 tsm_loss: 0.2576 reg_loss: 0.2698 N_Y: 405852 N_S: 1754772 N: 405852 N_HV: 75671 Val: 0.6749 Test: 0.6049\n",
      "Epoch: 066, Loss: 0.5131 tsm_loss: 0.2546 reg_loss: 0.2584 N_Y: 407418 N_S: 1754772 N: 407418 N_HV: 72292 Val: 0.7107 Test: 0.7297\n",
      "Epoch: 067, Loss: 0.5269 tsm_loss: 0.2431 reg_loss: 0.2839 N_Y: 406228 N_S: 1754772 N: 406228 N_HV: 75535 Val: 0.6835 Test: 0.6188\n",
      "Epoch: 068, Loss: 0.5032 tsm_loss: 0.2444 reg_loss: 0.2588 N_Y: 405977 N_S: 1754772 N: 405977 N_HV: 68426 Val: 0.6658 Test: 0.6216\n",
      "Epoch: 069, Loss: 0.5068 tsm_loss: 0.2183 reg_loss: 0.2885 N_Y: 407387 N_S: 1754772 N: 407387 N_HV: 68905 Val: 0.7008 Test: 0.6853\n",
      "Epoch: 070, Loss: 0.4953 tsm_loss: 0.2273 reg_loss: 0.2679 N_Y: 406059 N_S: 1754772 N: 406059 N_HV: 68031 Val: 0.6605 Test: 0.6298\n",
      "Epoch: 071, Loss: 0.5144 tsm_loss: 0.2133 reg_loss: 0.3011 N_Y: 405856 N_S: 1754772 N: 405856 N_HV: 69787 Val: 0.7053 Test: 0.6584\n",
      "Epoch: 072, Loss: 0.4788 tsm_loss: 0.2015 reg_loss: 0.2774 N_Y: 405569 N_S: 1754772 N: 405569 N_HV: 60181 Val: 0.6637 Test: 0.6167\n",
      "Epoch: 073, Loss: 0.5139 tsm_loss: 0.2057 reg_loss: 0.3082 N_Y: 406456 N_S: 1754772 N: 406456 N_HV: 62310 Val: 0.6795 Test: 0.6511\n",
      "Epoch: 074, Loss: 0.4830 tsm_loss: 0.1881 reg_loss: 0.2949 N_Y: 406829 N_S: 1754772 N: 406829 N_HV: 58645 Val: 0.6547 Test: 0.5903\n",
      "Epoch: 075, Loss: 0.4005 tsm_loss: 0.1730 reg_loss: 0.2274 N_Y: 405118 N_S: 1754772 N: 405118 N_HV: 59125 Val: 0.6601 Test: 0.6120\n",
      "Epoch: 076, Loss: 0.4336 tsm_loss: 0.1734 reg_loss: 0.2602 N_Y: 406922 N_S: 1754772 N: 406922 N_HV: 58180 Val: 0.6555 Test: 0.6064\n",
      "Epoch: 077, Loss: 0.3848 tsm_loss: 0.1543 reg_loss: 0.2305 N_Y: 406491 N_S: 1754772 N: 406491 N_HV: 52355 Val: 0.6455 Test: 0.5947\n",
      "Epoch: 078, Loss: 0.4284 tsm_loss: 0.1556 reg_loss: 0.2728 N_Y: 404984 N_S: 1754772 N: 404984 N_HV: 51488 Val: 0.6592 Test: 0.6254\n",
      "Epoch: 079, Loss: 0.4943 tsm_loss: 0.1795 reg_loss: 0.3148 N_Y: 404318 N_S: 1754772 N: 404318 N_HV: 58402 Val: 0.6799 Test: 0.6322\n",
      "Epoch: 080, Loss: 0.4047 tsm_loss: 0.1629 reg_loss: 0.2418 N_Y: 407036 N_S: 1754772 N: 407036 N_HV: 55120 Val: 0.6850 Test: 0.6279\n",
      "Epoch: 081, Loss: 0.4667 tsm_loss: 0.1570 reg_loss: 0.3097 N_Y: 406158 N_S: 1754772 N: 406158 N_HV: 52567 Val: 0.6457 Test: 0.6106\n",
      "Epoch: 082, Loss: 0.3843 tsm_loss: 0.1477 reg_loss: 0.2366 N_Y: 405766 N_S: 1754772 N: 405766 N_HV: 50764 Val: 0.6488 Test: 0.5978\n",
      "Epoch: 083, Loss: 0.3884 tsm_loss: 0.1493 reg_loss: 0.2391 N_Y: 405193 N_S: 1754772 N: 405193 N_HV: 49451 Val: 0.6736 Test: 0.5951\n",
      "Epoch: 084, Loss: 0.3450 tsm_loss: 0.1445 reg_loss: 0.2005 N_Y: 406090 N_S: 1754772 N: 406090 N_HV: 49445 Val: 0.6416 Test: 0.5949\n",
      "Epoch: 085, Loss: 0.3527 tsm_loss: 0.1427 reg_loss: 0.2100 N_Y: 406434 N_S: 1754772 N: 406434 N_HV: 51444 Val: 0.6584 Test: 0.6056\n",
      "Epoch: 086, Loss: 0.3860 tsm_loss: 0.1395 reg_loss: 0.2465 N_Y: 407006 N_S: 1754772 N: 407006 N_HV: 49599 Val: 0.6492 Test: 0.5946\n",
      "Epoch: 087, Loss: 0.3612 tsm_loss: 0.1426 reg_loss: 0.2185 N_Y: 405222 N_S: 1754772 N: 405222 N_HV: 47733 Val: 0.6555 Test: 0.6032\n",
      "Epoch: 088, Loss: 0.3450 tsm_loss: 0.1434 reg_loss: 0.2016 N_Y: 404675 N_S: 1754772 N: 404675 N_HV: 48653 Val: 0.6739 Test: 0.6151\n",
      "Epoch: 089, Loss: 0.3686 tsm_loss: 0.1586 reg_loss: 0.2101 N_Y: 406870 N_S: 1754772 N: 406870 N_HV: 55418 Val: 0.6842 Test: 0.6404\n",
      "Epoch: 090, Loss: 0.3718 tsm_loss: 0.1567 reg_loss: 0.2151 N_Y: 407519 N_S: 1754772 N: 407519 N_HV: 50941 Val: 0.6684 Test: 0.6202\n",
      "Epoch: 091, Loss: 0.3815 tsm_loss: 0.1549 reg_loss: 0.2265 N_Y: 404021 N_S: 1754772 N: 404021 N_HV: 54802 Val: 0.6594 Test: 0.5872\n",
      "Epoch: 092, Loss: 0.3923 tsm_loss: 0.1649 reg_loss: 0.2275 N_Y: 405637 N_S: 1754772 N: 405637 N_HV: 53738 Val: 0.6563 Test: 0.6419\n",
      "Epoch: 093, Loss: 0.4515 tsm_loss: 0.1549 reg_loss: 0.2966 N_Y: 404801 N_S: 1754772 N: 404801 N_HV: 52034 Val: 0.6534 Test: 0.5818\n",
      "Epoch: 094, Loss: 0.3922 tsm_loss: 0.1528 reg_loss: 0.2394 N_Y: 407779 N_S: 1754772 N: 407779 N_HV: 52145 Val: 0.6419 Test: 0.5798\n",
      "Epoch: 095, Loss: 0.3613 tsm_loss: 0.1361 reg_loss: 0.2251 N_Y: 406181 N_S: 1754772 N: 406181 N_HV: 47223 Val: 0.6678 Test: 0.6072\n",
      "Epoch: 096, Loss: 0.3738 tsm_loss: 0.1313 reg_loss: 0.2425 N_Y: 406479 N_S: 1754772 N: 406479 N_HV: 45828 Val: 0.6565 Test: 0.5970\n",
      "Epoch: 097, Loss: 0.3482 tsm_loss: 0.1349 reg_loss: 0.2133 N_Y: 407406 N_S: 1754772 N: 407406 N_HV: 48045 Val: 0.6519 Test: 0.5844\n",
      "Epoch: 098, Loss: 0.4389 tsm_loss: 0.1458 reg_loss: 0.2931 N_Y: 406066 N_S: 1754772 N: 406066 N_HV: 48819 Val: 0.6480 Test: 0.6125\n",
      "Epoch: 099, Loss: 0.4823 tsm_loss: 0.1515 reg_loss: 0.3308 N_Y: 404909 N_S: 1754772 N: 404909 N_HV: 49878 Val: 0.7450 Test: 0.6751\n",
      "Epoch: 100, Loss: 0.3926 tsm_loss: 0.1454 reg_loss: 0.2471 N_Y: 405644 N_S: 1754772 N: 405644 N_HV: 51964 Val: 0.6471 Test: 0.6148\n",
      "Epoch: 101, Loss: 0.3634 tsm_loss: 0.1399 reg_loss: 0.2235 N_Y: 406463 N_S: 1754772 N: 406463 N_HV: 46063 Val: 0.6804 Test: 0.5960\n",
      "Epoch: 102, Loss: 0.3659 tsm_loss: 0.1523 reg_loss: 0.2137 N_Y: 406729 N_S: 1754772 N: 406729 N_HV: 49078 Val: 0.6835 Test: 0.6309\n",
      "Epoch: 103, Loss: 0.3474 tsm_loss: 0.1194 reg_loss: 0.2281 N_Y: 406258 N_S: 1754772 N: 406258 N_HV: 40757 Val: 0.6849 Test: 0.6048\n",
      "Epoch: 104, Loss: 0.3584 tsm_loss: 0.1278 reg_loss: 0.2306 N_Y: 406326 N_S: 1754772 N: 406326 N_HV: 44985 Val: 0.6616 Test: 0.5846\n",
      "Epoch: 105, Loss: 0.3193 tsm_loss: 0.1157 reg_loss: 0.2036 N_Y: 406876 N_S: 1754772 N: 406876 N_HV: 40887 Val: 0.6568 Test: 0.6060\n",
      "Epoch: 106, Loss: 0.3360 tsm_loss: 0.1263 reg_loss: 0.2097 N_Y: 406131 N_S: 1754772 N: 406131 N_HV: 43179 Val: 0.6781 Test: 0.6003\n",
      "Epoch: 107, Loss: 0.3023 tsm_loss: 0.1187 reg_loss: 0.1836 N_Y: 406707 N_S: 1754772 N: 406707 N_HV: 44305 Val: 0.6649 Test: 0.5961\n",
      "Epoch: 108, Loss: 0.3129 tsm_loss: 0.1113 reg_loss: 0.2016 N_Y: 406186 N_S: 1754772 N: 406186 N_HV: 40540 Val: 0.6530 Test: 0.5905\n",
      "Epoch: 109, Loss: 0.2701 tsm_loss: 0.0949 reg_loss: 0.1753 N_Y: 405316 N_S: 1754772 N: 405316 N_HV: 36203 Val: 0.6489 Test: 0.5812\n",
      "Epoch: 110, Loss: 0.2666 tsm_loss: 0.0985 reg_loss: 0.1681 N_Y: 404106 N_S: 1754772 N: 404106 N_HV: 35865 Val: 0.6657 Test: 0.5912\n",
      "Epoch: 111, Loss: 0.2754 tsm_loss: 0.1020 reg_loss: 0.1735 N_Y: 406924 N_S: 1754772 N: 406924 N_HV: 38501 Val: 0.6590 Test: 0.5873\n",
      "Epoch: 112, Loss: 0.2827 tsm_loss: 0.1055 reg_loss: 0.1773 N_Y: 405292 N_S: 1754772 N: 405292 N_HV: 38755 Val: 0.6832 Test: 0.6233\n",
      "Epoch: 113, Loss: 0.3055 tsm_loss: 0.1187 reg_loss: 0.1869 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 41477 Val: 0.6686 Test: 0.5897\n",
      "Epoch: 114, Loss: 0.3399 tsm_loss: 0.1238 reg_loss: 0.2161 N_Y: 407354 N_S: 1754772 N: 407354 N_HV: 44494 Val: 0.6829 Test: 0.6134\n",
      "Epoch: 115, Loss: 0.3313 tsm_loss: 0.1069 reg_loss: 0.2244 N_Y: 404537 N_S: 1754772 N: 404537 N_HV: 39077 Val: 0.6523 Test: 0.5904\n",
      "Epoch: 116, Loss: 0.3059 tsm_loss: 0.1106 reg_loss: 0.1953 N_Y: 405245 N_S: 1754772 N: 405245 N_HV: 39094 Val: 0.6799 Test: 0.6055\n",
      "Epoch: 117, Loss: 0.3168 tsm_loss: 0.1116 reg_loss: 0.2051 N_Y: 403369 N_S: 1754772 N: 403369 N_HV: 41501 Val: 0.6464 Test: 0.5963\n",
      "Epoch: 118, Loss: 0.3204 tsm_loss: 0.1125 reg_loss: 0.2079 N_Y: 406939 N_S: 1754772 N: 406939 N_HV: 39112 Val: 0.6631 Test: 0.5950\n",
      "Epoch: 119, Loss: 0.2969 tsm_loss: 0.1001 reg_loss: 0.1968 N_Y: 405775 N_S: 1754772 N: 405775 N_HV: 35871 Val: 0.6602 Test: 0.5724\n",
      "Epoch: 120, Loss: 0.2892 tsm_loss: 0.1109 reg_loss: 0.1783 N_Y: 405718 N_S: 1754772 N: 405718 N_HV: 42363 Val: 0.6445 Test: 0.5914\n",
      "Epoch: 121, Loss: 0.2719 tsm_loss: 0.1067 reg_loss: 0.1652 N_Y: 407439 N_S: 1754772 N: 407439 N_HV: 38170 Val: 0.6405 Test: 0.6025\n",
      "Epoch: 122, Loss: 0.2666 tsm_loss: 0.0929 reg_loss: 0.1736 N_Y: 406141 N_S: 1754772 N: 406141 N_HV: 36318 Val: 0.6547 Test: 0.6130\n",
      "Epoch: 123, Loss: 0.2611 tsm_loss: 0.0900 reg_loss: 0.1710 N_Y: 406872 N_S: 1754772 N: 406872 N_HV: 37446 Val: 0.6351 Test: 0.5719\n",
      "Epoch: 124, Loss: 0.2692 tsm_loss: 0.0896 reg_loss: 0.1796 N_Y: 407102 N_S: 1754772 N: 407102 N_HV: 34239 Val: 0.6706 Test: 0.6242\n",
      "Epoch: 125, Loss: 0.2811 tsm_loss: 0.0889 reg_loss: 0.1922 N_Y: 406330 N_S: 1754772 N: 406330 N_HV: 35029 Val: 0.6503 Test: 0.5783\n",
      "Epoch: 126, Loss: 0.3205 tsm_loss: 0.0971 reg_loss: 0.2233 N_Y: 404927 N_S: 1754772 N: 404927 N_HV: 37648 Val: 0.6655 Test: 0.5757\n",
      "Epoch: 127, Loss: 0.2647 tsm_loss: 0.0920 reg_loss: 0.1728 N_Y: 406536 N_S: 1754772 N: 406536 N_HV: 34575 Val: 0.6566 Test: 0.6158\n",
      "Epoch: 128, Loss: 0.2744 tsm_loss: 0.0917 reg_loss: 0.1826 N_Y: 406174 N_S: 1754772 N: 406174 N_HV: 34609 Val: 0.6495 Test: 0.5914\n",
      "Epoch: 129, Loss: 0.2647 tsm_loss: 0.0922 reg_loss: 0.1726 N_Y: 405806 N_S: 1754772 N: 405806 N_HV: 35067 Val: 0.6336 Test: 0.5637\n",
      "Epoch: 130, Loss: 0.2854 tsm_loss: 0.0880 reg_loss: 0.1974 N_Y: 406162 N_S: 1754772 N: 406162 N_HV: 33152 Val: 0.6516 Test: 0.5949\n",
      "Epoch: 131, Loss: 0.2790 tsm_loss: 0.0985 reg_loss: 0.1805 N_Y: 406333 N_S: 1754772 N: 406333 N_HV: 36978 Val: 0.6505 Test: 0.5960\n",
      "Epoch: 132, Loss: 0.2461 tsm_loss: 0.0891 reg_loss: 0.1570 N_Y: 406173 N_S: 1754772 N: 406173 N_HV: 33785 Val: 0.6744 Test: 0.6024\n",
      "Epoch: 133, Loss: 0.2982 tsm_loss: 0.0987 reg_loss: 0.1994 N_Y: 406469 N_S: 1754772 N: 406469 N_HV: 35298 Val: 0.6466 Test: 0.5882\n",
      "Epoch: 134, Loss: 0.3127 tsm_loss: 0.0955 reg_loss: 0.2172 N_Y: 407921 N_S: 1754772 N: 407921 N_HV: 35785 Val: 0.6405 Test: 0.5931\n",
      "Epoch: 135, Loss: 0.2541 tsm_loss: 0.0936 reg_loss: 0.1605 N_Y: 406910 N_S: 1754772 N: 406910 N_HV: 34227 Val: 0.6608 Test: 0.5835\n",
      "Epoch: 136, Loss: 0.2489 tsm_loss: 0.0853 reg_loss: 0.1637 N_Y: 405676 N_S: 1754772 N: 405676 N_HV: 33438 Val: 0.6437 Test: 0.5845\n",
      "Epoch: 137, Loss: 0.2329 tsm_loss: 0.0864 reg_loss: 0.1466 N_Y: 405759 N_S: 1754772 N: 405759 N_HV: 33122 Val: 0.6546 Test: 0.5951\n",
      "Epoch: 138, Loss: 0.2485 tsm_loss: 0.0887 reg_loss: 0.1598 N_Y: 405236 N_S: 1754772 N: 405236 N_HV: 34034 Val: 0.6420 Test: 0.5863\n",
      "Epoch: 139, Loss: 0.2702 tsm_loss: 0.0964 reg_loss: 0.1738 N_Y: 406129 N_S: 1754772 N: 406129 N_HV: 34478 Val: 0.6568 Test: 0.6158\n",
      "Epoch: 140, Loss: 0.2961 tsm_loss: 0.0833 reg_loss: 0.2128 N_Y: 404875 N_S: 1754772 N: 404875 N_HV: 33012 Val: 0.6556 Test: 0.5809\n",
      "Epoch: 141, Loss: 0.2730 tsm_loss: 0.0903 reg_loss: 0.1827 N_Y: 405828 N_S: 1754772 N: 405828 N_HV: 33257 Val: 0.6550 Test: 0.6066\n",
      "Epoch: 142, Loss: 0.3021 tsm_loss: 0.0983 reg_loss: 0.2038 N_Y: 404139 N_S: 1754772 N: 404139 N_HV: 36015 Val: 0.6543 Test: 0.5809\n",
      "Epoch: 143, Loss: 0.2502 tsm_loss: 0.0853 reg_loss: 0.1649 N_Y: 406562 N_S: 1754772 N: 406562 N_HV: 33253 Val: 0.6701 Test: 0.6013\n",
      "Epoch: 144, Loss: 0.3069 tsm_loss: 0.0846 reg_loss: 0.2223 N_Y: 406577 N_S: 1754772 N: 406577 N_HV: 33902 Val: 0.6571 Test: 0.6180\n",
      "Epoch: 145, Loss: 0.2909 tsm_loss: 0.0856 reg_loss: 0.2053 N_Y: 406681 N_S: 1754772 N: 406681 N_HV: 34008 Val: 0.6623 Test: 0.5888\n",
      "Epoch: 146, Loss: 0.3025 tsm_loss: 0.0950 reg_loss: 0.2075 N_Y: 406289 N_S: 1754772 N: 406289 N_HV: 35938 Val: 0.6577 Test: 0.5791\n",
      "Epoch: 147, Loss: 0.2926 tsm_loss: 0.0920 reg_loss: 0.2006 N_Y: 406057 N_S: 1754772 N: 406057 N_HV: 35625 Val: 0.6544 Test: 0.5863\n",
      "Epoch: 148, Loss: 0.2749 tsm_loss: 0.0833 reg_loss: 0.1916 N_Y: 405415 N_S: 1754772 N: 405415 N_HV: 32245 Val: 0.6467 Test: 0.5828\n",
      "Epoch: 149, Loss: 0.3081 tsm_loss: 0.0926 reg_loss: 0.2155 N_Y: 405036 N_S: 1754772 N: 405036 N_HV: 35426 Val: 0.6372 Test: 0.5969\n",
      "Epoch: 150, Loss: 0.2520 tsm_loss: 0.0807 reg_loss: 0.1713 N_Y: 407155 N_S: 1754772 N: 407155 N_HV: 29511 Val: 0.6456 Test: 0.5893\n",
      "Epoch: 151, Loss: 0.2392 tsm_loss: 0.0876 reg_loss: 0.1516 N_Y: 406000 N_S: 1754772 N: 406000 N_HV: 33621 Val: 0.6597 Test: 0.5896\n",
      "Epoch: 152, Loss: 0.2226 tsm_loss: 0.0699 reg_loss: 0.1527 N_Y: 407945 N_S: 1754772 N: 407945 N_HV: 28410 Val: 0.6686 Test: 0.6004\n",
      "Epoch: 153, Loss: 0.2067 tsm_loss: 0.0712 reg_loss: 0.1356 N_Y: 406630 N_S: 1754772 N: 406630 N_HV: 28475 Val: 0.6369 Test: 0.5811\n",
      "Epoch: 154, Loss: 0.1940 tsm_loss: 0.0621 reg_loss: 0.1319 N_Y: 406408 N_S: 1754772 N: 406408 N_HV: 26895 Val: 0.6605 Test: 0.6017\n",
      "Epoch: 155, Loss: 0.2284 tsm_loss: 0.0686 reg_loss: 0.1598 N_Y: 407058 N_S: 1754772 N: 407058 N_HV: 27546 Val: 0.6552 Test: 0.5899\n",
      "Epoch: 156, Loss: 0.2399 tsm_loss: 0.0735 reg_loss: 0.1664 N_Y: 403853 N_S: 1754772 N: 403853 N_HV: 29756 Val: 0.6453 Test: 0.5739\n",
      "Epoch: 157, Loss: 0.2445 tsm_loss: 0.0648 reg_loss: 0.1797 N_Y: 406257 N_S: 1754772 N: 406257 N_HV: 27525 Val: 0.6631 Test: 0.5890\n",
      "Epoch: 158, Loss: 0.2944 tsm_loss: 0.0669 reg_loss: 0.2275 N_Y: 405295 N_S: 1754772 N: 405295 N_HV: 27932 Val: 0.6634 Test: 0.5990\n",
      "Epoch: 159, Loss: 0.2568 tsm_loss: 0.0604 reg_loss: 0.1964 N_Y: 405908 N_S: 1754772 N: 405908 N_HV: 26410 Val: 0.6721 Test: 0.6179\n",
      "Epoch: 160, Loss: 0.2683 tsm_loss: 0.0673 reg_loss: 0.2010 N_Y: 406271 N_S: 1754772 N: 406271 N_HV: 29448 Val: 0.6317 Test: 0.5751\n",
      "Epoch: 161, Loss: 0.2387 tsm_loss: 0.0630 reg_loss: 0.1757 N_Y: 407799 N_S: 1754772 N: 407799 N_HV: 24375 Val: 0.6603 Test: 0.5984\n",
      "Epoch: 162, Loss: 0.2299 tsm_loss: 0.0699 reg_loss: 0.1600 N_Y: 406273 N_S: 1754772 N: 406273 N_HV: 28176 Val: 0.6615 Test: 0.5925\n",
      "Epoch: 163, Loss: 0.2381 tsm_loss: 0.0686 reg_loss: 0.1695 N_Y: 407078 N_S: 1754772 N: 407078 N_HV: 27550 Val: 0.6536 Test: 0.5833\n",
      "Epoch: 164, Loss: 0.2501 tsm_loss: 0.0639 reg_loss: 0.1862 N_Y: 407295 N_S: 1754772 N: 407295 N_HV: 28043 Val: 0.6504 Test: 0.5723\n",
      "Epoch: 165, Loss: 0.2609 tsm_loss: 0.0707 reg_loss: 0.1902 N_Y: 406269 N_S: 1754772 N: 406269 N_HV: 27172 Val: 0.6517 Test: 0.5960\n",
      "Epoch: 166, Loss: 0.2330 tsm_loss: 0.0724 reg_loss: 0.1606 N_Y: 406299 N_S: 1754772 N: 406299 N_HV: 31783 Val: 0.6640 Test: 0.5889\n",
      "Epoch: 167, Loss: 0.2618 tsm_loss: 0.0726 reg_loss: 0.1892 N_Y: 405672 N_S: 1754772 N: 405672 N_HV: 30260 Val: 0.6709 Test: 0.6111\n",
      "Epoch: 168, Loss: 0.2269 tsm_loss: 0.0687 reg_loss: 0.1582 N_Y: 407344 N_S: 1754772 N: 407344 N_HV: 28920 Val: 0.6618 Test: 0.5804\n",
      "Epoch: 169, Loss: 0.2154 tsm_loss: 0.0673 reg_loss: 0.1481 N_Y: 406736 N_S: 1754772 N: 406736 N_HV: 26078 Val: 0.6589 Test: 0.5742\n",
      "Epoch: 170, Loss: 0.1926 tsm_loss: 0.0653 reg_loss: 0.1273 N_Y: 405863 N_S: 1754772 N: 405863 N_HV: 26643 Val: 0.6596 Test: 0.5751\n",
      "Epoch: 171, Loss: 0.2568 tsm_loss: 0.0641 reg_loss: 0.1927 N_Y: 407109 N_S: 1754772 N: 407109 N_HV: 28359 Val: 0.6583 Test: 0.5948\n",
      "Epoch: 172, Loss: 0.2177 tsm_loss: 0.0648 reg_loss: 0.1528 N_Y: 406730 N_S: 1754772 N: 406730 N_HV: 25062 Val: 0.6547 Test: 0.5940\n",
      "Epoch: 173, Loss: 0.2642 tsm_loss: 0.0648 reg_loss: 0.1995 N_Y: 406542 N_S: 1754772 N: 406542 N_HV: 26135 Val: 0.6714 Test: 0.5941\n",
      "Epoch: 174, Loss: 0.2701 tsm_loss: 0.0628 reg_loss: 0.2073 N_Y: 403669 N_S: 1754772 N: 403669 N_HV: 26908 Val: 0.6459 Test: 0.5783\n",
      "Epoch: 175, Loss: 0.2485 tsm_loss: 0.0654 reg_loss: 0.1831 N_Y: 407396 N_S: 1754772 N: 407396 N_HV: 26383 Val: 0.6542 Test: 0.5787\n",
      "Epoch: 176, Loss: 0.2246 tsm_loss: 0.0562 reg_loss: 0.1684 N_Y: 406142 N_S: 1754772 N: 406142 N_HV: 24496 Val: 0.6610 Test: 0.5810\n",
      "Epoch: 177, Loss: 0.1903 tsm_loss: 0.0549 reg_loss: 0.1354 N_Y: 405266 N_S: 1754772 N: 405266 N_HV: 24131 Val: 0.6609 Test: 0.5933\n",
      "Epoch: 178, Loss: 0.1702 tsm_loss: 0.0498 reg_loss: 0.1203 N_Y: 406765 N_S: 1754772 N: 406765 N_HV: 22590 Val: 0.6567 Test: 0.5865\n",
      "Epoch: 179, Loss: 0.1847 tsm_loss: 0.0522 reg_loss: 0.1325 N_Y: 406957 N_S: 1754772 N: 406957 N_HV: 23372 Val: 0.6503 Test: 0.5823\n",
      "Epoch: 180, Loss: 0.1997 tsm_loss: 0.0548 reg_loss: 0.1449 N_Y: 404782 N_S: 1754772 N: 404782 N_HV: 24187 Val: 0.6665 Test: 0.5845\n",
      "Epoch: 181, Loss: 0.2093 tsm_loss: 0.0502 reg_loss: 0.1591 N_Y: 405579 N_S: 1754772 N: 405579 N_HV: 21410 Val: 0.6655 Test: 0.6142\n",
      "Epoch: 182, Loss: 0.3095 tsm_loss: 0.0623 reg_loss: 0.2472 N_Y: 406194 N_S: 1754772 N: 406194 N_HV: 26786 Val: 0.6460 Test: 0.5729\n",
      "Epoch: 183, Loss: 0.2888 tsm_loss: 0.0675 reg_loss: 0.2212 N_Y: 405616 N_S: 1754772 N: 405616 N_HV: 26512 Val: 0.6302 Test: 0.5851\n",
      "Epoch: 184, Loss: 0.2362 tsm_loss: 0.0647 reg_loss: 0.1715 N_Y: 407099 N_S: 1754772 N: 407099 N_HV: 26462 Val: 0.6640 Test: 0.5893\n",
      "Epoch: 185, Loss: 0.2366 tsm_loss: 0.0648 reg_loss: 0.1718 N_Y: 405148 N_S: 1754772 N: 405148 N_HV: 27802 Val: 0.6579 Test: 0.5871\n",
      "Epoch: 186, Loss: 0.2232 tsm_loss: 0.0720 reg_loss: 0.1512 N_Y: 405871 N_S: 1754772 N: 405871 N_HV: 29783 Val: 0.6601 Test: 0.5848\n",
      "Epoch: 187, Loss: 0.2229 tsm_loss: 0.0588 reg_loss: 0.1641 N_Y: 406539 N_S: 1754772 N: 406539 N_HV: 24572 Val: 0.6367 Test: 0.5883\n",
      "Epoch: 188, Loss: 0.2314 tsm_loss: 0.0606 reg_loss: 0.1708 N_Y: 404984 N_S: 1754772 N: 404984 N_HV: 25178 Val: 0.6507 Test: 0.5855\n",
      "Epoch: 189, Loss: 0.2415 tsm_loss: 0.0619 reg_loss: 0.1796 N_Y: 406673 N_S: 1754772 N: 406673 N_HV: 25026 Val: 0.6514 Test: 0.5835\n",
      "Epoch: 190, Loss: 0.1747 tsm_loss: 0.0534 reg_loss: 0.1213 N_Y: 405937 N_S: 1754772 N: 405937 N_HV: 23992 Val: 0.6566 Test: 0.5835\n",
      "Epoch: 191, Loss: 0.1875 tsm_loss: 0.0510 reg_loss: 0.1365 N_Y: 405366 N_S: 1754772 N: 405366 N_HV: 21793 Val: 0.6829 Test: 0.6037\n",
      "Epoch: 192, Loss: 0.2071 tsm_loss: 0.0600 reg_loss: 0.1471 N_Y: 406747 N_S: 1754772 N: 406747 N_HV: 25664 Val: 0.6591 Test: 0.5700\n",
      "Epoch: 193, Loss: 0.1831 tsm_loss: 0.0507 reg_loss: 0.1325 N_Y: 405199 N_S: 1754772 N: 405199 N_HV: 22080 Val: 0.6551 Test: 0.5719\n",
      "Epoch: 194, Loss: 0.1639 tsm_loss: 0.0525 reg_loss: 0.1114 N_Y: 407539 N_S: 1754772 N: 407539 N_HV: 22819 Val: 0.6548 Test: 0.5829\n",
      "Epoch: 195, Loss: 0.1703 tsm_loss: 0.0534 reg_loss: 0.1169 N_Y: 406072 N_S: 1754772 N: 406072 N_HV: 24127 Val: 0.6645 Test: 0.6079\n",
      "Epoch: 196, Loss: 0.1864 tsm_loss: 0.0507 reg_loss: 0.1357 N_Y: 406493 N_S: 1754772 N: 406493 N_HV: 22054 Val: 0.6583 Test: 0.5741\n",
      "Epoch: 197, Loss: 0.2188 tsm_loss: 0.0548 reg_loss: 0.1640 N_Y: 406052 N_S: 1754772 N: 406052 N_HV: 24361 Val: 0.6538 Test: 0.5875\n",
      "Epoch: 198, Loss: 0.2104 tsm_loss: 0.0517 reg_loss: 0.1587 N_Y: 405360 N_S: 1754772 N: 405360 N_HV: 22233 Val: 0.6514 Test: 0.5755\n",
      "Epoch: 199, Loss: 0.1853 tsm_loss: 0.0534 reg_loss: 0.1319 N_Y: 405635 N_S: 1754772 N: 405635 N_HV: 23642 Val: 0.6477 Test: 0.5807\n",
      "Epoch: 200, Loss: 0.1929 tsm_loss: 0.0502 reg_loss: 0.1427 N_Y: 407098 N_S: 1754772 N: 407098 N_HV: 23289 Val: 0.6515 Test: 0.5835\n",
      "Epoch: 201, Loss: 0.1615 tsm_loss: 0.0459 reg_loss: 0.1157 N_Y: 405911 N_S: 1754772 N: 405911 N_HV: 20870 Val: 0.6452 Test: 0.5689\n",
      "Epoch: 202, Loss: 0.1652 tsm_loss: 0.0414 reg_loss: 0.1238 N_Y: 404144 N_S: 1754772 N: 404144 N_HV: 19262 Val: 0.6498 Test: 0.5955\n",
      "Epoch: 203, Loss: 0.1497 tsm_loss: 0.0425 reg_loss: 0.1071 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 20070 Val: 0.6548 Test: 0.5855\n",
      "Epoch: 204, Loss: 0.1484 tsm_loss: 0.0448 reg_loss: 0.1036 N_Y: 405318 N_S: 1754772 N: 405318 N_HV: 20159 Val: 0.6451 Test: 0.5825\n",
      "Epoch: 205, Loss: 0.2066 tsm_loss: 0.0457 reg_loss: 0.1609 N_Y: 406470 N_S: 1754772 N: 406470 N_HV: 20580 Val: 0.6439 Test: 0.5798\n",
      "Epoch: 206, Loss: 0.1901 tsm_loss: 0.0539 reg_loss: 0.1363 N_Y: 405566 N_S: 1754772 N: 405566 N_HV: 23270 Val: 0.6740 Test: 0.6259\n",
      "Epoch: 207, Loss: 0.2187 tsm_loss: 0.0531 reg_loss: 0.1656 N_Y: 405253 N_S: 1754772 N: 405253 N_HV: 23439 Val: 0.6528 Test: 0.5731\n",
      "Epoch: 208, Loss: 0.2037 tsm_loss: 0.0575 reg_loss: 0.1462 N_Y: 405789 N_S: 1754772 N: 405789 N_HV: 24003 Val: 0.6510 Test: 0.5806\n",
      "Epoch: 209, Loss: 0.1966 tsm_loss: 0.0473 reg_loss: 0.1494 N_Y: 406144 N_S: 1754772 N: 406144 N_HV: 21250 Val: 0.6408 Test: 0.5819\n",
      "Epoch: 210, Loss: 0.1916 tsm_loss: 0.0494 reg_loss: 0.1422 N_Y: 403127 N_S: 1754772 N: 403127 N_HV: 22390 Val: 0.6511 Test: 0.5736\n",
      "Epoch: 211, Loss: 0.2097 tsm_loss: 0.0416 reg_loss: 0.1681 N_Y: 407135 N_S: 1754772 N: 407135 N_HV: 18628 Val: 0.6539 Test: 0.5822\n",
      "Epoch: 212, Loss: 0.1774 tsm_loss: 0.0445 reg_loss: 0.1328 N_Y: 406901 N_S: 1754772 N: 406901 N_HV: 20029 Val: 0.6415 Test: 0.5967\n",
      "Epoch: 213, Loss: 0.1611 tsm_loss: 0.0438 reg_loss: 0.1173 N_Y: 405890 N_S: 1754772 N: 405890 N_HV: 20637 Val: 0.6452 Test: 0.5871\n",
      "Epoch: 214, Loss: 0.1914 tsm_loss: 0.0481 reg_loss: 0.1433 N_Y: 406296 N_S: 1754772 N: 406296 N_HV: 21283 Val: 0.6512 Test: 0.5898\n",
      "Epoch: 215, Loss: 0.1603 tsm_loss: 0.0483 reg_loss: 0.1120 N_Y: 403586 N_S: 1754772 N: 403586 N_HV: 21065 Val: 0.6348 Test: 0.5712\n",
      "Epoch: 216, Loss: 0.1496 tsm_loss: 0.0442 reg_loss: 0.1054 N_Y: 406639 N_S: 1754772 N: 406639 N_HV: 20138 Val: 0.6373 Test: 0.5830\n",
      "Epoch: 217, Loss: 0.1671 tsm_loss: 0.0421 reg_loss: 0.1250 N_Y: 406361 N_S: 1754772 N: 406361 N_HV: 19911 Val: 0.6476 Test: 0.5695\n",
      "Epoch: 218, Loss: 0.1870 tsm_loss: 0.0367 reg_loss: 0.1503 N_Y: 406864 N_S: 1754772 N: 406864 N_HV: 17553 Val: 0.6737 Test: 0.6185\n",
      "Epoch: 219, Loss: 0.1961 tsm_loss: 0.0445 reg_loss: 0.1515 N_Y: 406069 N_S: 1754772 N: 406069 N_HV: 19317 Val: 0.6460 Test: 0.5880\n",
      "Epoch: 220, Loss: 0.2303 tsm_loss: 0.0451 reg_loss: 0.1853 N_Y: 406819 N_S: 1754772 N: 406819 N_HV: 19809 Val: 0.6509 Test: 0.5785\n",
      "Epoch: 221, Loss: 0.2057 tsm_loss: 0.0452 reg_loss: 0.1606 N_Y: 406537 N_S: 1754772 N: 406537 N_HV: 21257 Val: 0.6496 Test: 0.5972\n",
      "Epoch: 222, Loss: 0.1638 tsm_loss: 0.0450 reg_loss: 0.1188 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 19833 Val: 0.6464 Test: 0.5700\n",
      "Epoch: 223, Loss: 0.1754 tsm_loss: 0.0444 reg_loss: 0.1309 N_Y: 405589 N_S: 1754772 N: 405589 N_HV: 19510 Val: 0.6530 Test: 0.5891\n",
      "Epoch: 224, Loss: 0.1674 tsm_loss: 0.0429 reg_loss: 0.1244 N_Y: 406410 N_S: 1754772 N: 406410 N_HV: 20103 Val: 0.6437 Test: 0.5743\n",
      "Epoch: 225, Loss: 0.1526 tsm_loss: 0.0393 reg_loss: 0.1133 N_Y: 405524 N_S: 1754772 N: 405524 N_HV: 18636 Val: 0.6588 Test: 0.6031\n",
      "Epoch: 226, Loss: 0.1608 tsm_loss: 0.0397 reg_loss: 0.1212 N_Y: 406650 N_S: 1754772 N: 406650 N_HV: 18516 Val: 0.6456 Test: 0.5976\n",
      "Epoch: 227, Loss: 0.1671 tsm_loss: 0.0448 reg_loss: 0.1224 N_Y: 405631 N_S: 1754772 N: 405631 N_HV: 20298 Val: 0.6388 Test: 0.5822\n",
      "Epoch: 228, Loss: 0.1716 tsm_loss: 0.0442 reg_loss: 0.1275 N_Y: 406719 N_S: 1754772 N: 406719 N_HV: 20402 Val: 0.6420 Test: 0.5866\n",
      "Epoch: 229, Loss: 0.1612 tsm_loss: 0.0465 reg_loss: 0.1147 N_Y: 406502 N_S: 1754772 N: 406502 N_HV: 21072 Val: 0.6418 Test: 0.5927\n",
      "Epoch: 230, Loss: 0.1852 tsm_loss: 0.0457 reg_loss: 0.1395 N_Y: 406822 N_S: 1754772 N: 406822 N_HV: 21127 Val: 0.6457 Test: 0.5745\n",
      "Epoch: 231, Loss: 0.1880 tsm_loss: 0.0501 reg_loss: 0.1379 N_Y: 406432 N_S: 1754772 N: 406432 N_HV: 22096 Val: 0.6505 Test: 0.5892\n",
      "Epoch: 232, Loss: 0.1808 tsm_loss: 0.0533 reg_loss: 0.1275 N_Y: 406902 N_S: 1754772 N: 406902 N_HV: 22482 Val: 0.6379 Test: 0.5832\n",
      "Epoch: 233, Loss: 0.1614 tsm_loss: 0.0481 reg_loss: 0.1134 N_Y: 406744 N_S: 1754772 N: 406744 N_HV: 20677 Val: 0.6477 Test: 0.5992\n",
      "Epoch: 234, Loss: 0.1704 tsm_loss: 0.0452 reg_loss: 0.1251 N_Y: 405061 N_S: 1754772 N: 405061 N_HV: 20164 Val: 0.6547 Test: 0.5929\n",
      "Epoch: 235, Loss: 0.1552 tsm_loss: 0.0444 reg_loss: 0.1107 N_Y: 406032 N_S: 1754772 N: 406032 N_HV: 20453 Val: 0.6435 Test: 0.5889\n",
      "Epoch: 236, Loss: 0.1707 tsm_loss: 0.0412 reg_loss: 0.1295 N_Y: 405734 N_S: 1754772 N: 405734 N_HV: 19463 Val: 0.6495 Test: 0.5831\n",
      "Epoch: 237, Loss: 0.1867 tsm_loss: 0.0454 reg_loss: 0.1413 N_Y: 406021 N_S: 1754772 N: 406021 N_HV: 20938 Val: 0.6287 Test: 0.5951\n",
      "Epoch: 238, Loss: 0.1736 tsm_loss: 0.0480 reg_loss: 0.1256 N_Y: 404844 N_S: 1754772 N: 404844 N_HV: 21385 Val: 0.6513 Test: 0.6142\n",
      "Epoch: 239, Loss: 0.1988 tsm_loss: 0.0437 reg_loss: 0.1551 N_Y: 404242 N_S: 1754772 N: 404242 N_HV: 20522 Val: 0.6387 Test: 0.5801\n",
      "Epoch: 240, Loss: 0.1449 tsm_loss: 0.0417 reg_loss: 0.1032 N_Y: 405252 N_S: 1754772 N: 405252 N_HV: 19075 Val: 0.6475 Test: 0.6067\n",
      "Epoch: 241, Loss: 0.1506 tsm_loss: 0.0405 reg_loss: 0.1101 N_Y: 406443 N_S: 1754772 N: 406443 N_HV: 19385 Val: 0.6534 Test: 0.5867\n",
      "Epoch: 242, Loss: 0.1721 tsm_loss: 0.0437 reg_loss: 0.1284 N_Y: 406082 N_S: 1754772 N: 406082 N_HV: 19694 Val: 0.6382 Test: 0.5754\n",
      "Epoch: 243, Loss: 0.1606 tsm_loss: 0.0423 reg_loss: 0.1182 N_Y: 407247 N_S: 1754772 N: 407247 N_HV: 19191 Val: 0.6378 Test: 0.6045\n",
      "Epoch: 244, Loss: 0.1878 tsm_loss: 0.0479 reg_loss: 0.1399 N_Y: 406834 N_S: 1754772 N: 406834 N_HV: 21209 Val: 0.6487 Test: 0.5876\n",
      "Epoch: 245, Loss: 0.1492 tsm_loss: 0.0392 reg_loss: 0.1101 N_Y: 405576 N_S: 1754772 N: 405576 N_HV: 19148 Val: 0.6380 Test: 0.5787\n",
      "Epoch: 246, Loss: 0.1625 tsm_loss: 0.0431 reg_loss: 0.1194 N_Y: 404776 N_S: 1754772 N: 404776 N_HV: 17845 Val: 0.6533 Test: 0.5809\n",
      "Epoch: 247, Loss: 0.1959 tsm_loss: 0.0446 reg_loss: 0.1513 N_Y: 406891 N_S: 1754772 N: 406891 N_HV: 20464 Val: 0.6445 Test: 0.6068\n",
      "Epoch: 248, Loss: 0.2150 tsm_loss: 0.0515 reg_loss: 0.1635 N_Y: 405142 N_S: 1754772 N: 405142 N_HV: 23027 Val: 0.6632 Test: 0.5922\n",
      "Epoch: 249, Loss: 0.2106 tsm_loss: 0.0433 reg_loss: 0.1673 N_Y: 406625 N_S: 1754772 N: 406625 N_HV: 19956 Val: 0.6545 Test: 0.5845\n",
      "Epoch: 250, Loss: 0.2345 tsm_loss: 0.0448 reg_loss: 0.1898 N_Y: 405641 N_S: 1754772 N: 405641 N_HV: 19485 Val: 0.6536 Test: 0.6060\n",
      "Epoch: 251, Loss: 0.2293 tsm_loss: 0.0450 reg_loss: 0.1844 N_Y: 404299 N_S: 1754772 N: 404299 N_HV: 20175 Val: 0.6350 Test: 0.5807\n",
      "Epoch: 252, Loss: 0.1599 tsm_loss: 0.0447 reg_loss: 0.1152 N_Y: 406095 N_S: 1754772 N: 406095 N_HV: 20091 Val: 0.6445 Test: 0.5763\n",
      "Epoch: 253, Loss: 0.1685 tsm_loss: 0.0395 reg_loss: 0.1290 N_Y: 407035 N_S: 1754772 N: 407035 N_HV: 18464 Val: 0.6350 Test: 0.5888\n",
      "Epoch: 254, Loss: 0.1468 tsm_loss: 0.0382 reg_loss: 0.1086 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 18565 Val: 0.6366 Test: 0.5835\n",
      "Epoch: 255, Loss: 0.1619 tsm_loss: 0.0397 reg_loss: 0.1222 N_Y: 408029 N_S: 1754772 N: 408029 N_HV: 18438 Val: 0.6318 Test: 0.5844\n",
      "Epoch: 256, Loss: 0.1486 tsm_loss: 0.0346 reg_loss: 0.1140 N_Y: 405826 N_S: 1754772 N: 405826 N_HV: 17373 Val: 0.6415 Test: 0.5772\n",
      "Epoch: 257, Loss: 0.1948 tsm_loss: 0.0497 reg_loss: 0.1451 N_Y: 405439 N_S: 1754772 N: 405439 N_HV: 21749 Val: 0.6525 Test: 0.5867\n",
      "Epoch: 258, Loss: 0.1612 tsm_loss: 0.0392 reg_loss: 0.1220 N_Y: 406304 N_S: 1754772 N: 406304 N_HV: 17873 Val: 0.6490 Test: 0.6032\n",
      "Epoch: 259, Loss: 0.1478 tsm_loss: 0.0371 reg_loss: 0.1107 N_Y: 406878 N_S: 1754772 N: 406878 N_HV: 17981 Val: 0.6388 Test: 0.5817\n",
      "Epoch: 260, Loss: 0.1889 tsm_loss: 0.0410 reg_loss: 0.1479 N_Y: 406689 N_S: 1754772 N: 406689 N_HV: 19184 Val: 0.6553 Test: 0.6045\n",
      "Epoch: 261, Loss: 0.1648 tsm_loss: 0.0326 reg_loss: 0.1322 N_Y: 406121 N_S: 1754772 N: 406121 N_HV: 16257 Val: 0.6544 Test: 0.6229\n",
      "Epoch: 262, Loss: 0.1890 tsm_loss: 0.0349 reg_loss: 0.1541 N_Y: 407349 N_S: 1754772 N: 407349 N_HV: 16860 Val: 0.6530 Test: 0.5793\n",
      "Epoch: 263, Loss: 0.1473 tsm_loss: 0.0337 reg_loss: 0.1136 N_Y: 406115 N_S: 1754772 N: 406115 N_HV: 16200 Val: 0.6298 Test: 0.5859\n",
      "Epoch: 264, Loss: 0.1631 tsm_loss: 0.0344 reg_loss: 0.1287 N_Y: 407381 N_S: 1754772 N: 407381 N_HV: 16166 Val: 0.6581 Test: 0.5726\n",
      "Epoch: 265, Loss: 0.1432 tsm_loss: 0.0368 reg_loss: 0.1064 N_Y: 404742 N_S: 1754772 N: 404742 N_HV: 17041 Val: 0.6346 Test: 0.5687\n",
      "Epoch: 266, Loss: 0.1568 tsm_loss: 0.0387 reg_loss: 0.1181 N_Y: 406645 N_S: 1754772 N: 406645 N_HV: 18663 Val: 0.6676 Test: 0.6166\n",
      "Epoch: 267, Loss: 0.1826 tsm_loss: 0.0380 reg_loss: 0.1445 N_Y: 405541 N_S: 1754772 N: 405541 N_HV: 16778 Val: 0.6549 Test: 0.5980\n",
      "Epoch: 268, Loss: 0.1759 tsm_loss: 0.0438 reg_loss: 0.1322 N_Y: 406361 N_S: 1754772 N: 406361 N_HV: 21148 Val: 0.6441 Test: 0.5784\n",
      "Epoch: 269, Loss: 0.1785 tsm_loss: 0.0501 reg_loss: 0.1284 N_Y: 405062 N_S: 1754772 N: 405062 N_HV: 20865 Val: 0.6367 Test: 0.5851\n",
      "Epoch: 270, Loss: 0.2588 tsm_loss: 0.0515 reg_loss: 0.2073 N_Y: 405297 N_S: 1754772 N: 405297 N_HV: 21971 Val: 0.6393 Test: 0.5807\n",
      "Epoch: 271, Loss: 0.2015 tsm_loss: 0.0464 reg_loss: 0.1552 N_Y: 407073 N_S: 1754772 N: 407073 N_HV: 20220 Val: 0.6530 Test: 0.5863\n",
      "Epoch: 272, Loss: 0.2113 tsm_loss: 0.0424 reg_loss: 0.1689 N_Y: 406788 N_S: 1754772 N: 406788 N_HV: 19694 Val: 0.6523 Test: 0.5894\n",
      "Epoch: 273, Loss: 0.1947 tsm_loss: 0.0444 reg_loss: 0.1503 N_Y: 403583 N_S: 1754772 N: 403583 N_HV: 18835 Val: 0.6660 Test: 0.6170\n",
      "Epoch: 274, Loss: 0.2203 tsm_loss: 0.0457 reg_loss: 0.1747 N_Y: 406181 N_S: 1754772 N: 406181 N_HV: 19966 Val: 0.6390 Test: 0.5891\n",
      "Epoch: 275, Loss: 0.2041 tsm_loss: 0.0386 reg_loss: 0.1655 N_Y: 404184 N_S: 1754772 N: 404184 N_HV: 18202 Val: 0.6275 Test: 0.5854\n",
      "Epoch: 276, Loss: 0.1661 tsm_loss: 0.0367 reg_loss: 0.1294 N_Y: 405246 N_S: 1754772 N: 405246 N_HV: 17292 Val: 0.6438 Test: 0.5986\n",
      "Epoch: 277, Loss: 0.1510 tsm_loss: 0.0372 reg_loss: 0.1138 N_Y: 406328 N_S: 1754772 N: 406328 N_HV: 17865 Val: 0.6347 Test: 0.5872\n",
      "Epoch: 278, Loss: 0.1800 tsm_loss: 0.0361 reg_loss: 0.1438 N_Y: 406011 N_S: 1754772 N: 406011 N_HV: 17584 Val: 0.6416 Test: 0.5779\n",
      "Epoch: 279, Loss: 0.1620 tsm_loss: 0.0330 reg_loss: 0.1290 N_Y: 402922 N_S: 1754772 N: 402922 N_HV: 16145 Val: 0.6437 Test: 0.5742\n",
      "Epoch: 280, Loss: 0.1236 tsm_loss: 0.0322 reg_loss: 0.0914 N_Y: 405462 N_S: 1754772 N: 405462 N_HV: 16012 Val: 0.6402 Test: 0.5717\n",
      "Epoch: 281, Loss: 0.1521 tsm_loss: 0.0354 reg_loss: 0.1167 N_Y: 405230 N_S: 1754772 N: 405230 N_HV: 16707 Val: 0.6395 Test: 0.5810\n",
      "Epoch: 282, Loss: 0.1293 tsm_loss: 0.0310 reg_loss: 0.0983 N_Y: 406514 N_S: 1754772 N: 406514 N_HV: 14468 Val: 0.6389 Test: 0.5965\n",
      "Epoch: 283, Loss: 0.1284 tsm_loss: 0.0313 reg_loss: 0.0971 N_Y: 406814 N_S: 1754772 N: 406814 N_HV: 16062 Val: 0.6488 Test: 0.5864\n",
      "Epoch: 284, Loss: 0.1411 tsm_loss: 0.0374 reg_loss: 0.1037 N_Y: 407713 N_S: 1754772 N: 407713 N_HV: 17477 Val: 0.6388 Test: 0.5826\n",
      "Epoch: 285, Loss: 0.1281 tsm_loss: 0.0317 reg_loss: 0.0964 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 15409 Val: 0.6420 Test: 0.6008\n",
      "Epoch: 286, Loss: 0.1629 tsm_loss: 0.0333 reg_loss: 0.1296 N_Y: 405746 N_S: 1754772 N: 405746 N_HV: 15897 Val: 0.6416 Test: 0.5849\n",
      "Epoch: 287, Loss: 0.1401 tsm_loss: 0.0294 reg_loss: 0.1107 N_Y: 404873 N_S: 1754772 N: 404873 N_HV: 15061 Val: 0.6328 Test: 0.5835\n",
      "Epoch: 288, Loss: 0.1268 tsm_loss: 0.0342 reg_loss: 0.0926 N_Y: 406414 N_S: 1754772 N: 406414 N_HV: 16491 Val: 0.6474 Test: 0.5958\n",
      "Epoch: 289, Loss: 0.1350 tsm_loss: 0.0323 reg_loss: 0.1028 N_Y: 404390 N_S: 1754772 N: 404390 N_HV: 14884 Val: 0.6496 Test: 0.5959\n",
      "Epoch: 290, Loss: 0.1585 tsm_loss: 0.0378 reg_loss: 0.1206 N_Y: 405676 N_S: 1754772 N: 405676 N_HV: 17330 Val: 0.6373 Test: 0.5773\n",
      "Epoch: 291, Loss: 0.1324 tsm_loss: 0.0292 reg_loss: 0.1032 N_Y: 407135 N_S: 1754772 N: 407135 N_HV: 14379 Val: 0.6441 Test: 0.5676\n",
      "Epoch: 292, Loss: 0.1368 tsm_loss: 0.0297 reg_loss: 0.1071 N_Y: 405760 N_S: 1754772 N: 405760 N_HV: 15265 Val: 0.6304 Test: 0.5865\n",
      "Epoch: 293, Loss: 0.1601 tsm_loss: 0.0314 reg_loss: 0.1286 N_Y: 405159 N_S: 1754772 N: 405159 N_HV: 15142 Val: 0.6365 Test: 0.5854\n",
      "Epoch: 294, Loss: 0.1376 tsm_loss: 0.0331 reg_loss: 0.1045 N_Y: 405869 N_S: 1754772 N: 405869 N_HV: 15972 Val: 0.6370 Test: 0.6004\n",
      "Epoch: 295, Loss: 0.1440 tsm_loss: 0.0337 reg_loss: 0.1103 N_Y: 406339 N_S: 1754772 N: 406339 N_HV: 15907 Val: 0.6399 Test: 0.5813\n",
      "Epoch: 296, Loss: 0.1644 tsm_loss: 0.0304 reg_loss: 0.1340 N_Y: 406727 N_S: 1754772 N: 406727 N_HV: 14948 Val: 0.6327 Test: 0.5933\n",
      "Epoch: 297, Loss: 0.1441 tsm_loss: 0.0313 reg_loss: 0.1128 N_Y: 406440 N_S: 1754772 N: 406440 N_HV: 15437 Val: 0.6314 Test: 0.5899\n",
      "Epoch: 298, Loss: 0.1571 tsm_loss: 0.0305 reg_loss: 0.1266 N_Y: 406206 N_S: 1754772 N: 406206 N_HV: 15239 Val: 0.6453 Test: 0.6099\n",
      "Epoch: 299, Loss: 0.1606 tsm_loss: 0.0322 reg_loss: 0.1284 N_Y: 405469 N_S: 1754772 N: 405469 N_HV: 15998 Val: 0.6626 Test: 0.5837\n",
      "Epoch: 300, Loss: 0.2381 tsm_loss: 0.0350 reg_loss: 0.2031 N_Y: 404108 N_S: 1754772 N: 404108 N_HV: 15265 Val: 0.6314 Test: 0.5952\n",
      "Epoch: 301, Loss: 0.1730 tsm_loss: 0.0366 reg_loss: 0.1364 N_Y: 404449 N_S: 1754772 N: 404449 N_HV: 16793 Val: 0.6622 Test: 0.5854\n",
      "Epoch: 302, Loss: 0.1657 tsm_loss: 0.0459 reg_loss: 0.1198 N_Y: 406549 N_S: 1754772 N: 406549 N_HV: 19214 Val: 0.6401 Test: 0.5950\n",
      "Epoch: 303, Loss: 0.1489 tsm_loss: 0.0358 reg_loss: 0.1131 N_Y: 404805 N_S: 1754772 N: 404805 N_HV: 16686 Val: 0.6391 Test: 0.5830\n",
      "Epoch: 304, Loss: 0.1456 tsm_loss: 0.0353 reg_loss: 0.1103 N_Y: 405193 N_S: 1754772 N: 405193 N_HV: 17368 Val: 0.6408 Test: 0.5818\n",
      "Epoch: 305, Loss: 0.1405 tsm_loss: 0.0305 reg_loss: 0.1099 N_Y: 405731 N_S: 1754772 N: 405731 N_HV: 14907 Val: 0.6591 Test: 0.6445\n",
      "Epoch: 306, Loss: 0.1406 tsm_loss: 0.0339 reg_loss: 0.1066 N_Y: 406022 N_S: 1754772 N: 406022 N_HV: 16589 Val: 0.6778 Test: 0.5907\n",
      "Epoch: 307, Loss: 0.1610 tsm_loss: 0.0367 reg_loss: 0.1242 N_Y: 405041 N_S: 1754772 N: 405041 N_HV: 17579 Val: 0.6512 Test: 0.5883\n",
      "Epoch: 308, Loss: 0.1554 tsm_loss: 0.0290 reg_loss: 0.1265 N_Y: 405739 N_S: 1754772 N: 405739 N_HV: 14921 Val: 0.6521 Test: 0.5922\n",
      "Epoch: 309, Loss: 0.1486 tsm_loss: 0.0327 reg_loss: 0.1159 N_Y: 406710 N_S: 1754772 N: 406710 N_HV: 15541 Val: 0.6581 Test: 0.6125\n",
      "Epoch: 310, Loss: 0.1503 tsm_loss: 0.0351 reg_loss: 0.1152 N_Y: 406561 N_S: 1754772 N: 406561 N_HV: 16400 Val: 0.6366 Test: 0.5791\n",
      "Epoch: 311, Loss: 0.1327 tsm_loss: 0.0323 reg_loss: 0.1004 N_Y: 406066 N_S: 1754772 N: 406066 N_HV: 15960 Val: 0.6321 Test: 0.5814\n",
      "Epoch: 312, Loss: 0.1414 tsm_loss: 0.0325 reg_loss: 0.1090 N_Y: 405356 N_S: 1754772 N: 405356 N_HV: 15458 Val: 0.6416 Test: 0.5937\n",
      "Epoch: 313, Loss: 0.1378 tsm_loss: 0.0304 reg_loss: 0.1074 N_Y: 407217 N_S: 1754772 N: 407217 N_HV: 15178 Val: 0.6452 Test: 0.5816\n",
      "Epoch: 314, Loss: 0.1327 tsm_loss: 0.0301 reg_loss: 0.1026 N_Y: 405885 N_S: 1754772 N: 405885 N_HV: 14860 Val: 0.6452 Test: 0.5864\n",
      "Epoch: 315, Loss: 0.1727 tsm_loss: 0.0308 reg_loss: 0.1419 N_Y: 406199 N_S: 1754772 N: 406199 N_HV: 14801 Val: 0.6388 Test: 0.5746\n",
      "Epoch: 316, Loss: 0.1398 tsm_loss: 0.0341 reg_loss: 0.1057 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 15215 Val: 0.6457 Test: 0.5838\n",
      "Epoch: 317, Loss: 0.1683 tsm_loss: 0.0309 reg_loss: 0.1373 N_Y: 406856 N_S: 1754772 N: 406856 N_HV: 14802 Val: 0.6357 Test: 0.5961\n",
      "Epoch: 318, Loss: 0.1583 tsm_loss: 0.0338 reg_loss: 0.1244 N_Y: 406986 N_S: 1754772 N: 406986 N_HV: 15718 Val: 0.6509 Test: 0.5899\n",
      "Epoch: 319, Loss: 0.2093 tsm_loss: 0.0409 reg_loss: 0.1685 N_Y: 405159 N_S: 1754772 N: 405159 N_HV: 17717 Val: 0.6328 Test: 0.5860\n",
      "Epoch: 320, Loss: 0.1898 tsm_loss: 0.0393 reg_loss: 0.1504 N_Y: 407001 N_S: 1754772 N: 407001 N_HV: 19160 Val: 0.6553 Test: 0.5777\n",
      "Epoch: 321, Loss: 0.1888 tsm_loss: 0.0380 reg_loss: 0.1509 N_Y: 406065 N_S: 1754772 N: 406065 N_HV: 17396 Val: 0.6382 Test: 0.6048\n",
      "Epoch: 322, Loss: 0.1719 tsm_loss: 0.0341 reg_loss: 0.1378 N_Y: 405940 N_S: 1754772 N: 405940 N_HV: 15989 Val: 0.6435 Test: 0.5964\n",
      "Epoch: 323, Loss: 0.1735 tsm_loss: 0.0370 reg_loss: 0.1365 N_Y: 404848 N_S: 1754772 N: 404848 N_HV: 17734 Val: 0.6360 Test: 0.5866\n",
      "Epoch: 324, Loss: 0.2134 tsm_loss: 0.0391 reg_loss: 0.1743 N_Y: 406716 N_S: 1754772 N: 406716 N_HV: 18241 Val: 0.6314 Test: 0.5904\n",
      "Epoch: 325, Loss: 0.1445 tsm_loss: 0.0358 reg_loss: 0.1087 N_Y: 404574 N_S: 1754772 N: 404574 N_HV: 16360 Val: 0.6428 Test: 0.5916\n",
      "Epoch: 326, Loss: 0.1717 tsm_loss: 0.0338 reg_loss: 0.1379 N_Y: 406556 N_S: 1754772 N: 406556 N_HV: 16191 Val: 0.6461 Test: 0.5811\n",
      "Epoch: 327, Loss: 0.1287 tsm_loss: 0.0306 reg_loss: 0.0981 N_Y: 405558 N_S: 1754772 N: 405558 N_HV: 14625 Val: 0.6328 Test: 0.5788\n",
      "Epoch: 328, Loss: 0.1696 tsm_loss: 0.0366 reg_loss: 0.1330 N_Y: 406371 N_S: 1754772 N: 406371 N_HV: 17382 Val: 0.6448 Test: 0.5736\n",
      "Epoch: 329, Loss: 0.1555 tsm_loss: 0.0360 reg_loss: 0.1195 N_Y: 406013 N_S: 1754772 N: 406013 N_HV: 17438 Val: 0.6562 Test: 0.6086\n",
      "Epoch: 330, Loss: 0.1625 tsm_loss: 0.0355 reg_loss: 0.1270 N_Y: 405517 N_S: 1754772 N: 405517 N_HV: 16788 Val: 0.6503 Test: 0.6045\n",
      "Epoch: 331, Loss: 0.2045 tsm_loss: 0.0400 reg_loss: 0.1645 N_Y: 405623 N_S: 1754772 N: 405623 N_HV: 17587 Val: 0.6378 Test: 0.5964\n",
      "Epoch: 332, Loss: 0.1609 tsm_loss: 0.0380 reg_loss: 0.1229 N_Y: 404253 N_S: 1754772 N: 404253 N_HV: 17220 Val: 0.6232 Test: 0.5838\n",
      "Epoch: 333, Loss: 0.1669 tsm_loss: 0.0312 reg_loss: 0.1357 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 14983 Val: 0.6491 Test: 0.5890\n",
      "Epoch: 334, Loss: 0.1417 tsm_loss: 0.0305 reg_loss: 0.1111 N_Y: 407152 N_S: 1754772 N: 407152 N_HV: 14770 Val: 0.6366 Test: 0.5920\n",
      "Epoch: 335, Loss: 0.1275 tsm_loss: 0.0259 reg_loss: 0.1016 N_Y: 406655 N_S: 1754772 N: 406655 N_HV: 14023 Val: 0.6595 Test: 0.6147\n",
      "Epoch: 336, Loss: 0.1482 tsm_loss: 0.0275 reg_loss: 0.1207 N_Y: 405895 N_S: 1754772 N: 405895 N_HV: 14574 Val: 0.6388 Test: 0.5885\n",
      "Epoch: 337, Loss: 0.1308 tsm_loss: 0.0280 reg_loss: 0.1027 N_Y: 405636 N_S: 1754772 N: 405636 N_HV: 13887 Val: 0.6342 Test: 0.5820\n",
      "Epoch: 338, Loss: 0.1386 tsm_loss: 0.0275 reg_loss: 0.1111 N_Y: 406117 N_S: 1754772 N: 406117 N_HV: 13745 Val: 0.6449 Test: 0.5869\n",
      "Epoch: 339, Loss: 0.1237 tsm_loss: 0.0254 reg_loss: 0.0983 N_Y: 405840 N_S: 1754772 N: 405840 N_HV: 12970 Val: 0.6340 Test: 0.5874\n",
      "Epoch: 340, Loss: 0.1392 tsm_loss: 0.0301 reg_loss: 0.1091 N_Y: 407036 N_S: 1754772 N: 407036 N_HV: 14914 Val: 0.6509 Test: 0.5870\n",
      "Epoch: 341, Loss: 0.1254 tsm_loss: 0.0273 reg_loss: 0.0981 N_Y: 406529 N_S: 1754772 N: 406529 N_HV: 13870 Val: 0.6394 Test: 0.5878\n",
      "Epoch: 342, Loss: 0.1249 tsm_loss: 0.0268 reg_loss: 0.0981 N_Y: 406923 N_S: 1754772 N: 406923 N_HV: 13354 Val: 0.6306 Test: 0.6031\n",
      "Epoch: 343, Loss: 0.1597 tsm_loss: 0.0296 reg_loss: 0.1301 N_Y: 406316 N_S: 1754772 N: 406316 N_HV: 15288 Val: 0.6268 Test: 0.5934\n",
      "Epoch: 344, Loss: 0.1529 tsm_loss: 0.0296 reg_loss: 0.1233 N_Y: 407063 N_S: 1754772 N: 407063 N_HV: 14299 Val: 0.6397 Test: 0.5968\n",
      "Epoch: 345, Loss: 0.1321 tsm_loss: 0.0275 reg_loss: 0.1046 N_Y: 406412 N_S: 1754772 N: 406412 N_HV: 13906 Val: 0.6259 Test: 0.5860\n",
      "Epoch: 346, Loss: 0.1345 tsm_loss: 0.0303 reg_loss: 0.1042 N_Y: 406204 N_S: 1754772 N: 406204 N_HV: 14293 Val: 0.6359 Test: 0.5790\n",
      "Epoch: 347, Loss: 0.1197 tsm_loss: 0.0268 reg_loss: 0.0929 N_Y: 406305 N_S: 1754772 N: 406305 N_HV: 13338 Val: 0.6414 Test: 0.5863\n",
      "Epoch: 348, Loss: 0.1385 tsm_loss: 0.0247 reg_loss: 0.1138 N_Y: 405982 N_S: 1754772 N: 405982 N_HV: 13171 Val: 0.6408 Test: 0.5836\n",
      "Epoch: 349, Loss: 0.2394 tsm_loss: 0.0244 reg_loss: 0.2149 N_Y: 407161 N_S: 1754772 N: 407161 N_HV: 12291 Val: 0.6511 Test: 0.6122\n",
      "Epoch: 350, Loss: 0.2155 tsm_loss: 0.0252 reg_loss: 0.1903 N_Y: 406451 N_S: 1754772 N: 406451 N_HV: 13370 Val: 0.6536 Test: 0.5870\n",
      "Epoch: 351, Loss: 0.1632 tsm_loss: 0.0257 reg_loss: 0.1375 N_Y: 407895 N_S: 1754772 N: 407895 N_HV: 13190 Val: 0.6427 Test: 0.5794\n",
      "Epoch: 352, Loss: 0.1852 tsm_loss: 0.0247 reg_loss: 0.1606 N_Y: 406297 N_S: 1754772 N: 406297 N_HV: 12452 Val: 0.6407 Test: 0.6070\n",
      "Epoch: 353, Loss: 0.1403 tsm_loss: 0.0236 reg_loss: 0.1167 N_Y: 405620 N_S: 1754772 N: 405620 N_HV: 12690 Val: 0.6356 Test: 0.5987\n",
      "Epoch: 354, Loss: 0.2160 tsm_loss: 0.0257 reg_loss: 0.1903 N_Y: 405196 N_S: 1754772 N: 405196 N_HV: 12634 Val: 0.6362 Test: 0.5821\n",
      "Epoch: 355, Loss: 0.1944 tsm_loss: 0.0242 reg_loss: 0.1702 N_Y: 404082 N_S: 1754772 N: 404082 N_HV: 12464 Val: 0.6462 Test: 0.5771\n",
      "Epoch: 356, Loss: 0.1782 tsm_loss: 0.0307 reg_loss: 0.1475 N_Y: 405656 N_S: 1754772 N: 405656 N_HV: 14279 Val: 0.6472 Test: 0.5886\n",
      "Epoch: 357, Loss: 0.2022 tsm_loss: 0.0279 reg_loss: 0.1744 N_Y: 406342 N_S: 1754772 N: 406342 N_HV: 12996 Val: 0.6289 Test: 0.5912\n",
      "Epoch: 358, Loss: 0.1646 tsm_loss: 0.0279 reg_loss: 0.1367 N_Y: 405049 N_S: 1754772 N: 405049 N_HV: 14258 Val: 0.6295 Test: 0.5921\n",
      "Epoch: 359, Loss: 0.1557 tsm_loss: 0.0249 reg_loss: 0.1309 N_Y: 404673 N_S: 1754772 N: 404673 N_HV: 11931 Val: 0.6459 Test: 0.5996\n",
      "Epoch: 360, Loss: 0.1403 tsm_loss: 0.0262 reg_loss: 0.1141 N_Y: 406973 N_S: 1754772 N: 406973 N_HV: 12555 Val: 0.6260 Test: 0.5969\n",
      "Epoch: 361, Loss: 0.1255 tsm_loss: 0.0282 reg_loss: 0.0972 N_Y: 406607 N_S: 1754772 N: 406607 N_HV: 12941 Val: 0.6435 Test: 0.5954\n",
      "Epoch: 362, Loss: 0.1307 tsm_loss: 0.0271 reg_loss: 0.1035 N_Y: 406359 N_S: 1754772 N: 406359 N_HV: 13464 Val: 0.6623 Test: 0.5871\n",
      "Epoch: 363, Loss: 0.1389 tsm_loss: 0.0274 reg_loss: 0.1115 N_Y: 406123 N_S: 1754772 N: 406123 N_HV: 13753 Val: 0.6340 Test: 0.6006\n",
      "Epoch: 364, Loss: 0.1298 tsm_loss: 0.0256 reg_loss: 0.1042 N_Y: 407195 N_S: 1754772 N: 407195 N_HV: 11701 Val: 0.6407 Test: 0.5914\n",
      "Epoch: 365, Loss: 0.1553 tsm_loss: 0.0318 reg_loss: 0.1236 N_Y: 405720 N_S: 1754772 N: 405720 N_HV: 15999 Val: 0.6374 Test: 0.5925\n",
      "Epoch: 366, Loss: 0.1439 tsm_loss: 0.0290 reg_loss: 0.1149 N_Y: 406046 N_S: 1754772 N: 406046 N_HV: 13738 Val: 0.6617 Test: 0.6043\n",
      "Epoch: 367, Loss: 0.1355 tsm_loss: 0.0284 reg_loss: 0.1071 N_Y: 406423 N_S: 1754772 N: 406423 N_HV: 13627 Val: 0.6374 Test: 0.5827\n",
      "Epoch: 368, Loss: 0.1386 tsm_loss: 0.0302 reg_loss: 0.1084 N_Y: 405552 N_S: 1754772 N: 405552 N_HV: 14918 Val: 0.6367 Test: 0.5889\n",
      "Epoch: 369, Loss: 0.1764 tsm_loss: 0.0312 reg_loss: 0.1452 N_Y: 405840 N_S: 1754772 N: 405840 N_HV: 15166 Val: 0.6427 Test: 0.5948\n",
      "Epoch: 370, Loss: 0.1336 tsm_loss: 0.0272 reg_loss: 0.1064 N_Y: 405772 N_S: 1754772 N: 405772 N_HV: 13880 Val: 0.6472 Test: 0.5854\n",
      "Epoch: 371, Loss: 0.1323 tsm_loss: 0.0272 reg_loss: 0.1051 N_Y: 405417 N_S: 1754772 N: 405417 N_HV: 13277 Val: 0.6284 Test: 0.5880\n",
      "Epoch: 372, Loss: 0.1295 tsm_loss: 0.0294 reg_loss: 0.1001 N_Y: 406648 N_S: 1754772 N: 406648 N_HV: 13827 Val: 0.6320 Test: 0.5968\n",
      "Epoch: 373, Loss: 0.1493 tsm_loss: 0.0293 reg_loss: 0.1201 N_Y: 407996 N_S: 1754772 N: 407996 N_HV: 14687 Val: 0.6458 Test: 0.6019\n",
      "Epoch: 374, Loss: 0.1170 tsm_loss: 0.0257 reg_loss: 0.0914 N_Y: 405927 N_S: 1754772 N: 405927 N_HV: 13484 Val: 0.6374 Test: 0.5828\n",
      "Epoch: 375, Loss: 0.1286 tsm_loss: 0.0271 reg_loss: 0.1015 N_Y: 405277 N_S: 1754772 N: 405277 N_HV: 13828 Val: 0.6443 Test: 0.5881\n",
      "Epoch: 376, Loss: 0.1423 tsm_loss: 0.0236 reg_loss: 0.1187 N_Y: 405724 N_S: 1754772 N: 405724 N_HV: 12271 Val: 0.6510 Test: 0.6083\n",
      "Epoch: 377, Loss: 0.1223 tsm_loss: 0.0234 reg_loss: 0.0989 N_Y: 406675 N_S: 1754772 N: 406675 N_HV: 12582 Val: 0.6346 Test: 0.5897\n",
      "Epoch: 378, Loss: 0.1340 tsm_loss: 0.0258 reg_loss: 0.1082 N_Y: 406793 N_S: 1754772 N: 406793 N_HV: 12505 Val: 0.6379 Test: 0.5998\n",
      "Epoch: 379, Loss: 0.1443 tsm_loss: 0.0285 reg_loss: 0.1157 N_Y: 405124 N_S: 1754772 N: 405124 N_HV: 13978 Val: 0.6573 Test: 0.5858\n",
      "Epoch: 380, Loss: 0.1802 tsm_loss: 0.0339 reg_loss: 0.1463 N_Y: 405105 N_S: 1754772 N: 405105 N_HV: 15433 Val: 0.6353 Test: 0.6064\n",
      "Epoch: 381, Loss: 0.1570 tsm_loss: 0.0328 reg_loss: 0.1242 N_Y: 406296 N_S: 1754772 N: 406296 N_HV: 15261 Val: 0.6431 Test: 0.5937\n",
      "Epoch: 382, Loss: 0.1367 tsm_loss: 0.0308 reg_loss: 0.1059 N_Y: 405211 N_S: 1754772 N: 405211 N_HV: 14404 Val: 0.6254 Test: 0.5931\n",
      "Epoch: 383, Loss: 0.1736 tsm_loss: 0.0247 reg_loss: 0.1489 N_Y: 404147 N_S: 1754772 N: 404147 N_HV: 12343 Val: 0.6380 Test: 0.5959\n",
      "Epoch: 384, Loss: 0.1606 tsm_loss: 0.0235 reg_loss: 0.1370 N_Y: 406786 N_S: 1754772 N: 406786 N_HV: 12899 Val: 0.6476 Test: 0.5873\n",
      "Epoch: 385, Loss: 0.1549 tsm_loss: 0.0261 reg_loss: 0.1288 N_Y: 407053 N_S: 1754772 N: 407053 N_HV: 12996 Val: 0.6434 Test: 0.5808\n",
      "Epoch: 386, Loss: 0.1747 tsm_loss: 0.0241 reg_loss: 0.1507 N_Y: 405573 N_S: 1754772 N: 405573 N_HV: 12212 Val: 0.6501 Test: 0.6018\n",
      "Epoch: 387, Loss: 0.1866 tsm_loss: 0.0262 reg_loss: 0.1604 N_Y: 406838 N_S: 1754772 N: 406838 N_HV: 12884 Val: 0.6330 Test: 0.5813\n",
      "Epoch: 388, Loss: 0.1582 tsm_loss: 0.0284 reg_loss: 0.1299 N_Y: 407879 N_S: 1754772 N: 407879 N_HV: 13252 Val: 0.6418 Test: 0.5778\n",
      "Epoch: 389, Loss: 0.1807 tsm_loss: 0.0305 reg_loss: 0.1502 N_Y: 404104 N_S: 1754772 N: 404104 N_HV: 14465 Val: 0.6380 Test: 0.5981\n",
      "Epoch: 390, Loss: 0.1808 tsm_loss: 0.0293 reg_loss: 0.1515 N_Y: 405536 N_S: 1754772 N: 405536 N_HV: 13870 Val: 0.6277 Test: 0.5806\n",
      "Epoch: 391, Loss: 0.1812 tsm_loss: 0.0329 reg_loss: 0.1483 N_Y: 406087 N_S: 1754772 N: 406087 N_HV: 14888 Val: 0.6473 Test: 0.5776\n",
      "Epoch: 392, Loss: 0.1716 tsm_loss: 0.0299 reg_loss: 0.1417 N_Y: 406017 N_S: 1754772 N: 406017 N_HV: 13507 Val: 0.6473 Test: 0.5716\n",
      "Epoch: 393, Loss: 0.1484 tsm_loss: 0.0358 reg_loss: 0.1126 N_Y: 402317 N_S: 1754772 N: 402317 N_HV: 15702 Val: 0.6450 Test: 0.6094\n",
      "Epoch: 394, Loss: 0.1563 tsm_loss: 0.0314 reg_loss: 0.1248 N_Y: 406700 N_S: 1754772 N: 406700 N_HV: 15156 Val: 0.6382 Test: 0.6018\n",
      "Epoch: 395, Loss: 0.1373 tsm_loss: 0.0313 reg_loss: 0.1060 N_Y: 405928 N_S: 1754772 N: 405928 N_HV: 14713 Val: 0.6574 Test: 0.5905\n",
      "Epoch: 396, Loss: 0.1368 tsm_loss: 0.0281 reg_loss: 0.1087 N_Y: 406150 N_S: 1754772 N: 406150 N_HV: 14295 Val: 0.6455 Test: 0.6047\n",
      "Epoch: 397, Loss: 0.1452 tsm_loss: 0.0281 reg_loss: 0.1171 N_Y: 406108 N_S: 1754772 N: 406108 N_HV: 13793 Val: 0.6392 Test: 0.5848\n",
      "Epoch: 398, Loss: 0.1712 tsm_loss: 0.0266 reg_loss: 0.1447 N_Y: 406194 N_S: 1754772 N: 406194 N_HV: 13322 Val: 0.6342 Test: 0.5862\n",
      "Epoch: 399, Loss: 0.1574 tsm_loss: 0.0253 reg_loss: 0.1320 N_Y: 405980 N_S: 1754772 N: 405980 N_HV: 12953 Val: 0.6377 Test: 0.5911\n",
      "Epoch: 400, Loss: 0.1497 tsm_loss: 0.0263 reg_loss: 0.1234 N_Y: 406868 N_S: 1754772 N: 406868 N_HV: 12815 Val: 0.6401 Test: 0.6020\n",
      "Epoch: 401, Loss: 0.1491 tsm_loss: 0.0220 reg_loss: 0.1271 N_Y: 406012 N_S: 1754772 N: 406012 N_HV: 12027 Val: 0.6438 Test: 0.5774\n",
      "Epoch: 402, Loss: 0.1380 tsm_loss: 0.0234 reg_loss: 0.1146 N_Y: 405777 N_S: 1754772 N: 405777 N_HV: 11734 Val: 0.6439 Test: 0.5929\n",
      "Epoch: 403, Loss: 0.1129 tsm_loss: 0.0229 reg_loss: 0.0900 N_Y: 405863 N_S: 1754772 N: 405863 N_HV: 11234 Val: 0.6491 Test: 0.5907\n",
      "Epoch: 404, Loss: 0.1182 tsm_loss: 0.0222 reg_loss: 0.0961 N_Y: 406567 N_S: 1754772 N: 406567 N_HV: 12154 Val: 0.6419 Test: 0.5782\n",
      "Epoch: 405, Loss: 0.1253 tsm_loss: 0.0221 reg_loss: 0.1033 N_Y: 404835 N_S: 1754772 N: 404835 N_HV: 10737 Val: 0.6491 Test: 0.5982\n",
      "Epoch: 406, Loss: 0.2219 tsm_loss: 0.0259 reg_loss: 0.1960 N_Y: 406385 N_S: 1754772 N: 406385 N_HV: 13185 Val: 0.6529 Test: 0.6020\n",
      "Epoch: 407, Loss: 0.2075 tsm_loss: 0.0277 reg_loss: 0.1798 N_Y: 405890 N_S: 1754772 N: 405890 N_HV: 12941 Val: 0.6494 Test: 0.5957\n",
      "Epoch: 408, Loss: 0.1899 tsm_loss: 0.0260 reg_loss: 0.1639 N_Y: 404739 N_S: 1754772 N: 404739 N_HV: 13047 Val: 0.6507 Test: 0.6001\n",
      "Epoch: 409, Loss: 0.1690 tsm_loss: 0.0281 reg_loss: 0.1409 N_Y: 407854 N_S: 1754772 N: 407854 N_HV: 13247 Val: 0.6489 Test: 0.5888\n",
      "Epoch: 410, Loss: 0.2029 tsm_loss: 0.0291 reg_loss: 0.1738 N_Y: 405691 N_S: 1754772 N: 405691 N_HV: 13172 Val: 0.6461 Test: 0.5915\n",
      "Epoch: 411, Loss: 0.1388 tsm_loss: 0.0275 reg_loss: 0.1113 N_Y: 406351 N_S: 1754772 N: 406351 N_HV: 13209 Val: 0.6341 Test: 0.5817\n",
      "Epoch: 412, Loss: 0.1414 tsm_loss: 0.0290 reg_loss: 0.1124 N_Y: 405283 N_S: 1754772 N: 405283 N_HV: 13533 Val: 0.6494 Test: 0.6093\n",
      "Epoch: 413, Loss: 0.1541 tsm_loss: 0.0300 reg_loss: 0.1241 N_Y: 405618 N_S: 1754772 N: 405618 N_HV: 14390 Val: 0.6609 Test: 0.5891\n",
      "Epoch: 414, Loss: 0.1765 tsm_loss: 0.0274 reg_loss: 0.1490 N_Y: 405763 N_S: 1754772 N: 405763 N_HV: 13272 Val: 0.6544 Test: 0.5915\n",
      "Epoch: 415, Loss: 0.1606 tsm_loss: 0.0264 reg_loss: 0.1342 N_Y: 404405 N_S: 1754772 N: 404405 N_HV: 12484 Val: 0.6406 Test: 0.6050\n",
      "Epoch: 416, Loss: 0.1584 tsm_loss: 0.0241 reg_loss: 0.1343 N_Y: 406339 N_S: 1754772 N: 406339 N_HV: 12790 Val: 0.6548 Test: 0.5893\n",
      "Epoch: 417, Loss: 0.1324 tsm_loss: 0.0248 reg_loss: 0.1076 N_Y: 407131 N_S: 1754772 N: 407131 N_HV: 11978 Val: 0.6500 Test: 0.5980\n",
      "Epoch: 418, Loss: 0.1545 tsm_loss: 0.0257 reg_loss: 0.1288 N_Y: 406599 N_S: 1754772 N: 406599 N_HV: 12738 Val: 0.6306 Test: 0.5941\n",
      "Epoch: 419, Loss: 0.1778 tsm_loss: 0.0316 reg_loss: 0.1462 N_Y: 405727 N_S: 1754772 N: 405727 N_HV: 13162 Val: 0.6533 Test: 0.6028\n",
      "Epoch: 420, Loss: 0.1811 tsm_loss: 0.0296 reg_loss: 0.1515 N_Y: 406040 N_S: 1754772 N: 406040 N_HV: 14843 Val: 0.6470 Test: 0.5982\n",
      "Epoch: 421, Loss: 0.1462 tsm_loss: 0.0327 reg_loss: 0.1135 N_Y: 406017 N_S: 1754772 N: 406017 N_HV: 14762 Val: 0.6380 Test: 0.5960\n",
      "Epoch: 422, Loss: 0.1387 tsm_loss: 0.0284 reg_loss: 0.1103 N_Y: 405988 N_S: 1754772 N: 405988 N_HV: 14670 Val: 0.6385 Test: 0.5966\n",
      "Epoch: 423, Loss: 0.1294 tsm_loss: 0.0255 reg_loss: 0.1039 N_Y: 404829 N_S: 1754772 N: 404829 N_HV: 12307 Val: 0.6358 Test: 0.5824\n",
      "Epoch: 424, Loss: 0.1506 tsm_loss: 0.0247 reg_loss: 0.1259 N_Y: 406745 N_S: 1754772 N: 406745 N_HV: 12946 Val: 0.6326 Test: 0.5846\n",
      "Epoch: 425, Loss: 0.1469 tsm_loss: 0.0287 reg_loss: 0.1182 N_Y: 404851 N_S: 1754772 N: 404851 N_HV: 12829 Val: 0.6498 Test: 0.5974\n",
      "Epoch: 426, Loss: 0.1321 tsm_loss: 0.0230 reg_loss: 0.1091 N_Y: 404844 N_S: 1754772 N: 404844 N_HV: 11635 Val: 0.6392 Test: 0.5996\n",
      "Epoch: 427, Loss: 0.1169 tsm_loss: 0.0250 reg_loss: 0.0919 N_Y: 407267 N_S: 1754772 N: 407267 N_HV: 13094 Val: 0.6471 Test: 0.5850\n",
      "Epoch: 428, Loss: 0.1446 tsm_loss: 0.0236 reg_loss: 0.1210 N_Y: 406832 N_S: 1754772 N: 406832 N_HV: 12545 Val: 0.6543 Test: 0.6012\n",
      "Epoch: 429, Loss: 0.1332 tsm_loss: 0.0220 reg_loss: 0.1111 N_Y: 405685 N_S: 1754772 N: 405685 N_HV: 10863 Val: 0.6357 Test: 0.5911\n",
      "Epoch: 430, Loss: 0.1290 tsm_loss: 0.0237 reg_loss: 0.1054 N_Y: 406369 N_S: 1754772 N: 406369 N_HV: 11765 Val: 0.6426 Test: 0.5964\n",
      "Epoch: 431, Loss: 0.1294 tsm_loss: 0.0197 reg_loss: 0.1096 N_Y: 405507 N_S: 1754772 N: 405507 N_HV: 10528 Val: 0.6554 Test: 0.6207\n",
      "Epoch: 432, Loss: 0.1357 tsm_loss: 0.0208 reg_loss: 0.1149 N_Y: 405227 N_S: 1754772 N: 405227 N_HV: 11155 Val: 0.6554 Test: 0.5950\n",
      "Epoch: 433, Loss: 0.1229 tsm_loss: 0.0214 reg_loss: 0.1015 N_Y: 406365 N_S: 1754772 N: 406365 N_HV: 11091 Val: 0.6342 Test: 0.5923\n",
      "Epoch: 434, Loss: 0.1279 tsm_loss: 0.0236 reg_loss: 0.1044 N_Y: 406852 N_S: 1754772 N: 406852 N_HV: 11432 Val: 0.6426 Test: 0.5994\n",
      "Epoch: 435, Loss: 0.1237 tsm_loss: 0.0204 reg_loss: 0.1033 N_Y: 406311 N_S: 1754772 N: 406311 N_HV: 10675 Val: 0.6426 Test: 0.6091\n",
      "Epoch: 436, Loss: 0.1341 tsm_loss: 0.0222 reg_loss: 0.1119 N_Y: 407414 N_S: 1754772 N: 407414 N_HV: 11789 Val: 0.6348 Test: 0.5826\n",
      "Epoch: 437, Loss: 0.1532 tsm_loss: 0.0228 reg_loss: 0.1305 N_Y: 406175 N_S: 1754772 N: 406175 N_HV: 11473 Val: 0.6405 Test: 0.5858\n",
      "Epoch: 438, Loss: 0.1317 tsm_loss: 0.0211 reg_loss: 0.1106 N_Y: 405294 N_S: 1754772 N: 405294 N_HV: 11454 Val: 0.6449 Test: 0.5915\n",
      "Epoch: 439, Loss: 0.1443 tsm_loss: 0.0239 reg_loss: 0.1204 N_Y: 406234 N_S: 1754772 N: 406234 N_HV: 11530 Val: 0.6501 Test: 0.6216\n",
      "Epoch: 440, Loss: 0.1802 tsm_loss: 0.0223 reg_loss: 0.1578 N_Y: 406603 N_S: 1754772 N: 406603 N_HV: 12147 Val: 0.6491 Test: 0.5742\n",
      "Epoch: 441, Loss: 0.1604 tsm_loss: 0.0254 reg_loss: 0.1350 N_Y: 407237 N_S: 1754772 N: 407237 N_HV: 12044 Val: 0.6251 Test: 0.5833\n",
      "Epoch: 442, Loss: 0.1643 tsm_loss: 0.0239 reg_loss: 0.1404 N_Y: 405782 N_S: 1754772 N: 405782 N_HV: 12025 Val: 0.6330 Test: 0.5985\n",
      "Epoch: 443, Loss: 0.1774 tsm_loss: 0.0218 reg_loss: 0.1556 N_Y: 406100 N_S: 1754772 N: 406100 N_HV: 11275 Val: 0.6489 Test: 0.5808\n",
      "Epoch: 444, Loss: 0.1409 tsm_loss: 0.0225 reg_loss: 0.1184 N_Y: 406517 N_S: 1754772 N: 406517 N_HV: 11910 Val: 0.6442 Test: 0.5827\n",
      "Epoch: 445, Loss: 0.1806 tsm_loss: 0.0228 reg_loss: 0.1579 N_Y: 405435 N_S: 1754772 N: 405435 N_HV: 11056 Val: 0.6427 Test: 0.5910\n",
      "Epoch: 446, Loss: 0.1240 tsm_loss: 0.0257 reg_loss: 0.0983 N_Y: 403214 N_S: 1754772 N: 403214 N_HV: 12462 Val: 0.6419 Test: 0.6016\n",
      "Epoch: 447, Loss: 0.1112 tsm_loss: 0.0212 reg_loss: 0.0900 N_Y: 405965 N_S: 1754772 N: 405965 N_HV: 11317 Val: 0.6296 Test: 0.5902\n",
      "Epoch: 448, Loss: 0.1466 tsm_loss: 0.0214 reg_loss: 0.1253 N_Y: 407486 N_S: 1754772 N: 407486 N_HV: 11024 Val: 0.6429 Test: 0.5970\n",
      "Epoch: 449, Loss: 0.1217 tsm_loss: 0.0223 reg_loss: 0.0994 N_Y: 405937 N_S: 1754772 N: 405937 N_HV: 11335 Val: 0.6389 Test: 0.5772\n",
      "Epoch: 450, Loss: 0.1438 tsm_loss: 0.0229 reg_loss: 0.1209 N_Y: 404943 N_S: 1754772 N: 404943 N_HV: 11759 Val: 0.6475 Test: 0.5831\n",
      "Epoch: 451, Loss: 0.1297 tsm_loss: 0.0221 reg_loss: 0.1076 N_Y: 406571 N_S: 1754772 N: 406571 N_HV: 11790 Val: 0.6415 Test: 0.5960\n",
      "Epoch: 452, Loss: 0.1212 tsm_loss: 0.0218 reg_loss: 0.0995 N_Y: 404846 N_S: 1754772 N: 404846 N_HV: 11374 Val: 0.6509 Test: 0.5856\n",
      "Epoch: 453, Loss: 0.1350 tsm_loss: 0.0204 reg_loss: 0.1147 N_Y: 406886 N_S: 1754772 N: 406886 N_HV: 10632 Val: 0.6350 Test: 0.5970\n",
      "Epoch: 454, Loss: 0.1188 tsm_loss: 0.0216 reg_loss: 0.0972 N_Y: 405700 N_S: 1754772 N: 405700 N_HV: 10957 Val: 0.6414 Test: 0.5850\n",
      "Epoch: 455, Loss: 0.1121 tsm_loss: 0.0205 reg_loss: 0.0916 N_Y: 405408 N_S: 1754772 N: 405408 N_HV: 10302 Val: 0.6407 Test: 0.5858\n",
      "Epoch: 456, Loss: 0.0952 tsm_loss: 0.0195 reg_loss: 0.0757 N_Y: 406468 N_S: 1754772 N: 406468 N_HV: 10574 Val: 0.6412 Test: 0.5943\n",
      "Epoch: 457, Loss: 0.0885 tsm_loss: 0.0164 reg_loss: 0.0720 N_Y: 405611 N_S: 1754772 N: 405611 N_HV: 9334 Val: 0.6469 Test: 0.6123\n",
      "Epoch: 458, Loss: 0.1205 tsm_loss: 0.0182 reg_loss: 0.1023 N_Y: 405630 N_S: 1754772 N: 405630 N_HV: 9502 Val: 0.6356 Test: 0.5870\n",
      "Epoch: 459, Loss: 0.1110 tsm_loss: 0.0218 reg_loss: 0.0892 N_Y: 406251 N_S: 1754772 N: 406251 N_HV: 11358 Val: 0.6432 Test: 0.5856\n",
      "Epoch: 460, Loss: 0.1179 tsm_loss: 0.0192 reg_loss: 0.0986 N_Y: 406969 N_S: 1754772 N: 406969 N_HV: 10255 Val: 0.6297 Test: 0.5852\n",
      "Epoch: 461, Loss: 0.1214 tsm_loss: 0.0225 reg_loss: 0.0989 N_Y: 407199 N_S: 1754772 N: 407199 N_HV: 10717 Val: 0.6336 Test: 0.6134\n",
      "Epoch: 462, Loss: 0.1443 tsm_loss: 0.0255 reg_loss: 0.1188 N_Y: 403692 N_S: 1754772 N: 403692 N_HV: 13086 Val: 0.6615 Test: 0.6208\n",
      "Epoch: 463, Loss: 0.1443 tsm_loss: 0.0240 reg_loss: 0.1203 N_Y: 405858 N_S: 1754772 N: 405858 N_HV: 11930 Val: 0.6297 Test: 0.6089\n",
      "Epoch: 464, Loss: 0.1588 tsm_loss: 0.0241 reg_loss: 0.1347 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 12698 Val: 0.6496 Test: 0.5653\n",
      "Epoch: 465, Loss: 0.1276 tsm_loss: 0.0221 reg_loss: 0.1056 N_Y: 406021 N_S: 1754772 N: 406021 N_HV: 11182 Val: 0.6393 Test: 0.5945\n",
      "Epoch: 466, Loss: 0.1227 tsm_loss: 0.0239 reg_loss: 0.0987 N_Y: 406628 N_S: 1754772 N: 406628 N_HV: 12401 Val: 0.6382 Test: 0.5812\n",
      "Epoch: 467, Loss: 0.1178 tsm_loss: 0.0197 reg_loss: 0.0981 N_Y: 405791 N_S: 1754772 N: 405791 N_HV: 10508 Val: 0.6332 Test: 0.6073\n",
      "Epoch: 468, Loss: 0.1049 tsm_loss: 0.0190 reg_loss: 0.0858 N_Y: 406619 N_S: 1754772 N: 406619 N_HV: 10666 Val: 0.6743 Test: 0.6420\n",
      "Epoch: 469, Loss: 0.1333 tsm_loss: 0.0243 reg_loss: 0.1090 N_Y: 406236 N_S: 1754772 N: 406236 N_HV: 12171 Val: 0.6391 Test: 0.5833\n",
      "Epoch: 470, Loss: 0.1140 tsm_loss: 0.0218 reg_loss: 0.0922 N_Y: 405856 N_S: 1754772 N: 405856 N_HV: 10955 Val: 0.6487 Test: 0.5884\n",
      "Epoch: 471, Loss: 0.1332 tsm_loss: 0.0235 reg_loss: 0.1097 N_Y: 403921 N_S: 1754772 N: 403921 N_HV: 11616 Val: 0.6429 Test: 0.5986\n",
      "Epoch: 472, Loss: 0.1127 tsm_loss: 0.0206 reg_loss: 0.0921 N_Y: 404591 N_S: 1754772 N: 404591 N_HV: 10079 Val: 0.6375 Test: 0.5934\n",
      "Epoch: 473, Loss: 0.1142 tsm_loss: 0.0211 reg_loss: 0.0931 N_Y: 406504 N_S: 1754772 N: 406504 N_HV: 10763 Val: 0.6400 Test: 0.5843\n",
      "Epoch: 474, Loss: 0.1119 tsm_loss: 0.0201 reg_loss: 0.0918 N_Y: 405277 N_S: 1754772 N: 405277 N_HV: 10413 Val: 0.6296 Test: 0.5810\n",
      "Epoch: 475, Loss: 0.1305 tsm_loss: 0.0199 reg_loss: 0.1106 N_Y: 406328 N_S: 1754772 N: 406328 N_HV: 10731 Val: 0.6536 Test: 0.5890\n",
      "Epoch: 476, Loss: 0.1086 tsm_loss: 0.0194 reg_loss: 0.0892 N_Y: 405162 N_S: 1754772 N: 405162 N_HV: 10197 Val: 0.6323 Test: 0.6016\n",
      "Epoch: 477, Loss: 0.1095 tsm_loss: 0.0218 reg_loss: 0.0877 N_Y: 406568 N_S: 1754772 N: 406568 N_HV: 11119 Val: 0.6299 Test: 0.5905\n",
      "Epoch: 478, Loss: 0.1166 tsm_loss: 0.0203 reg_loss: 0.0963 N_Y: 405976 N_S: 1754772 N: 405976 N_HV: 10842 Val: 0.6368 Test: 0.5910\n",
      "Epoch: 479, Loss: 0.1084 tsm_loss: 0.0204 reg_loss: 0.0881 N_Y: 405606 N_S: 1754772 N: 405606 N_HV: 10122 Val: 0.6388 Test: 0.5934\n",
      "Epoch: 480, Loss: 0.1349 tsm_loss: 0.0204 reg_loss: 0.1146 N_Y: 406155 N_S: 1754772 N: 406155 N_HV: 10851 Val: 0.6400 Test: 0.5838\n",
      "Epoch: 481, Loss: 0.1250 tsm_loss: 0.0175 reg_loss: 0.1075 N_Y: 406099 N_S: 1754772 N: 406099 N_HV: 9689 Val: 0.6332 Test: 0.5835\n",
      "Epoch: 482, Loss: 0.1121 tsm_loss: 0.0193 reg_loss: 0.0928 N_Y: 406771 N_S: 1754772 N: 406771 N_HV: 10443 Val: 0.6331 Test: 0.5809\n",
      "Epoch: 483, Loss: 0.1212 tsm_loss: 0.0202 reg_loss: 0.1011 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 10128 Val: 0.6315 Test: 0.6085\n",
      "Epoch: 484, Loss: 0.1158 tsm_loss: 0.0185 reg_loss: 0.0972 N_Y: 406467 N_S: 1754772 N: 406467 N_HV: 9911 Val: 0.6303 Test: 0.5931\n",
      "Epoch: 485, Loss: 0.1410 tsm_loss: 0.0232 reg_loss: 0.1178 N_Y: 406237 N_S: 1754772 N: 406237 N_HV: 11599 Val: 0.6404 Test: 0.6000\n",
      "Epoch: 486, Loss: 0.1296 tsm_loss: 0.0198 reg_loss: 0.1099 N_Y: 406665 N_S: 1754772 N: 406665 N_HV: 10216 Val: 0.6361 Test: 0.5801\n",
      "Epoch: 487, Loss: 0.1653 tsm_loss: 0.0193 reg_loss: 0.1460 N_Y: 406167 N_S: 1754772 N: 406167 N_HV: 10022 Val: 0.6371 Test: 0.5961\n",
      "Epoch: 488, Loss: 0.1738 tsm_loss: 0.0184 reg_loss: 0.1554 N_Y: 405854 N_S: 1754772 N: 405854 N_HV: 9620 Val: 0.6228 Test: 0.5841\n",
      "Epoch: 489, Loss: 0.1603 tsm_loss: 0.0268 reg_loss: 0.1335 N_Y: 407073 N_S: 1754772 N: 407073 N_HV: 13560 Val: 0.6368 Test: 0.6000\n",
      "Epoch: 490, Loss: 0.1137 tsm_loss: 0.0220 reg_loss: 0.0918 N_Y: 406441 N_S: 1754772 N: 406441 N_HV: 10765 Val: 0.6456 Test: 0.6008\n",
      "Epoch: 491, Loss: 0.1395 tsm_loss: 0.0265 reg_loss: 0.1130 N_Y: 405362 N_S: 1754772 N: 405362 N_HV: 12364 Val: 0.6347 Test: 0.5958\n",
      "Epoch: 492, Loss: 0.1064 tsm_loss: 0.0205 reg_loss: 0.0858 N_Y: 406188 N_S: 1754772 N: 406188 N_HV: 10790 Val: 0.6349 Test: 0.5908\n",
      "Epoch: 493, Loss: 0.1040 tsm_loss: 0.0179 reg_loss: 0.0862 N_Y: 406424 N_S: 1754772 N: 406424 N_HV: 9668 Val: 0.6513 Test: 0.5938\n",
      "Epoch: 494, Loss: 0.1286 tsm_loss: 0.0208 reg_loss: 0.1078 N_Y: 405259 N_S: 1754772 N: 405259 N_HV: 10894 Val: 0.6441 Test: 0.5873\n",
      "Epoch: 495, Loss: 0.1289 tsm_loss: 0.0212 reg_loss: 0.1077 N_Y: 405920 N_S: 1754772 N: 405920 N_HV: 10838 Val: 0.6452 Test: 0.5809\n",
      "Epoch: 496, Loss: 0.1240 tsm_loss: 0.0188 reg_loss: 0.1051 N_Y: 405845 N_S: 1754772 N: 405845 N_HV: 9952 Val: 0.6238 Test: 0.6112\n",
      "Epoch: 497, Loss: 0.1094 tsm_loss: 0.0186 reg_loss: 0.0908 N_Y: 405544 N_S: 1754772 N: 405544 N_HV: 9886 Val: 0.6475 Test: 0.6033\n",
      "Epoch: 498, Loss: 0.1179 tsm_loss: 0.0207 reg_loss: 0.0972 N_Y: 407269 N_S: 1754772 N: 407269 N_HV: 11274 Val: 0.6479 Test: 0.6174\n",
      "Epoch: 499, Loss: 0.1264 tsm_loss: 0.0232 reg_loss: 0.1032 N_Y: 407045 N_S: 1754772 N: 407045 N_HV: 11609 Val: 0.6483 Test: 0.6022\n",
      "Epoch: 500, Loss: 0.1246 tsm_loss: 0.0223 reg_loss: 0.1023 N_Y: 405771 N_S: 1754772 N: 405771 N_HV: 11506 Val: 0.6530 Test: 0.6043\n",
      "Epoch: 501, Loss: 0.1241 tsm_loss: 0.0198 reg_loss: 0.1043 N_Y: 406070 N_S: 1754772 N: 406070 N_HV: 9963 Val: 0.6574 Test: 0.6143\n",
      "Epoch: 502, Loss: 0.1405 tsm_loss: 0.0213 reg_loss: 0.1193 N_Y: 405607 N_S: 1754772 N: 405607 N_HV: 11392 Val: 0.6306 Test: 0.5951\n",
      "Epoch: 503, Loss: 0.1462 tsm_loss: 0.0210 reg_loss: 0.1252 N_Y: 406377 N_S: 1754772 N: 406377 N_HV: 10713 Val: 0.6237 Test: 0.6078\n",
      "Epoch: 504, Loss: 0.1189 tsm_loss: 0.0222 reg_loss: 0.0967 N_Y: 406929 N_S: 1754772 N: 406929 N_HV: 11237 Val: 0.6542 Test: 0.5879\n",
      "Epoch: 505, Loss: 0.1291 tsm_loss: 0.0227 reg_loss: 0.1064 N_Y: 402495 N_S: 1754772 N: 402495 N_HV: 11121 Val: 0.6564 Test: 0.6067\n",
      "Epoch: 506, Loss: 0.1284 tsm_loss: 0.0194 reg_loss: 0.1089 N_Y: 407296 N_S: 1754772 N: 407296 N_HV: 10441 Val: 0.6318 Test: 0.5805\n",
      "Epoch: 507, Loss: 0.1165 tsm_loss: 0.0221 reg_loss: 0.0944 N_Y: 406614 N_S: 1754772 N: 406614 N_HV: 10678 Val: 0.6465 Test: 0.6017\n",
      "Epoch: 508, Loss: 0.1187 tsm_loss: 0.0221 reg_loss: 0.0965 N_Y: 404829 N_S: 1754772 N: 404829 N_HV: 10454 Val: 0.6375 Test: 0.5976\n",
      "Epoch: 509, Loss: 0.1297 tsm_loss: 0.0226 reg_loss: 0.1071 N_Y: 406439 N_S: 1754772 N: 406439 N_HV: 11418 Val: 0.6369 Test: 0.6029\n",
      "Epoch: 510, Loss: 0.1262 tsm_loss: 0.0237 reg_loss: 0.1025 N_Y: 406650 N_S: 1754772 N: 406650 N_HV: 11339 Val: 0.6332 Test: 0.5923\n",
      "Epoch: 511, Loss: 0.1252 tsm_loss: 0.0232 reg_loss: 0.1020 N_Y: 404975 N_S: 1754772 N: 404975 N_HV: 11471 Val: 0.6385 Test: 0.5849\n",
      "Epoch: 512, Loss: 0.1178 tsm_loss: 0.0206 reg_loss: 0.0971 N_Y: 405997 N_S: 1754772 N: 405997 N_HV: 10271 Val: 0.6465 Test: 0.5841\n",
      "Epoch: 513, Loss: 0.1320 tsm_loss: 0.0187 reg_loss: 0.1133 N_Y: 406467 N_S: 1754772 N: 406467 N_HV: 9364 Val: 0.6383 Test: 0.6018\n",
      "Epoch: 514, Loss: 0.1462 tsm_loss: 0.0215 reg_loss: 0.1246 N_Y: 404153 N_S: 1754772 N: 404153 N_HV: 10704 Val: 0.6268 Test: 0.5970\n",
      "Epoch: 515, Loss: 0.1393 tsm_loss: 0.0202 reg_loss: 0.1191 N_Y: 406675 N_S: 1754772 N: 406675 N_HV: 10618 Val: 0.6351 Test: 0.5968\n",
      "Epoch: 516, Loss: 0.0914 tsm_loss: 0.0172 reg_loss: 0.0742 N_Y: 407657 N_S: 1754772 N: 407657 N_HV: 9606 Val: 0.6415 Test: 0.5931\n",
      "Epoch: 517, Loss: 0.1091 tsm_loss: 0.0184 reg_loss: 0.0907 N_Y: 406657 N_S: 1754772 N: 406657 N_HV: 9823 Val: 0.6409 Test: 0.5964\n",
      "Epoch: 518, Loss: 0.1353 tsm_loss: 0.0199 reg_loss: 0.1154 N_Y: 407471 N_S: 1754772 N: 407471 N_HV: 9840 Val: 0.6396 Test: 0.5940\n",
      "Epoch: 519, Loss: 0.1093 tsm_loss: 0.0173 reg_loss: 0.0919 N_Y: 406447 N_S: 1754772 N: 406447 N_HV: 9723 Val: 0.6366 Test: 0.5861\n",
      "Epoch: 520, Loss: 0.1046 tsm_loss: 0.0173 reg_loss: 0.0873 N_Y: 407544 N_S: 1754772 N: 407544 N_HV: 9084 Val: 0.6451 Test: 0.5995\n",
      "Epoch: 521, Loss: 0.1264 tsm_loss: 0.0178 reg_loss: 0.1086 N_Y: 406764 N_S: 1754772 N: 406764 N_HV: 9989 Val: 0.6411 Test: 0.5870\n",
      "Epoch: 522, Loss: 0.1180 tsm_loss: 0.0170 reg_loss: 0.1011 N_Y: 406043 N_S: 1754772 N: 406043 N_HV: 9405 Val: 0.6258 Test: 0.5910\n",
      "Epoch: 523, Loss: 0.1019 tsm_loss: 0.0182 reg_loss: 0.0837 N_Y: 406728 N_S: 1754772 N: 406728 N_HV: 9574 Val: 0.6392 Test: 0.6032\n",
      "Epoch: 524, Loss: 0.1158 tsm_loss: 0.0165 reg_loss: 0.0993 N_Y: 404042 N_S: 1754772 N: 404042 N_HV: 8868 Val: 0.6447 Test: 0.5955\n",
      "Epoch: 525, Loss: 0.1167 tsm_loss: 0.0169 reg_loss: 0.0998 N_Y: 405073 N_S: 1754772 N: 405073 N_HV: 9283 Val: 0.6448 Test: 0.5938\n",
      "Epoch: 526, Loss: 0.1221 tsm_loss: 0.0166 reg_loss: 0.1054 N_Y: 406379 N_S: 1754772 N: 406379 N_HV: 9009 Val: 0.6345 Test: 0.6000\n",
      "Epoch: 527, Loss: 0.1494 tsm_loss: 0.0183 reg_loss: 0.1311 N_Y: 405510 N_S: 1754772 N: 405510 N_HV: 9956 Val: 0.6280 Test: 0.5858\n",
      "Epoch: 528, Loss: 0.1239 tsm_loss: 0.0178 reg_loss: 0.1061 N_Y: 407011 N_S: 1754772 N: 407011 N_HV: 9582 Val: 0.6347 Test: 0.5851\n",
      "Epoch: 529, Loss: 0.1024 tsm_loss: 0.0158 reg_loss: 0.0866 N_Y: 406255 N_S: 1754772 N: 406255 N_HV: 8423 Val: 0.6409 Test: 0.5892\n",
      "Epoch: 530, Loss: 0.1194 tsm_loss: 0.0175 reg_loss: 0.1020 N_Y: 401427 N_S: 1754772 N: 401427 N_HV: 9226 Val: 0.6358 Test: 0.5892\n",
      "Epoch: 531, Loss: 0.1138 tsm_loss: 0.0163 reg_loss: 0.0975 N_Y: 406278 N_S: 1754772 N: 406278 N_HV: 8898 Val: 0.6337 Test: 0.5858\n",
      "Epoch: 532, Loss: 0.0904 tsm_loss: 0.0160 reg_loss: 0.0744 N_Y: 407542 N_S: 1754772 N: 407542 N_HV: 8715 Val: 0.6347 Test: 0.5877\n",
      "Epoch: 533, Loss: 0.1219 tsm_loss: 0.0180 reg_loss: 0.1039 N_Y: 406210 N_S: 1754772 N: 406210 N_HV: 9001 Val: 0.6440 Test: 0.5988\n",
      "Epoch: 534, Loss: 0.1002 tsm_loss: 0.0150 reg_loss: 0.0852 N_Y: 405808 N_S: 1754772 N: 405808 N_HV: 8575 Val: 0.6294 Test: 0.5868\n",
      "Epoch: 535, Loss: 0.1069 tsm_loss: 0.0168 reg_loss: 0.0901 N_Y: 407052 N_S: 1754772 N: 407052 N_HV: 9443 Val: 0.6416 Test: 0.5854\n",
      "Epoch: 536, Loss: 0.1266 tsm_loss: 0.0181 reg_loss: 0.1085 N_Y: 407522 N_S: 1754772 N: 407522 N_HV: 9367 Val: 0.6477 Test: 0.5891\n",
      "Epoch: 537, Loss: 0.1507 tsm_loss: 0.0205 reg_loss: 0.1302 N_Y: 405855 N_S: 1754772 N: 405855 N_HV: 10303 Val: 0.6459 Test: 0.6062\n",
      "Epoch: 538, Loss: 0.1761 tsm_loss: 0.0194 reg_loss: 0.1568 N_Y: 406114 N_S: 1754772 N: 406114 N_HV: 9629 Val: 0.6361 Test: 0.5911\n",
      "Epoch: 539, Loss: 0.1626 tsm_loss: 0.0231 reg_loss: 0.1395 N_Y: 408260 N_S: 1754772 N: 408260 N_HV: 12311 Val: 0.6502 Test: 0.5995\n",
      "Epoch: 540, Loss: 0.1840 tsm_loss: 0.0251 reg_loss: 0.1589 N_Y: 404510 N_S: 1754772 N: 404510 N_HV: 11480 Val: 0.6584 Test: 0.6157\n",
      "Epoch: 541, Loss: 0.1375 tsm_loss: 0.0189 reg_loss: 0.1187 N_Y: 404980 N_S: 1754772 N: 404980 N_HV: 9828 Val: 0.6328 Test: 0.5907\n",
      "Epoch: 542, Loss: 0.1089 tsm_loss: 0.0194 reg_loss: 0.0895 N_Y: 403498 N_S: 1754772 N: 403498 N_HV: 9918 Val: 0.6471 Test: 0.6018\n",
      "Epoch: 543, Loss: 0.1191 tsm_loss: 0.0192 reg_loss: 0.1000 N_Y: 404571 N_S: 1754772 N: 404571 N_HV: 9926 Val: 0.6486 Test: 0.6200\n",
      "Epoch: 544, Loss: 0.0989 tsm_loss: 0.0174 reg_loss: 0.0815 N_Y: 406807 N_S: 1754772 N: 406807 N_HV: 9263 Val: 0.6470 Test: 0.6130\n",
      "Epoch: 545, Loss: 0.1395 tsm_loss: 0.0181 reg_loss: 0.1214 N_Y: 406688 N_S: 1754772 N: 406688 N_HV: 9615 Val: 0.6313 Test: 0.5792\n",
      "Epoch: 546, Loss: 0.1551 tsm_loss: 0.0173 reg_loss: 0.1379 N_Y: 406542 N_S: 1754772 N: 406542 N_HV: 9088 Val: 0.6474 Test: 0.5915\n",
      "Epoch: 547, Loss: 0.1167 tsm_loss: 0.0150 reg_loss: 0.1017 N_Y: 405923 N_S: 1754772 N: 405923 N_HV: 8191 Val: 0.6479 Test: 0.5915\n",
      "Epoch: 548, Loss: 0.1501 tsm_loss: 0.0173 reg_loss: 0.1327 N_Y: 406370 N_S: 1754772 N: 406370 N_HV: 8965 Val: 0.6539 Test: 0.5849\n",
      "Epoch: 549, Loss: 0.1545 tsm_loss: 0.0191 reg_loss: 0.1354 N_Y: 406498 N_S: 1754772 N: 406498 N_HV: 9841 Val: 0.6409 Test: 0.5830\n",
      "Epoch: 550, Loss: 0.1573 tsm_loss: 0.0237 reg_loss: 0.1336 N_Y: 405784 N_S: 1754772 N: 405784 N_HV: 10963 Val: 0.6497 Test: 0.5935\n",
      "Epoch: 551, Loss: 0.1312 tsm_loss: 0.0213 reg_loss: 0.1099 N_Y: 406314 N_S: 1754772 N: 406314 N_HV: 10560 Val: 0.6393 Test: 0.5825\n",
      "Epoch: 552, Loss: 0.1243 tsm_loss: 0.0221 reg_loss: 0.1021 N_Y: 406942 N_S: 1754772 N: 406942 N_HV: 10833 Val: 0.6406 Test: 0.5978\n",
      "Epoch: 553, Loss: 0.1180 tsm_loss: 0.0215 reg_loss: 0.0965 N_Y: 407059 N_S: 1754772 N: 407059 N_HV: 11032 Val: 0.6421 Test: 0.5928\n",
      "Epoch: 554, Loss: 0.1312 tsm_loss: 0.0215 reg_loss: 0.1098 N_Y: 406120 N_S: 1754772 N: 406120 N_HV: 10524 Val: 0.6382 Test: 0.5917\n",
      "Epoch: 555, Loss: 0.1074 tsm_loss: 0.0190 reg_loss: 0.0884 N_Y: 407853 N_S: 1754772 N: 407853 N_HV: 10079 Val: 0.6228 Test: 0.5909\n",
      "Epoch: 556, Loss: 0.1107 tsm_loss: 0.0188 reg_loss: 0.0919 N_Y: 404548 N_S: 1754772 N: 404548 N_HV: 10341 Val: 0.6445 Test: 0.5898\n",
      "Epoch: 557, Loss: 0.1299 tsm_loss: 0.0186 reg_loss: 0.1113 N_Y: 404957 N_S: 1754772 N: 404957 N_HV: 10059 Val: 0.6492 Test: 0.5840\n",
      "Epoch: 558, Loss: 0.1028 tsm_loss: 0.0161 reg_loss: 0.0867 N_Y: 404438 N_S: 1754772 N: 404438 N_HV: 8456 Val: 0.6382 Test: 0.5918\n",
      "Epoch: 559, Loss: 0.1107 tsm_loss: 0.0192 reg_loss: 0.0915 N_Y: 403862 N_S: 1754772 N: 403862 N_HV: 9947 Val: 0.6441 Test: 0.5853\n",
      "Epoch: 560, Loss: 0.0989 tsm_loss: 0.0197 reg_loss: 0.0792 N_Y: 407336 N_S: 1754772 N: 407336 N_HV: 10105 Val: 0.6507 Test: 0.6035\n",
      "Epoch: 561, Loss: 0.0992 tsm_loss: 0.0166 reg_loss: 0.0826 N_Y: 407049 N_S: 1754772 N: 407049 N_HV: 9430 Val: 0.6405 Test: 0.5778\n",
      "Epoch: 562, Loss: 0.1078 tsm_loss: 0.0174 reg_loss: 0.0904 N_Y: 406436 N_S: 1754772 N: 406436 N_HV: 9227 Val: 0.6337 Test: 0.5832\n",
      "Epoch: 563, Loss: 0.1026 tsm_loss: 0.0159 reg_loss: 0.0867 N_Y: 405960 N_S: 1754772 N: 405960 N_HV: 8573 Val: 0.6284 Test: 0.5811\n",
      "Epoch: 564, Loss: 0.1051 tsm_loss: 0.0140 reg_loss: 0.0911 N_Y: 406459 N_S: 1754772 N: 406459 N_HV: 8206 Val: 0.6422 Test: 0.5821\n",
      "Epoch: 565, Loss: 0.1051 tsm_loss: 0.0159 reg_loss: 0.0892 N_Y: 405547 N_S: 1754772 N: 405547 N_HV: 8623 Val: 0.6455 Test: 0.5792\n",
      "Epoch: 566, Loss: 0.1037 tsm_loss: 0.0165 reg_loss: 0.0872 N_Y: 406011 N_S: 1754772 N: 406011 N_HV: 8680 Val: 0.6366 Test: 0.5991\n",
      "Epoch: 567, Loss: 0.1008 tsm_loss: 0.0142 reg_loss: 0.0866 N_Y: 404139 N_S: 1754772 N: 404139 N_HV: 8211 Val: 0.6364 Test: 0.5810\n",
      "Epoch: 568, Loss: 0.1049 tsm_loss: 0.0181 reg_loss: 0.0868 N_Y: 405325 N_S: 1754772 N: 405325 N_HV: 9384 Val: 0.6445 Test: 0.5910\n",
      "Epoch: 569, Loss: 0.0926 tsm_loss: 0.0176 reg_loss: 0.0750 N_Y: 406169 N_S: 1754772 N: 406169 N_HV: 9294 Val: 0.6389 Test: 0.5914\n",
      "Epoch: 570, Loss: 0.0963 tsm_loss: 0.0189 reg_loss: 0.0774 N_Y: 406198 N_S: 1754772 N: 406198 N_HV: 9668 Val: 0.6440 Test: 0.5975\n",
      "Epoch: 571, Loss: 0.1153 tsm_loss: 0.0183 reg_loss: 0.0970 N_Y: 407358 N_S: 1754772 N: 407358 N_HV: 9585 Val: 0.6311 Test: 0.5946\n",
      "Epoch: 572, Loss: 0.1112 tsm_loss: 0.0181 reg_loss: 0.0930 N_Y: 406104 N_S: 1754772 N: 406104 N_HV: 9624 Val: 0.6440 Test: 0.5858\n",
      "Epoch: 573, Loss: 0.1388 tsm_loss: 0.0205 reg_loss: 0.1182 N_Y: 406228 N_S: 1754772 N: 406228 N_HV: 10142 Val: 0.6281 Test: 0.5949\n",
      "Epoch: 574, Loss: 0.1180 tsm_loss: 0.0186 reg_loss: 0.0994 N_Y: 406010 N_S: 1754772 N: 406010 N_HV: 9543 Val: 0.6315 Test: 0.6007\n",
      "Epoch: 575, Loss: 0.1038 tsm_loss: 0.0191 reg_loss: 0.0846 N_Y: 406774 N_S: 1754772 N: 406774 N_HV: 9802 Val: 0.6336 Test: 0.5864\n",
      "Epoch: 576, Loss: 0.1131 tsm_loss: 0.0176 reg_loss: 0.0955 N_Y: 401376 N_S: 1754772 N: 401376 N_HV: 9328 Val: 0.6451 Test: 0.5871\n",
      "Epoch: 577, Loss: 0.1417 tsm_loss: 0.0185 reg_loss: 0.1232 N_Y: 407498 N_S: 1754772 N: 407498 N_HV: 9559 Val: 0.6294 Test: 0.5740\n",
      "Epoch: 578, Loss: 0.1246 tsm_loss: 0.0195 reg_loss: 0.1050 N_Y: 405086 N_S: 1754772 N: 405086 N_HV: 10505 Val: 0.6544 Test: 0.5839\n",
      "Epoch: 579, Loss: 0.1669 tsm_loss: 0.0202 reg_loss: 0.1467 N_Y: 406584 N_S: 1754772 N: 406584 N_HV: 10179 Val: 0.6499 Test: 0.6098\n",
      "Epoch: 580, Loss: 0.1343 tsm_loss: 0.0184 reg_loss: 0.1159 N_Y: 407464 N_S: 1754772 N: 407464 N_HV: 9634 Val: 0.6414 Test: 0.5852\n",
      "Epoch: 581, Loss: 0.1679 tsm_loss: 0.0204 reg_loss: 0.1475 N_Y: 404296 N_S: 1754772 N: 404296 N_HV: 10115 Val: 0.6490 Test: 0.5861\n",
      "Epoch: 582, Loss: 0.1391 tsm_loss: 0.0188 reg_loss: 0.1203 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 9371 Val: 0.6447 Test: 0.6090\n",
      "Epoch: 583, Loss: 0.1303 tsm_loss: 0.0165 reg_loss: 0.1138 N_Y: 403768 N_S: 1754772 N: 403768 N_HV: 9063 Val: 0.6413 Test: 0.6074\n",
      "Epoch: 584, Loss: 0.1065 tsm_loss: 0.0165 reg_loss: 0.0900 N_Y: 405923 N_S: 1754772 N: 405923 N_HV: 8488 Val: 0.6246 Test: 0.5886\n",
      "Epoch: 585, Loss: 0.1080 tsm_loss: 0.0159 reg_loss: 0.0921 N_Y: 403995 N_S: 1754772 N: 403995 N_HV: 8913 Val: 0.6412 Test: 0.5911\n",
      "Epoch: 586, Loss: 0.1168 tsm_loss: 0.0175 reg_loss: 0.0993 N_Y: 406358 N_S: 1754772 N: 406358 N_HV: 9408 Val: 0.6436 Test: 0.5830\n",
      "Epoch: 587, Loss: 0.1347 tsm_loss: 0.0155 reg_loss: 0.1192 N_Y: 406756 N_S: 1754772 N: 406756 N_HV: 8661 Val: 0.6340 Test: 0.5863\n",
      "Epoch: 588, Loss: 0.0967 tsm_loss: 0.0171 reg_loss: 0.0796 N_Y: 405012 N_S: 1754772 N: 405012 N_HV: 9290 Val: 0.6332 Test: 0.5943\n",
      "Epoch: 589, Loss: 0.1064 tsm_loss: 0.0170 reg_loss: 0.0894 N_Y: 406660 N_S: 1754772 N: 406660 N_HV: 8796 Val: 0.6382 Test: 0.5994\n",
      "Epoch: 590, Loss: 0.1221 tsm_loss: 0.0193 reg_loss: 0.1028 N_Y: 406883 N_S: 1754772 N: 406883 N_HV: 9239 Val: 0.6342 Test: 0.5851\n",
      "Epoch: 591, Loss: 0.1539 tsm_loss: 0.0195 reg_loss: 0.1344 N_Y: 403602 N_S: 1754772 N: 403602 N_HV: 10555 Val: 0.6339 Test: 0.5907\n",
      "Epoch: 592, Loss: 0.1233 tsm_loss: 0.0234 reg_loss: 0.0999 N_Y: 406373 N_S: 1754772 N: 406373 N_HV: 11497 Val: 0.6296 Test: 0.5946\n",
      "Epoch: 593, Loss: 0.1123 tsm_loss: 0.0193 reg_loss: 0.0930 N_Y: 405501 N_S: 1754772 N: 405501 N_HV: 10127 Val: 0.6444 Test: 0.5894\n",
      "Epoch: 594, Loss: 0.0981 tsm_loss: 0.0190 reg_loss: 0.0790 N_Y: 406363 N_S: 1754772 N: 406363 N_HV: 9529 Val: 0.6253 Test: 0.5846\n",
      "Epoch: 595, Loss: 0.1119 tsm_loss: 0.0191 reg_loss: 0.0928 N_Y: 406534 N_S: 1754772 N: 406534 N_HV: 9844 Val: 0.6339 Test: 0.5915\n",
      "Epoch: 596, Loss: 0.1107 tsm_loss: 0.0190 reg_loss: 0.0916 N_Y: 407498 N_S: 1754772 N: 407498 N_HV: 9860 Val: 0.6351 Test: 0.5874\n",
      "Epoch: 597, Loss: 0.1033 tsm_loss: 0.0187 reg_loss: 0.0847 N_Y: 406967 N_S: 1754772 N: 406967 N_HV: 9816 Val: 0.6327 Test: 0.6054\n",
      "Epoch: 598, Loss: 0.1095 tsm_loss: 0.0151 reg_loss: 0.0944 N_Y: 408400 N_S: 1754772 N: 408400 N_HV: 8608 Val: 0.6322 Test: 0.5954\n",
      "Epoch: 599, Loss: 0.1459 tsm_loss: 0.0151 reg_loss: 0.1308 N_Y: 405530 N_S: 1754772 N: 405530 N_HV: 8632 Val: 0.6340 Test: 0.5907\n",
      "Epoch: 600, Loss: 0.1326 tsm_loss: 0.0190 reg_loss: 0.1136 N_Y: 406830 N_S: 1754772 N: 406830 N_HV: 10148 Val: 0.6401 Test: 0.5849\n",
      "Epoch: 601, Loss: 0.1093 tsm_loss: 0.0149 reg_loss: 0.0944 N_Y: 407442 N_S: 1754772 N: 407442 N_HV: 8378 Val: 0.6337 Test: 0.6091\n",
      "Epoch: 602, Loss: 0.1102 tsm_loss: 0.0202 reg_loss: 0.0900 N_Y: 405879 N_S: 1754772 N: 405879 N_HV: 10092 Val: 0.6389 Test: 0.5738\n",
      "Epoch: 603, Loss: 0.1218 tsm_loss: 0.0182 reg_loss: 0.1036 N_Y: 406124 N_S: 1754772 N: 406124 N_HV: 9699 Val: 0.6388 Test: 0.6031\n",
      "Epoch: 604, Loss: 0.1154 tsm_loss: 0.0183 reg_loss: 0.0971 N_Y: 403139 N_S: 1754772 N: 403139 N_HV: 9665 Val: 0.6354 Test: 0.5925\n",
      "Epoch: 605, Loss: 0.1039 tsm_loss: 0.0172 reg_loss: 0.0867 N_Y: 406477 N_S: 1754772 N: 406477 N_HV: 9294 Val: 0.6347 Test: 0.6066\n",
      "Epoch: 606, Loss: 0.1091 tsm_loss: 0.0166 reg_loss: 0.0926 N_Y: 403884 N_S: 1754772 N: 403884 N_HV: 9009 Val: 0.6363 Test: 0.5936\n",
      "Epoch: 607, Loss: 0.1052 tsm_loss: 0.0162 reg_loss: 0.0890 N_Y: 404210 N_S: 1754772 N: 404210 N_HV: 8944 Val: 0.6478 Test: 0.5993\n",
      "Epoch: 608, Loss: 0.1262 tsm_loss: 0.0138 reg_loss: 0.1124 N_Y: 405989 N_S: 1754772 N: 405989 N_HV: 7731 Val: 0.6501 Test: 0.5959\n",
      "Epoch: 609, Loss: 0.1496 tsm_loss: 0.0156 reg_loss: 0.1340 N_Y: 407490 N_S: 1754772 N: 407490 N_HV: 8655 Val: 0.6270 Test: 0.6278\n",
      "Epoch: 610, Loss: 0.1648 tsm_loss: 0.0169 reg_loss: 0.1479 N_Y: 404498 N_S: 1754772 N: 404498 N_HV: 9449 Val: 0.6444 Test: 0.6026\n",
      "Epoch: 611, Loss: 0.1198 tsm_loss: 0.0154 reg_loss: 0.1044 N_Y: 404254 N_S: 1754772 N: 404254 N_HV: 8259 Val: 0.6510 Test: 0.5991\n",
      "Epoch: 612, Loss: 0.1091 tsm_loss: 0.0164 reg_loss: 0.0927 N_Y: 405999 N_S: 1754772 N: 405999 N_HV: 7735 Val: 0.6299 Test: 0.5876\n",
      "Epoch: 613, Loss: 0.0932 tsm_loss: 0.0177 reg_loss: 0.0755 N_Y: 405264 N_S: 1754772 N: 405264 N_HV: 8627 Val: 0.6343 Test: 0.6053\n",
      "Epoch: 614, Loss: 0.1121 tsm_loss: 0.0204 reg_loss: 0.0917 N_Y: 406125 N_S: 1754772 N: 406125 N_HV: 10298 Val: 0.6315 Test: 0.5986\n",
      "Epoch: 615, Loss: 0.1065 tsm_loss: 0.0183 reg_loss: 0.0882 N_Y: 407499 N_S: 1754772 N: 407499 N_HV: 9744 Val: 0.6373 Test: 0.5876\n",
      "Epoch: 616, Loss: 0.1051 tsm_loss: 0.0152 reg_loss: 0.0899 N_Y: 405660 N_S: 1754772 N: 405660 N_HV: 8580 Val: 0.6430 Test: 0.5922\n",
      "Epoch: 617, Loss: 0.1080 tsm_loss: 0.0170 reg_loss: 0.0910 N_Y: 406227 N_S: 1754772 N: 406227 N_HV: 8898 Val: 0.6412 Test: 0.5970\n",
      "Epoch: 618, Loss: 0.1115 tsm_loss: 0.0163 reg_loss: 0.0952 N_Y: 406474 N_S: 1754772 N: 406474 N_HV: 8835 Val: 0.6348 Test: 0.6102\n",
      "Epoch: 619, Loss: 0.1184 tsm_loss: 0.0185 reg_loss: 0.0999 N_Y: 406337 N_S: 1754772 N: 406337 N_HV: 9567 Val: 0.6347 Test: 0.6089\n",
      "Epoch: 620, Loss: 0.1228 tsm_loss: 0.0166 reg_loss: 0.1063 N_Y: 407065 N_S: 1754772 N: 407065 N_HV: 9028 Val: 0.6441 Test: 0.5982\n",
      "Epoch: 621, Loss: 0.1464 tsm_loss: 0.0158 reg_loss: 0.1306 N_Y: 405576 N_S: 1754772 N: 405576 N_HV: 8887 Val: 0.6367 Test: 0.5943\n",
      "Epoch: 622, Loss: 0.1168 tsm_loss: 0.0163 reg_loss: 0.1006 N_Y: 406797 N_S: 1754772 N: 406797 N_HV: 8692 Val: 0.6537 Test: 0.6056\n",
      "Epoch: 623, Loss: 0.1386 tsm_loss: 0.0178 reg_loss: 0.1209 N_Y: 405930 N_S: 1754772 N: 405930 N_HV: 9262 Val: 0.6457 Test: 0.5869\n",
      "Epoch: 624, Loss: 0.1357 tsm_loss: 0.0194 reg_loss: 0.1163 N_Y: 405841 N_S: 1754772 N: 405841 N_HV: 9247 Val: 0.6340 Test: 0.5855\n",
      "Epoch: 625, Loss: 0.1397 tsm_loss: 0.0259 reg_loss: 0.1138 N_Y: 404498 N_S: 1754772 N: 404498 N_HV: 10275 Val: 0.6501 Test: 0.6021\n",
      "Epoch: 626, Loss: 0.1451 tsm_loss: 0.0241 reg_loss: 0.1210 N_Y: 403478 N_S: 1754772 N: 403478 N_HV: 11470 Val: 0.6461 Test: 0.6088\n",
      "Epoch: 627, Loss: 0.1440 tsm_loss: 0.0225 reg_loss: 0.1215 N_Y: 406227 N_S: 1754772 N: 406227 N_HV: 10913 Val: 0.6546 Test: 0.6119\n",
      "Epoch: 628, Loss: 0.1526 tsm_loss: 0.0218 reg_loss: 0.1308 N_Y: 405110 N_S: 1754772 N: 405110 N_HV: 11443 Val: 0.6430 Test: 0.5790\n",
      "Epoch: 629, Loss: 0.1252 tsm_loss: 0.0206 reg_loss: 0.1047 N_Y: 406977 N_S: 1754772 N: 406977 N_HV: 10460 Val: 0.6308 Test: 0.5918\n",
      "Epoch: 630, Loss: 0.1050 tsm_loss: 0.0189 reg_loss: 0.0861 N_Y: 405935 N_S: 1754772 N: 405935 N_HV: 9819 Val: 0.6404 Test: 0.5834\n",
      "Epoch: 631, Loss: 0.1034 tsm_loss: 0.0203 reg_loss: 0.0830 N_Y: 405788 N_S: 1754772 N: 405788 N_HV: 10047 Val: 0.6415 Test: 0.5851\n",
      "Epoch: 632, Loss: 0.0907 tsm_loss: 0.0170 reg_loss: 0.0738 N_Y: 405438 N_S: 1754772 N: 405438 N_HV: 9338 Val: 0.6476 Test: 0.6081\n",
      "Epoch: 633, Loss: 0.0936 tsm_loss: 0.0160 reg_loss: 0.0776 N_Y: 407049 N_S: 1754772 N: 407049 N_HV: 8515 Val: 0.6400 Test: 0.6064\n",
      "Epoch: 634, Loss: 0.1008 tsm_loss: 0.0163 reg_loss: 0.0845 N_Y: 404542 N_S: 1754772 N: 404542 N_HV: 8693 Val: 0.6392 Test: 0.6019\n",
      "Epoch: 635, Loss: 0.1235 tsm_loss: 0.0185 reg_loss: 0.1050 N_Y: 406641 N_S: 1754772 N: 406641 N_HV: 9498 Val: 0.6437 Test: 0.5897\n",
      "Epoch: 636, Loss: 0.1157 tsm_loss: 0.0161 reg_loss: 0.0996 N_Y: 405600 N_S: 1754772 N: 405600 N_HV: 8547 Val: 0.6275 Test: 0.5963\n",
      "Epoch: 637, Loss: 0.1014 tsm_loss: 0.0154 reg_loss: 0.0860 N_Y: 404783 N_S: 1754772 N: 404783 N_HV: 8205 Val: 0.6444 Test: 0.5789\n",
      "Epoch: 638, Loss: 0.1180 tsm_loss: 0.0170 reg_loss: 0.1010 N_Y: 405958 N_S: 1754772 N: 405958 N_HV: 9066 Val: 0.6353 Test: 0.5816\n",
      "Epoch: 639, Loss: 0.1118 tsm_loss: 0.0155 reg_loss: 0.0963 N_Y: 405033 N_S: 1754772 N: 405033 N_HV: 8251 Val: 0.6472 Test: 0.5883\n",
      "Epoch: 640, Loss: 0.1142 tsm_loss: 0.0197 reg_loss: 0.0945 N_Y: 407601 N_S: 1754772 N: 407601 N_HV: 9687 Val: 0.6454 Test: 0.5707\n",
      "Epoch: 641, Loss: 0.1124 tsm_loss: 0.0178 reg_loss: 0.0946 N_Y: 406065 N_S: 1754772 N: 406065 N_HV: 8871 Val: 0.6392 Test: 0.5884\n",
      "Epoch: 642, Loss: 0.1399 tsm_loss: 0.0201 reg_loss: 0.1198 N_Y: 405822 N_S: 1754772 N: 405822 N_HV: 9355 Val: 0.6252 Test: 0.5907\n",
      "Epoch: 643, Loss: 0.1257 tsm_loss: 0.0239 reg_loss: 0.1018 N_Y: 406561 N_S: 1754772 N: 406561 N_HV: 11442 Val: 0.6352 Test: 0.5803\n",
      "Epoch: 644, Loss: 0.1005 tsm_loss: 0.0185 reg_loss: 0.0820 N_Y: 406604 N_S: 1754772 N: 406604 N_HV: 9308 Val: 0.6454 Test: 0.5895\n",
      "Epoch: 645, Loss: 0.1186 tsm_loss: 0.0160 reg_loss: 0.1026 N_Y: 406204 N_S: 1754772 N: 406204 N_HV: 8859 Val: 0.6343 Test: 0.5914\n",
      "Epoch: 646, Loss: 0.1303 tsm_loss: 0.0196 reg_loss: 0.1107 N_Y: 406189 N_S: 1754772 N: 406189 N_HV: 10210 Val: 0.6434 Test: 0.5888\n",
      "Epoch: 647, Loss: 0.1175 tsm_loss: 0.0171 reg_loss: 0.1003 N_Y: 406732 N_S: 1754772 N: 406732 N_HV: 9237 Val: 0.6360 Test: 0.5809\n",
      "Epoch: 648, Loss: 0.1002 tsm_loss: 0.0156 reg_loss: 0.0846 N_Y: 406111 N_S: 1754772 N: 406111 N_HV: 8712 Val: 0.6303 Test: 0.5861\n",
      "Epoch: 649, Loss: 0.0950 tsm_loss: 0.0153 reg_loss: 0.0797 N_Y: 407431 N_S: 1754772 N: 407431 N_HV: 8285 Val: 0.6489 Test: 0.5951\n",
      "Epoch: 650, Loss: 0.1329 tsm_loss: 0.0167 reg_loss: 0.1162 N_Y: 407150 N_S: 1754772 N: 407150 N_HV: 8787 Val: 0.6410 Test: 0.5963\n",
      "Epoch: 651, Loss: 0.1174 tsm_loss: 0.0163 reg_loss: 0.1011 N_Y: 407395 N_S: 1754772 N: 407395 N_HV: 9012 Val: 0.6364 Test: 0.5808\n",
      "Epoch: 652, Loss: 0.1902 tsm_loss: 0.0164 reg_loss: 0.1739 N_Y: 404439 N_S: 1754772 N: 404439 N_HV: 8385 Val: 0.6429 Test: 0.6071\n",
      "Epoch: 653, Loss: 0.1478 tsm_loss: 0.0149 reg_loss: 0.1328 N_Y: 407272 N_S: 1754772 N: 407272 N_HV: 8224 Val: 0.6477 Test: 0.5833\n",
      "Epoch: 654, Loss: 0.1366 tsm_loss: 0.0167 reg_loss: 0.1199 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 8750 Val: 0.6365 Test: 0.5819\n",
      "Epoch: 655, Loss: 0.1592 tsm_loss: 0.0170 reg_loss: 0.1422 N_Y: 405488 N_S: 1754772 N: 405488 N_HV: 8588 Val: 0.6337 Test: 0.6064\n",
      "Epoch: 656, Loss: 0.1381 tsm_loss: 0.0155 reg_loss: 0.1227 N_Y: 407472 N_S: 1754772 N: 407472 N_HV: 7883 Val: 0.6351 Test: 0.5863\n",
      "Epoch: 657, Loss: 0.1206 tsm_loss: 0.0152 reg_loss: 0.1054 N_Y: 405240 N_S: 1754772 N: 405240 N_HV: 8183 Val: 0.6277 Test: 0.5924\n",
      "Epoch: 658, Loss: 0.1147 tsm_loss: 0.0159 reg_loss: 0.0988 N_Y: 406658 N_S: 1754772 N: 406658 N_HV: 8989 Val: 0.6447 Test: 0.5831\n",
      "Epoch: 659, Loss: 0.1472 tsm_loss: 0.0146 reg_loss: 0.1325 N_Y: 404714 N_S: 1754772 N: 404714 N_HV: 7629 Val: 0.6489 Test: 0.6251\n",
      "Epoch: 660, Loss: 0.1257 tsm_loss: 0.0165 reg_loss: 0.1092 N_Y: 406817 N_S: 1754772 N: 406817 N_HV: 9023 Val: 0.6513 Test: 0.5848\n",
      "Epoch: 661, Loss: 0.1158 tsm_loss: 0.0175 reg_loss: 0.0983 N_Y: 407446 N_S: 1754772 N: 407446 N_HV: 8928 Val: 0.6327 Test: 0.5808\n",
      "Epoch: 662, Loss: 0.1021 tsm_loss: 0.0174 reg_loss: 0.0847 N_Y: 407357 N_S: 1754772 N: 407357 N_HV: 9614 Val: 0.6320 Test: 0.5766\n",
      "Epoch: 663, Loss: 0.1034 tsm_loss: 0.0152 reg_loss: 0.0882 N_Y: 405126 N_S: 1754772 N: 405126 N_HV: 8272 Val: 0.6494 Test: 0.5903\n",
      "Epoch: 664, Loss: 0.1059 tsm_loss: 0.0150 reg_loss: 0.0909 N_Y: 406353 N_S: 1754772 N: 406353 N_HV: 8227 Val: 0.6328 Test: 0.5843\n",
      "Epoch: 665, Loss: 0.1086 tsm_loss: 0.0154 reg_loss: 0.0932 N_Y: 405289 N_S: 1754772 N: 405289 N_HV: 8432 Val: 0.6370 Test: 0.5909\n",
      "Epoch: 666, Loss: 0.1206 tsm_loss: 0.0139 reg_loss: 0.1067 N_Y: 405628 N_S: 1754772 N: 405628 N_HV: 8300 Val: 0.6451 Test: 0.5941\n",
      "Epoch: 667, Loss: 0.1009 tsm_loss: 0.0115 reg_loss: 0.0894 N_Y: 407745 N_S: 1754772 N: 407745 N_HV: 6747 Val: 0.6446 Test: 0.6120\n",
      "Epoch: 668, Loss: 0.0978 tsm_loss: 0.0140 reg_loss: 0.0838 N_Y: 406653 N_S: 1754772 N: 406653 N_HV: 7608 Val: 0.6398 Test: 0.5867\n",
      "Epoch: 669, Loss: 0.1101 tsm_loss: 0.0146 reg_loss: 0.0955 N_Y: 406248 N_S: 1754772 N: 406248 N_HV: 7914 Val: 0.6239 Test: 0.5912\n",
      "Epoch: 670, Loss: 0.1098 tsm_loss: 0.0153 reg_loss: 0.0946 N_Y: 404587 N_S: 1754772 N: 404587 N_HV: 8450 Val: 0.6462 Test: 0.5968\n",
      "Epoch: 671, Loss: 0.0900 tsm_loss: 0.0156 reg_loss: 0.0744 N_Y: 407779 N_S: 1754772 N: 407779 N_HV: 8513 Val: 0.6414 Test: 0.5753\n",
      "Epoch: 672, Loss: 0.0965 tsm_loss: 0.0159 reg_loss: 0.0806 N_Y: 407240 N_S: 1754772 N: 407240 N_HV: 8782 Val: 0.6317 Test: 0.5819\n",
      "Epoch: 673, Loss: 0.1087 tsm_loss: 0.0155 reg_loss: 0.0932 N_Y: 404779 N_S: 1754772 N: 404779 N_HV: 8486 Val: 0.6328 Test: 0.5894\n",
      "Epoch: 674, Loss: 0.0995 tsm_loss: 0.0130 reg_loss: 0.0865 N_Y: 406302 N_S: 1754772 N: 406302 N_HV: 7579 Val: 0.6387 Test: 0.5806\n",
      "Epoch: 675, Loss: 0.1120 tsm_loss: 0.0146 reg_loss: 0.0974 N_Y: 407440 N_S: 1754772 N: 407440 N_HV: 7782 Val: 0.6400 Test: 0.5851\n",
      "Epoch: 676, Loss: 0.1153 tsm_loss: 0.0125 reg_loss: 0.1028 N_Y: 405115 N_S: 1754772 N: 405115 N_HV: 7247 Val: 0.6448 Test: 0.5863\n",
      "Epoch: 677, Loss: 0.1399 tsm_loss: 0.0136 reg_loss: 0.1263 N_Y: 407288 N_S: 1754772 N: 407288 N_HV: 7479 Val: 0.6436 Test: 0.5979\n",
      "Epoch: 678, Loss: 0.1134 tsm_loss: 0.0145 reg_loss: 0.0989 N_Y: 405757 N_S: 1754772 N: 405757 N_HV: 8187 Val: 0.6444 Test: 0.6009\n",
      "Epoch: 679, Loss: 0.1238 tsm_loss: 0.0135 reg_loss: 0.1104 N_Y: 405471 N_S: 1754772 N: 405471 N_HV: 7604 Val: 0.6425 Test: 0.5928\n",
      "Epoch: 680, Loss: 0.1241 tsm_loss: 0.0147 reg_loss: 0.1095 N_Y: 405066 N_S: 1754772 N: 405066 N_HV: 8008 Val: 0.6430 Test: 0.5882\n",
      "Epoch: 681, Loss: 0.1440 tsm_loss: 0.0156 reg_loss: 0.1284 N_Y: 406034 N_S: 1754772 N: 406034 N_HV: 8238 Val: 0.6455 Test: 0.6012\n",
      "Epoch: 682, Loss: 0.1345 tsm_loss: 0.0164 reg_loss: 0.1181 N_Y: 403376 N_S: 1754772 N: 403376 N_HV: 8660 Val: 0.6395 Test: 0.6100\n",
      "Epoch: 683, Loss: 0.1518 tsm_loss: 0.0181 reg_loss: 0.1337 N_Y: 405731 N_S: 1754772 N: 405731 N_HV: 9990 Val: 0.6533 Test: 0.5959\n",
      "Epoch: 684, Loss: 0.1133 tsm_loss: 0.0146 reg_loss: 0.0986 N_Y: 406268 N_S: 1754772 N: 406268 N_HV: 7834 Val: 0.6576 Test: 0.6100\n",
      "Epoch: 685, Loss: 0.1089 tsm_loss: 0.0169 reg_loss: 0.0920 N_Y: 406911 N_S: 1754772 N: 406911 N_HV: 9406 Val: 0.6392 Test: 0.5991\n",
      "Epoch: 686, Loss: 0.1125 tsm_loss: 0.0142 reg_loss: 0.0983 N_Y: 405630 N_S: 1754772 N: 405630 N_HV: 7829 Val: 0.6517 Test: 0.5881\n",
      "Epoch: 687, Loss: 0.1307 tsm_loss: 0.0146 reg_loss: 0.1161 N_Y: 405527 N_S: 1754772 N: 405527 N_HV: 8094 Val: 0.6618 Test: 0.5914\n",
      "Epoch: 688, Loss: 0.1202 tsm_loss: 0.0153 reg_loss: 0.1049 N_Y: 404888 N_S: 1754772 N: 404888 N_HV: 8358 Val: 0.6529 Test: 0.5979\n",
      "Epoch: 689, Loss: 0.0952 tsm_loss: 0.0153 reg_loss: 0.0799 N_Y: 403462 N_S: 1754772 N: 403462 N_HV: 8110 Val: 0.6500 Test: 0.5952\n",
      "Epoch: 690, Loss: 0.0894 tsm_loss: 0.0139 reg_loss: 0.0754 N_Y: 407068 N_S: 1754772 N: 407068 N_HV: 7645 Val: 0.6354 Test: 0.5917\n",
      "Epoch: 691, Loss: 0.0935 tsm_loss: 0.0152 reg_loss: 0.0783 N_Y: 406168 N_S: 1754772 N: 406168 N_HV: 8250 Val: 0.6399 Test: 0.5946\n",
      "Epoch: 692, Loss: 0.1114 tsm_loss: 0.0154 reg_loss: 0.0960 N_Y: 407010 N_S: 1754772 N: 407010 N_HV: 8341 Val: 0.6338 Test: 0.5957\n",
      "Epoch: 693, Loss: 0.1101 tsm_loss: 0.0155 reg_loss: 0.0947 N_Y: 406498 N_S: 1754772 N: 406498 N_HV: 8293 Val: 0.6388 Test: 0.5950\n",
      "Epoch: 694, Loss: 0.1207 tsm_loss: 0.0164 reg_loss: 0.1043 N_Y: 404873 N_S: 1754772 N: 404873 N_HV: 8995 Val: 0.6312 Test: 0.5989\n",
      "Epoch: 695, Loss: 0.1150 tsm_loss: 0.0139 reg_loss: 0.1011 N_Y: 406057 N_S: 1754772 N: 406057 N_HV: 7971 Val: 0.6362 Test: 0.5943\n",
      "Epoch: 696, Loss: 0.0940 tsm_loss: 0.0136 reg_loss: 0.0804 N_Y: 407390 N_S: 1754772 N: 407390 N_HV: 7511 Val: 0.6384 Test: 0.5821\n",
      "Epoch: 697, Loss: 0.0802 tsm_loss: 0.0115 reg_loss: 0.0687 N_Y: 406793 N_S: 1754772 N: 406793 N_HV: 6733 Val: 0.6360 Test: 0.5910\n",
      "Epoch: 698, Loss: 0.0988 tsm_loss: 0.0124 reg_loss: 0.0864 N_Y: 406583 N_S: 1754772 N: 406583 N_HV: 7008 Val: 0.6496 Test: 0.5998\n",
      "Epoch: 699, Loss: 0.0822 tsm_loss: 0.0119 reg_loss: 0.0702 N_Y: 406948 N_S: 1754772 N: 406948 N_HV: 6288 Val: 0.6313 Test: 0.5901\n",
      "Epoch: 700, Loss: 0.1039 tsm_loss: 0.0147 reg_loss: 0.0892 N_Y: 405411 N_S: 1754772 N: 405411 N_HV: 7722 Val: 0.6438 Test: 0.5943\n",
      "Epoch: 701, Loss: 0.1068 tsm_loss: 0.0132 reg_loss: 0.0937 N_Y: 408080 N_S: 1754772 N: 408080 N_HV: 7581 Val: 0.6498 Test: 0.5925\n",
      "Epoch: 702, Loss: 0.0965 tsm_loss: 0.0118 reg_loss: 0.0847 N_Y: 405694 N_S: 1754772 N: 405694 N_HV: 7109 Val: 0.6482 Test: 0.5955\n",
      "Epoch: 703, Loss: 0.0886 tsm_loss: 0.0120 reg_loss: 0.0765 N_Y: 406248 N_S: 1754772 N: 406248 N_HV: 6736 Val: 0.6432 Test: 0.6005\n",
      "Epoch: 704, Loss: 0.0953 tsm_loss: 0.0123 reg_loss: 0.0830 N_Y: 407122 N_S: 1754772 N: 407122 N_HV: 7159 Val: 0.6421 Test: 0.5804\n",
      "Epoch: 705, Loss: 0.1020 tsm_loss: 0.0121 reg_loss: 0.0899 N_Y: 405983 N_S: 1754772 N: 405983 N_HV: 7095 Val: 0.6423 Test: 0.5916\n",
      "Epoch: 706, Loss: 0.1062 tsm_loss: 0.0133 reg_loss: 0.0929 N_Y: 407325 N_S: 1754772 N: 407325 N_HV: 7398 Val: 0.6464 Test: 0.5910\n",
      "Epoch: 707, Loss: 0.1173 tsm_loss: 0.0153 reg_loss: 0.1020 N_Y: 407436 N_S: 1754772 N: 407436 N_HV: 8086 Val: 0.6390 Test: 0.5964\n",
      "Epoch: 708, Loss: 0.1357 tsm_loss: 0.0184 reg_loss: 0.1173 N_Y: 407833 N_S: 1754772 N: 407833 N_HV: 8014 Val: 0.6572 Test: 0.6006\n",
      "Epoch: 709, Loss: 0.1192 tsm_loss: 0.0169 reg_loss: 0.1023 N_Y: 405810 N_S: 1754772 N: 405810 N_HV: 9176 Val: 0.6634 Test: 0.6032\n",
      "Epoch: 710, Loss: 0.1069 tsm_loss: 0.0183 reg_loss: 0.0887 N_Y: 405034 N_S: 1754772 N: 405034 N_HV: 9483 Val: 0.6368 Test: 0.5835\n",
      "Epoch: 711, Loss: 0.1075 tsm_loss: 0.0140 reg_loss: 0.0936 N_Y: 406360 N_S: 1754772 N: 406360 N_HV: 8034 Val: 0.6428 Test: 0.6034\n",
      "Epoch: 712, Loss: 0.0933 tsm_loss: 0.0147 reg_loss: 0.0785 N_Y: 405408 N_S: 1754772 N: 405408 N_HV: 8187 Val: 0.6355 Test: 0.5902\n",
      "Epoch: 713, Loss: 0.1108 tsm_loss: 0.0163 reg_loss: 0.0945 N_Y: 405743 N_S: 1754772 N: 405743 N_HV: 8676 Val: 0.6465 Test: 0.6011\n",
      "Epoch: 714, Loss: 0.1092 tsm_loss: 0.0144 reg_loss: 0.0949 N_Y: 405743 N_S: 1754772 N: 405743 N_HV: 8202 Val: 0.6520 Test: 0.6085\n",
      "Epoch: 715, Loss: 0.1056 tsm_loss: 0.0137 reg_loss: 0.0920 N_Y: 406704 N_S: 1754772 N: 406704 N_HV: 7735 Val: 0.6512 Test: 0.5967\n",
      "Epoch: 716, Loss: 0.0977 tsm_loss: 0.0144 reg_loss: 0.0834 N_Y: 405715 N_S: 1754772 N: 405715 N_HV: 7909 Val: 0.6506 Test: 0.5884\n",
      "Epoch: 717, Loss: 0.0884 tsm_loss: 0.0133 reg_loss: 0.0751 N_Y: 406163 N_S: 1754772 N: 406163 N_HV: 7553 Val: 0.6343 Test: 0.5967\n",
      "Epoch: 718, Loss: 0.0951 tsm_loss: 0.0135 reg_loss: 0.0815 N_Y: 407154 N_S: 1754772 N: 407154 N_HV: 7407 Val: 0.6364 Test: 0.5990\n",
      "Epoch: 719, Loss: 0.0986 tsm_loss: 0.0140 reg_loss: 0.0846 N_Y: 405944 N_S: 1754772 N: 405944 N_HV: 7472 Val: 0.6374 Test: 0.5926\n",
      "Epoch: 720, Loss: 0.1074 tsm_loss: 0.0137 reg_loss: 0.0937 N_Y: 406495 N_S: 1754772 N: 406495 N_HV: 7636 Val: 0.6413 Test: 0.5867\n",
      "Epoch: 721, Loss: 0.1078 tsm_loss: 0.0169 reg_loss: 0.0909 N_Y: 406417 N_S: 1754772 N: 406417 N_HV: 9008 Val: 0.6452 Test: 0.5876\n",
      "Epoch: 722, Loss: 0.1342 tsm_loss: 0.0133 reg_loss: 0.1209 N_Y: 405644 N_S: 1754772 N: 405644 N_HV: 7442 Val: 0.6303 Test: 0.5908\n",
      "Epoch: 723, Loss: 0.1507 tsm_loss: 0.0139 reg_loss: 0.1369 N_Y: 405242 N_S: 1754772 N: 405242 N_HV: 7389 Val: 0.6560 Test: 0.5923\n",
      "Epoch: 724, Loss: 0.1307 tsm_loss: 0.0196 reg_loss: 0.1111 N_Y: 406776 N_S: 1754772 N: 406776 N_HV: 10197 Val: 0.6401 Test: 0.5983\n",
      "Epoch: 725, Loss: 0.1399 tsm_loss: 0.0161 reg_loss: 0.1238 N_Y: 406388 N_S: 1754772 N: 406388 N_HV: 8713 Val: 0.6388 Test: 0.6069\n",
      "Epoch: 726, Loss: 0.1335 tsm_loss: 0.0167 reg_loss: 0.1168 N_Y: 404889 N_S: 1754772 N: 404889 N_HV: 8633 Val: 0.6438 Test: 0.6027\n",
      "Epoch: 727, Loss: 0.1178 tsm_loss: 0.0162 reg_loss: 0.1015 N_Y: 406033 N_S: 1754772 N: 406033 N_HV: 8627 Val: 0.6445 Test: 0.5940\n",
      "Epoch: 728, Loss: 0.1322 tsm_loss: 0.0144 reg_loss: 0.1178 N_Y: 405327 N_S: 1754772 N: 405327 N_HV: 7992 Val: 0.6435 Test: 0.5945\n",
      "Epoch: 729, Loss: 0.0994 tsm_loss: 0.0161 reg_loss: 0.0834 N_Y: 404687 N_S: 1754772 N: 404687 N_HV: 8587 Val: 0.6379 Test: 0.5903\n",
      "Epoch: 730, Loss: 0.1038 tsm_loss: 0.0163 reg_loss: 0.0875 N_Y: 405917 N_S: 1754772 N: 405917 N_HV: 8521 Val: 0.6461 Test: 0.5888\n",
      "Epoch: 731, Loss: 0.0933 tsm_loss: 0.0132 reg_loss: 0.0801 N_Y: 407023 N_S: 1754772 N: 407023 N_HV: 6999 Val: 0.6495 Test: 0.5922\n",
      "Epoch: 732, Loss: 0.1077 tsm_loss: 0.0139 reg_loss: 0.0938 N_Y: 407644 N_S: 1754772 N: 407644 N_HV: 7584 Val: 0.6341 Test: 0.5985\n",
      "Epoch: 733, Loss: 0.0919 tsm_loss: 0.0151 reg_loss: 0.0768 N_Y: 405568 N_S: 1754772 N: 405568 N_HV: 8177 Val: 0.6458 Test: 0.6035\n",
      "Epoch: 734, Loss: 0.1067 tsm_loss: 0.0143 reg_loss: 0.0924 N_Y: 406363 N_S: 1754772 N: 406363 N_HV: 7754 Val: 0.6423 Test: 0.5912\n",
      "Epoch: 735, Loss: 0.0939 tsm_loss: 0.0120 reg_loss: 0.0819 N_Y: 405804 N_S: 1754772 N: 405804 N_HV: 7198 Val: 0.6254 Test: 0.5928\n",
      "Epoch: 736, Loss: 0.1007 tsm_loss: 0.0148 reg_loss: 0.0859 N_Y: 406007 N_S: 1754772 N: 406007 N_HV: 7875 Val: 0.6264 Test: 0.5844\n",
      "Epoch: 737, Loss: 0.0846 tsm_loss: 0.0136 reg_loss: 0.0710 N_Y: 406315 N_S: 1754772 N: 406315 N_HV: 7311 Val: 0.6471 Test: 0.5876\n",
      "Epoch: 738, Loss: 0.0896 tsm_loss: 0.0138 reg_loss: 0.0758 N_Y: 405894 N_S: 1754772 N: 405894 N_HV: 7698 Val: 0.6358 Test: 0.5916\n",
      "Epoch: 739, Loss: 0.0968 tsm_loss: 0.0126 reg_loss: 0.0841 N_Y: 404599 N_S: 1754772 N: 404599 N_HV: 7425 Val: 0.6445 Test: 0.5844\n",
      "Epoch: 740, Loss: 0.0968 tsm_loss: 0.0117 reg_loss: 0.0851 N_Y: 405301 N_S: 1754772 N: 405301 N_HV: 6848 Val: 0.6491 Test: 0.5968\n",
      "Epoch: 741, Loss: 0.0916 tsm_loss: 0.0127 reg_loss: 0.0789 N_Y: 406428 N_S: 1754772 N: 406428 N_HV: 7175 Val: 0.6540 Test: 0.5946\n",
      "Epoch: 742, Loss: 0.0929 tsm_loss: 0.0110 reg_loss: 0.0819 N_Y: 407289 N_S: 1754772 N: 407289 N_HV: 6552 Val: 0.6367 Test: 0.5887\n",
      "Epoch: 743, Loss: 0.0992 tsm_loss: 0.0113 reg_loss: 0.0879 N_Y: 405857 N_S: 1754772 N: 405857 N_HV: 6689 Val: 0.6315 Test: 0.5825\n",
      "Epoch: 744, Loss: 0.1044 tsm_loss: 0.0120 reg_loss: 0.0924 N_Y: 406988 N_S: 1754772 N: 406988 N_HV: 6740 Val: 0.6346 Test: 0.5938\n",
      "Epoch: 745, Loss: 0.0814 tsm_loss: 0.0118 reg_loss: 0.0696 N_Y: 407010 N_S: 1754772 N: 407010 N_HV: 6809 Val: 0.6372 Test: 0.5950\n",
      "Epoch: 746, Loss: 0.0935 tsm_loss: 0.0105 reg_loss: 0.0829 N_Y: 408010 N_S: 1754772 N: 408010 N_HV: 6308 Val: 0.6419 Test: 0.5917\n",
      "Epoch: 747, Loss: 0.1183 tsm_loss: 0.0144 reg_loss: 0.1039 N_Y: 407316 N_S: 1754772 N: 407316 N_HV: 8047 Val: 0.6622 Test: 0.5915\n",
      "Epoch: 748, Loss: 0.1232 tsm_loss: 0.0113 reg_loss: 0.1119 N_Y: 405755 N_S: 1754772 N: 405755 N_HV: 6612 Val: 0.6419 Test: 0.5823\n",
      "Epoch: 749, Loss: 0.1231 tsm_loss: 0.0106 reg_loss: 0.1124 N_Y: 406518 N_S: 1754772 N: 406518 N_HV: 6131 Val: 0.6417 Test: 0.5985\n",
      "Epoch: 750, Loss: 0.1092 tsm_loss: 0.0124 reg_loss: 0.0967 N_Y: 404972 N_S: 1754772 N: 404972 N_HV: 7127 Val: 0.6374 Test: 0.5918\n",
      "Epoch: 751, Loss: 0.1026 tsm_loss: 0.0123 reg_loss: 0.0903 N_Y: 406586 N_S: 1754772 N: 406586 N_HV: 7089 Val: 0.6462 Test: 0.6032\n",
      "Epoch: 752, Loss: 0.1165 tsm_loss: 0.0125 reg_loss: 0.1039 N_Y: 406791 N_S: 1754772 N: 406791 N_HV: 6992 Val: 0.6448 Test: 0.5867\n",
      "Epoch: 753, Loss: 0.1172 tsm_loss: 0.0149 reg_loss: 0.1023 N_Y: 406385 N_S: 1754772 N: 406385 N_HV: 7918 Val: 0.6627 Test: 0.5983\n",
      "Epoch: 754, Loss: 0.1018 tsm_loss: 0.0117 reg_loss: 0.0901 N_Y: 405379 N_S: 1754772 N: 405379 N_HV: 6824 Val: 0.6477 Test: 0.6076\n",
      "Epoch: 755, Loss: 0.0822 tsm_loss: 0.0118 reg_loss: 0.0704 N_Y: 406239 N_S: 1754772 N: 406239 N_HV: 6584 Val: 0.6407 Test: 0.5948\n",
      "Epoch: 756, Loss: 0.0946 tsm_loss: 0.0120 reg_loss: 0.0826 N_Y: 405908 N_S: 1754772 N: 405908 N_HV: 6500 Val: 0.6371 Test: 0.5884\n",
      "Epoch: 757, Loss: 0.1075 tsm_loss: 0.0128 reg_loss: 0.0947 N_Y: 405187 N_S: 1754772 N: 405187 N_HV: 7112 Val: 0.6403 Test: 0.5895\n",
      "Epoch: 758, Loss: 0.1330 tsm_loss: 0.0135 reg_loss: 0.1195 N_Y: 406782 N_S: 1754772 N: 406782 N_HV: 7314 Val: 0.6476 Test: 0.6087\n",
      "Epoch: 759, Loss: 0.1276 tsm_loss: 0.0136 reg_loss: 0.1140 N_Y: 404233 N_S: 1754772 N: 404233 N_HV: 7580 Val: 0.6388 Test: 0.5947\n",
      "Epoch: 760, Loss: 0.1594 tsm_loss: 0.0175 reg_loss: 0.1419 N_Y: 406320 N_S: 1754772 N: 406320 N_HV: 8707 Val: 0.6347 Test: 0.5917\n",
      "Epoch: 761, Loss: 0.1131 tsm_loss: 0.0157 reg_loss: 0.0974 N_Y: 406419 N_S: 1754772 N: 406419 N_HV: 8552 Val: 0.6482 Test: 0.5894\n",
      "Epoch: 762, Loss: 0.0883 tsm_loss: 0.0156 reg_loss: 0.0728 N_Y: 406148 N_S: 1754772 N: 406148 N_HV: 8653 Val: 0.6444 Test: 0.5894\n",
      "Epoch: 763, Loss: 0.0979 tsm_loss: 0.0118 reg_loss: 0.0861 N_Y: 405820 N_S: 1754772 N: 405820 N_HV: 6801 Val: 0.6495 Test: 0.6066\n",
      "Epoch: 764, Loss: 0.1098 tsm_loss: 0.0121 reg_loss: 0.0978 N_Y: 404718 N_S: 1754772 N: 404718 N_HV: 7041 Val: 0.6466 Test: 0.5942\n",
      "Epoch: 765, Loss: 0.1322 tsm_loss: 0.0121 reg_loss: 0.1201 N_Y: 404039 N_S: 1754772 N: 404039 N_HV: 7291 Val: 0.6475 Test: 0.5896\n",
      "Epoch: 766, Loss: 0.1317 tsm_loss: 0.0116 reg_loss: 0.1201 N_Y: 404103 N_S: 1754772 N: 404103 N_HV: 6826 Val: 0.6385 Test: 0.6027\n",
      "Epoch: 767, Loss: 0.1056 tsm_loss: 0.0118 reg_loss: 0.0938 N_Y: 405488 N_S: 1754772 N: 405488 N_HV: 6837 Val: 0.6427 Test: 0.6064\n",
      "Epoch: 768, Loss: 0.1243 tsm_loss: 0.0113 reg_loss: 0.1130 N_Y: 406530 N_S: 1754772 N: 406530 N_HV: 6919 Val: 0.6678 Test: 0.5984\n",
      "Epoch: 769, Loss: 0.1412 tsm_loss: 0.0114 reg_loss: 0.1298 N_Y: 405330 N_S: 1754772 N: 405330 N_HV: 6895 Val: 0.6552 Test: 0.5955\n",
      "Epoch: 770, Loss: 0.1133 tsm_loss: 0.0109 reg_loss: 0.1025 N_Y: 406767 N_S: 1754772 N: 406767 N_HV: 6535 Val: 0.6389 Test: 0.5943\n",
      "Epoch: 771, Loss: 0.0974 tsm_loss: 0.0114 reg_loss: 0.0860 N_Y: 404764 N_S: 1754772 N: 404764 N_HV: 6586 Val: 0.6481 Test: 0.5864\n",
      "Epoch: 772, Loss: 0.1221 tsm_loss: 0.0111 reg_loss: 0.1110 N_Y: 406995 N_S: 1754772 N: 406995 N_HV: 6446 Val: 0.6453 Test: 0.5903\n",
      "Epoch: 773, Loss: 0.0856 tsm_loss: 0.0111 reg_loss: 0.0745 N_Y: 405347 N_S: 1754772 N: 405347 N_HV: 6416 Val: 0.6361 Test: 0.5844\n",
      "Epoch: 774, Loss: 0.0931 tsm_loss: 0.0117 reg_loss: 0.0813 N_Y: 406078 N_S: 1754772 N: 406078 N_HV: 6650 Val: 0.6581 Test: 0.6132\n",
      "Epoch: 775, Loss: 0.0851 tsm_loss: 0.0116 reg_loss: 0.0735 N_Y: 406581 N_S: 1754772 N: 406581 N_HV: 6722 Val: 0.6447 Test: 0.5965\n",
      "Epoch: 776, Loss: 0.1041 tsm_loss: 0.0117 reg_loss: 0.0924 N_Y: 405724 N_S: 1754772 N: 405724 N_HV: 6540 Val: 0.6569 Test: 0.6135\n",
      "Epoch: 777, Loss: 0.1125 tsm_loss: 0.0123 reg_loss: 0.1002 N_Y: 406548 N_S: 1754772 N: 406548 N_HV: 7135 Val: 0.6581 Test: 0.5770\n",
      "Epoch: 778, Loss: 0.1069 tsm_loss: 0.0156 reg_loss: 0.0912 N_Y: 406564 N_S: 1754772 N: 406564 N_HV: 8014 Val: 0.6405 Test: 0.5915\n",
      "Epoch: 779, Loss: 0.1162 tsm_loss: 0.0159 reg_loss: 0.1003 N_Y: 405015 N_S: 1754772 N: 405015 N_HV: 8285 Val: 0.6498 Test: 0.5825\n",
      "Epoch: 780, Loss: 0.1059 tsm_loss: 0.0150 reg_loss: 0.0909 N_Y: 406247 N_S: 1754772 N: 406247 N_HV: 8348 Val: 0.6470 Test: 0.6078\n",
      "Epoch: 781, Loss: 0.0977 tsm_loss: 0.0139 reg_loss: 0.0839 N_Y: 406364 N_S: 1754772 N: 406364 N_HV: 7717 Val: 0.6434 Test: 0.5938\n",
      "Epoch: 782, Loss: 0.1137 tsm_loss: 0.0126 reg_loss: 0.1011 N_Y: 405882 N_S: 1754772 N: 405882 N_HV: 7183 Val: 0.6486 Test: 0.5952\n",
      "Epoch: 783, Loss: 0.0957 tsm_loss: 0.0133 reg_loss: 0.0824 N_Y: 404477 N_S: 1754772 N: 404477 N_HV: 7463 Val: 0.6447 Test: 0.5859\n",
      "Epoch: 784, Loss: 0.0931 tsm_loss: 0.0141 reg_loss: 0.0790 N_Y: 405077 N_S: 1754772 N: 405077 N_HV: 7228 Val: 0.6579 Test: 0.5971\n",
      "Epoch: 785, Loss: 0.0959 tsm_loss: 0.0137 reg_loss: 0.0822 N_Y: 406505 N_S: 1754772 N: 406505 N_HV: 7414 Val: 0.6404 Test: 0.5943\n",
      "Epoch: 786, Loss: 0.1050 tsm_loss: 0.0137 reg_loss: 0.0913 N_Y: 403163 N_S: 1754772 N: 403163 N_HV: 7520 Val: 0.6380 Test: 0.5837\n",
      "Epoch: 787, Loss: 0.0955 tsm_loss: 0.0134 reg_loss: 0.0821 N_Y: 406462 N_S: 1754772 N: 406462 N_HV: 7219 Val: 0.6511 Test: 0.5939\n",
      "Epoch: 788, Loss: 0.0795 tsm_loss: 0.0120 reg_loss: 0.0675 N_Y: 404240 N_S: 1754772 N: 404240 N_HV: 6858 Val: 0.6623 Test: 0.5994\n",
      "Epoch: 789, Loss: 0.0775 tsm_loss: 0.0109 reg_loss: 0.0666 N_Y: 405003 N_S: 1754772 N: 405003 N_HV: 6202 Val: 0.6434 Test: 0.6065\n",
      "Epoch: 790, Loss: 0.1180 tsm_loss: 0.0142 reg_loss: 0.1038 N_Y: 407386 N_S: 1754772 N: 407386 N_HV: 7678 Val: 0.6666 Test: 0.5818\n",
      "Epoch: 791, Loss: 0.1002 tsm_loss: 0.0125 reg_loss: 0.0878 N_Y: 406084 N_S: 1754772 N: 406084 N_HV: 7045 Val: 0.6440 Test: 0.5980\n",
      "Epoch: 792, Loss: 0.0971 tsm_loss: 0.0133 reg_loss: 0.0838 N_Y: 405982 N_S: 1754772 N: 405982 N_HV: 7045 Val: 0.6460 Test: 0.5878\n",
      "Epoch: 793, Loss: 0.0913 tsm_loss: 0.0105 reg_loss: 0.0808 N_Y: 405124 N_S: 1754772 N: 405124 N_HV: 6182 Val: 0.6436 Test: 0.6030\n",
      "Epoch: 794, Loss: 0.1062 tsm_loss: 0.0121 reg_loss: 0.0941 N_Y: 405599 N_S: 1754772 N: 405599 N_HV: 6680 Val: 0.6430 Test: 0.5906\n",
      "Epoch: 795, Loss: 0.0900 tsm_loss: 0.0131 reg_loss: 0.0770 N_Y: 406121 N_S: 1754772 N: 406121 N_HV: 7700 Val: 0.6514 Test: 0.5873\n",
      "Epoch: 796, Loss: 0.1360 tsm_loss: 0.0138 reg_loss: 0.1222 N_Y: 406706 N_S: 1754772 N: 406706 N_HV: 7528 Val: 0.6447 Test: 0.5885\n",
      "Epoch: 797, Loss: 0.1450 tsm_loss: 0.0138 reg_loss: 0.1312 N_Y: 407016 N_S: 1754772 N: 407016 N_HV: 7642 Val: 0.6370 Test: 0.6158\n",
      "Epoch: 798, Loss: 0.1334 tsm_loss: 0.0131 reg_loss: 0.1203 N_Y: 405813 N_S: 1754772 N: 405813 N_HV: 7261 Val: 0.6497 Test: 0.5958\n",
      "Epoch: 799, Loss: 0.1174 tsm_loss: 0.0127 reg_loss: 0.1048 N_Y: 407625 N_S: 1754772 N: 407625 N_HV: 6839 Val: 0.6408 Test: 0.5930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "/home/shenwanxiang/anaconda3/envs/clsar/lib/python3.8/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fp_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# With AC-Awareness and structure gate\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m df3 \u001b[38;5;241m=\u001b[39m \u001b[43mTest_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_gate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgate_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m df3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# With AC-Awareness and structure gate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36mTest_performance\u001b[0;34m(alpha, similarity_gate, gate_type)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#ls_his = []\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs):\n\u001b[0;32m--> 129\u001b[0m     train_loss, tsm_loss, reg_loss, n_label_triplets, n_structure_triplets, n_triplets, n_hv_triplets \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maca_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     _, _, _, _, _, _, train_n_hv_triplets, train_rmse \u001b[38;5;241m=\u001b[39m test(train_loader, model, aca_loss)\n\u001b[1;32m    132\u001b[0m     _, _, _, _, _, _, val_n_hv_triplets, val_rmse \u001b[38;5;241m=\u001b[39m test(val_loader, model, aca_loss)\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, aca_loss)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m predictions, embeddings \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat(), data\u001b[38;5;241m.\u001b[39medge_index, \n\u001b[1;32m     19\u001b[0m                                 data\u001b[38;5;241m.\u001b[39medge_attr, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m---> 22\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m \u001b[43maca_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfps_smiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp_smiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfps_scaffold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp_scaffold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msmiles_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss, reg_loss, tsm_loss,  N_Y_ACTs, N_S_ACTs, N_ACTs, N_HV_ACTs \u001b[38;5;241m=\u001b[39m loss_out\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/clsar/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Research/bidd-clsar/clsar/model/loss.py:73\u001b[0m, in \u001b[0;36mACALoss.forward\u001b[0;34m(self, labels, predictions, embeddings, fps_smiles, fps_scaffold, smiles_list)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m             labels: Tensor,\n\u001b[1;32m     67\u001b[0m             predictions: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m             fps_scaffold: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             smiles_list: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_aca_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# structure\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps_smiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_smiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps_scaffold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_scaffold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmiles_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmiles_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# parameters\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcliff_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcliff_lower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcliff_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcliff_upper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_gate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_gate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgate_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdev_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdev_mode\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/bidd-clsar/clsar/model/loss.py:370\u001b[0m, in \u001b[0;36m_aca_loss\u001b[0;34m(labels, predictions, embeddings, fps_smiles, fps_scaffold, smiles_list, alpha, cliff_lower, cliff_upper, similarity_gate, similarity_neg, similarity_pos, gate_type, squared, p, dev_mode, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# 5. Build “structure‐based” mask if requested; else all True\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m similarity_gate:\n\u001b[0;32m--> 370\u001b[0m     mask_by_s \u001b[38;5;241m=\u001b[39m \u001b[43mget_structure_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps_smiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_smiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps_scaffold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_scaffold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmiles_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmiles_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_pos\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, B, B], bool\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     N_S_ACTs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(mask_by_s\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# 6. Combine label and structure masks\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/bidd-clsar/clsar/model/loss.py:272\u001b[0m, in \u001b[0;36mget_structure_mask\u001b[0;34m(fps_smiles, fps_scaffold, smiles_list, device, similarity_neg, similarity_pos, eps)\u001b[0m\n\u001b[1;32m    269\u001b[0m j_ne_k \u001b[38;5;241m=\u001b[39m neq\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)                \u001b[38;5;66;03m# [1, B, B]\u001b[39;00m\n\u001b[1;32m    270\u001b[0m distinct_idx \u001b[38;5;241m=\u001b[39m i_ne_j \u001b[38;5;241m&\u001b[39m i_ne_k \u001b[38;5;241m&\u001b[39m j_ne_k   \u001b[38;5;66;03m# [B, B, B]\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m final_mask \u001b[38;5;241m=\u001b[39m (\u001b[43mfp_mask\u001b[49m \u001b[38;5;241m|\u001b[39m scaffold_mask \u001b[38;5;241m|\u001b[39m smiles_mask) \u001b[38;5;241m&\u001b[39m distinct_idx     \u001b[38;5;66;03m# [B, B, B]\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_mask\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fp_mask' is not defined"
     ]
    }
   ],
   "source": [
    "# train, valid, test splitting\n",
    "res1 = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "res4 = []\n",
    "\n",
    "for seed in [8, 16, 24, 42, 64, 128, 256, 512, 1024, 2048]: #,  \n",
    "    dataset = Dataset(path, name=dataset_name, pre_transform=pre_transform).shuffle(42)\n",
    "    N = len(dataset) // 5\n",
    "    val_dataset = dataset[:N]\n",
    "    test_dataset = dataset[N:2 * N]\n",
    "    train_dataset = dataset[2 * N:]\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    deg = get_deg(train_dataset)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Without AC-Awareness ($\\alpha = 0$)\n",
    "    df1 = Test_performance(alpha=0.0, similarity_gate = False)\n",
    "    df1['seed'] = seed\n",
    "\n",
    "    # With AC-Awareness ($\\alpha = 1$)\n",
    "    df2 = Test_performance(alpha=1.0, similarity_gate = False)\n",
    "    df2['seed'] = seed\n",
    "    \n",
    "    # With AC-Awareness and structure gate\n",
    "    df3 = Test_performance(alpha=1.0, similarity_gate = True, gate_type = 'OR')\n",
    "    df3['seed'] = seed\n",
    "\n",
    "    # With AC-Awareness and structure gate\n",
    "    df4 = Test_performance(alpha=1.0, similarity_gate = True, gate_type = 'AND')\n",
    "    df4['seed'] = seed    \n",
    "\n",
    "\n",
    "    res1.append(df1)\n",
    "    res2.append(df2)\n",
    "    res3.append(df3)\n",
    "    res4.append(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89985197-243c-4c24-a56c-0afd4a4600e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c58580-3e75-4e17-a09b-5a9cd24618d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59ad23-a5a4-4366-833d-001a20e59ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(res1)\n",
    "df2 = pd.concat(res2)\n",
    "df3 = pd.concat(res3)\n",
    "df4 = pd.concat(res4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f99d19-0b7a-4cb9-9f1a-02fa309afdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./results/Baseline (no ACA).csv')\n",
    "df2.to_csv('./results/ACA (label-only).csv')\n",
    "df3.to_csv('./results/ACA (label OR structure).csv')\n",
    "df4.to_csv('./results/ACA (label AND structure).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d31f4-839e-4b83-bca9-f1368556077a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31c76b-0328-4014-8c25-a783ad1a785b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a95e88-f850-48a7-8f0b-f0fe3ed48878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "colors = ['#FFE699','#00B0F0','#0a16f5', 'green']\n",
    "\n",
    "y = 'val_rmse'\n",
    "\n",
    "n1 = r'Baseline (no ACA)' # ($\\mathcal{L}_{mae}$)\n",
    "n2 = r'ACA (label-only)'\n",
    "n3 = r'ACA (label ∪ structure)'\n",
    "n4 = r'ACA (label ∩  structure)'\n",
    "\n",
    "res = []\n",
    "res_std = []\n",
    "for df, n, color in zip([df1, df2,df3,df4], [n1, n2, n3, n4], colors):\n",
    "    dfp = df.groupby('Epoch')[y].mean().to_frame(name = n).rolling(1).mean()\n",
    "    dfp_std = df.groupby('Epoch')[y].std().to_frame(name = n).rolling(1).mean()\n",
    "    res.append(dfp)\n",
    "    res_std.append(dfp_std)\n",
    "\n",
    "dfp = pd.concat(res, axis=1)\n",
    "dfp_std = pd.concat(res_std, axis=1)\n",
    "\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "\n",
    "for n, color in zip([n1, n2, n3, n4], colors):\n",
    "    ax.fill_between(dfp.index, (dfp - dfp_std)[n], (dfp + dfp_std)[n], color=color, alpha=0.2)\n",
    "\n",
    "ax.set_ylabel('Validation RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "#ax.set_xlim(1,500)\n",
    "ax.set_ylim(0.50, 1.0)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./results/Validation_RMSE.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./results/Validation_RMSE.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae243a-9b9d-4001-baab-16b8750003a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110c566-a71d-4d5c-afe0-185322298de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ab1d9-b7b4-4ec4-bc0c-d46590478bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y = 'test_rmse'\n",
    "\n",
    "\n",
    "res = []\n",
    "res_std = []\n",
    "for df, n, color in zip([df1, df2,df3,df4], [n1, n2, n3, n4], colors):\n",
    "    dfp = df.groupby('Epoch')[y].mean().to_frame(name = n).rolling(2).mean()\n",
    "    dfp_std = df.groupby('Epoch')[y].std().to_frame(name = n).rolling(2).mean()\n",
    "    res.append(dfp)\n",
    "    res_std.append(dfp_std)\n",
    "\n",
    "dfp = pd.concat(res, axis=1)\n",
    "dfp_std = pd.concat(res_std, axis=1)\n",
    "\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "\n",
    "for n, color in zip([n1, n2, n3, n4], colors):\n",
    "    ax.fill_between(dfp.index, (dfp - dfp_std)[n], (dfp + dfp_std)[n], color=color, alpha=0.2)\n",
    "\n",
    "ax.set_ylabel('Test RMSE')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_xlim(1,800)\n",
    "ax.set_ylim(0.50, 1.0)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./results/Test_RMSE.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./results/Test_RMSE.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833c01f-1177-446d-a66a-42397f63f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y = 'n_hv_triplets'\n",
    "\n",
    "\n",
    "res = []\n",
    "res_std = []\n",
    "for df, n, color in zip([df1, df2,df3,df4], [n1, n2, n3, n4], colors):\n",
    "    dfp = df.groupby('Epoch')[y].mean().to_frame(name = n).rolling(1).mean()\n",
    "    dfp_std = df.groupby('Epoch')[y].std().to_frame(name = n).rolling(1).mean()\n",
    "    res.append(dfp)\n",
    "    res_std.append(dfp_std)\n",
    "\n",
    "dfp = pd.concat(res, axis=1)\n",
    "dfp_std = pd.concat(res_std, axis=1)\n",
    "\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "\n",
    "for n, color in zip([n1, n2, n3, n4], colors):\n",
    "    ax.fill_between(dfp.index, (dfp - dfp_std)[n], (dfp + dfp_std)[n], color=color, alpha=0.2)\n",
    "\n",
    "\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "ax.set_ylabel(\"No. of HV-ACTs ($M^'$)\")\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "#ax.set_xlim(-5,500)\n",
    "\n",
    "\n",
    "fig.savefig('./results/Number_of_mined_ACTs_during_training.svg' , bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./results/Number_of_mined_ACTs_during_training.pdf' , bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec5ea8-56cc-448c-8484-78d7c9fc859e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfc08d-511f-401b-a3d4-f75920b540c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252857fd-09d4-4253-8b4a-64bac6201929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b16ad-c854-408d-9fcc-9ec0014dc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "y = 'train_triplet_loss'\n",
    "\n",
    "\n",
    "res = []\n",
    "res_std = []\n",
    "for df, n, color in zip([df1, df2,df3,df4], [n1, n2, n3, n4], colors):\n",
    "    dfp = df.groupby('Epoch')[y].mean().to_frame(name = n).rolling(1).mean()\n",
    "    dfp_std = df.groupby('Epoch')[y].std().to_frame(name = n).rolling(1).mean()\n",
    "    res.append(dfp)\n",
    "    res_std.append(dfp_std)\n",
    "\n",
    "dfp = pd.concat(res, axis=1)\n",
    "dfp_std = pd.concat(res_std, axis=1)\n",
    "\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "\n",
    "for n, color in zip([n1, n2, n3, n4], colors):\n",
    "    ax.fill_between(dfp.index, (dfp - dfp_std)[n], (dfp + dfp_std)[n], color=color, alpha=0.2)\n",
    "\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "# ax.set_xlim(-5,800)\n",
    "ax.set_ylim(-1,10)\n",
    "\n",
    "ax.set_ylabel('Training TSM Loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./results/Triplet_loss_during_training.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./results/Triplet_loss_during_training.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27b9a-5db5-4c6d-a50f-018db4be7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y = 'train_reg_loss'\n",
    "\n",
    "res = []\n",
    "res_std = []\n",
    "for df, n, color in zip([df1, df2,df3,df4], [n1, n2, n3, n4], colors):\n",
    "    dfp = df.groupby('Epoch')[y].mean().to_frame(name = n).rolling(1).mean()\n",
    "    dfp_std = df.groupby('Epoch')[y].std().to_frame(name = n).rolling(1).mean()\n",
    "    res.append(dfp)\n",
    "    res_std.append(dfp_std)\n",
    "\n",
    "dfp = pd.concat(res, axis=1)\n",
    "dfp_std = pd.concat(res_std, axis=1)\n",
    "\n",
    "\n",
    "dfp.plot(lw = 2, ax=ax,color = colors, alpha =1)\n",
    "\n",
    "for n, color in zip([n1, n2, n3, n4], colors):\n",
    "    ax.fill_between(dfp.index, (dfp - dfp_std)[n], (dfp + dfp_std)[n], color=color, alpha=0.2)\n",
    "\n",
    "ax.set_ylim(0.0, 0.8)\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_ylabel('Training MAE loss')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.legend(loc='center', bbox_to_anchor=(0.55, 0.5))\n",
    "\n",
    "#ax.set_xlim(1,800)\n",
    "\n",
    "ax.tick_params(left='off', labelleft='on', labelbottom='on', bottom = 'off',  pad=.5,)\n",
    "fig.savefig('./results/Train_mae_los.svg', bbox_inches='tight', dpi=400) \n",
    "fig.savefig('./results/Train_mae_los.pdf', bbox_inches='tight', dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314788a9-afe4-486c-a861-9e1c5708f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f5326-f4d7-42ae-935c-c20f2df351a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8aa35-2eaa-4c90-8168-4d62ff358d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4f511-db6b-4395-b2bd-2e993b9167f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16898f-b4bb-46ae-8d18-229ad9ab42b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d62189-e6b5-4d5c-8e51-bafc0c9fe2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0887761-ae13-424c-959a-07653e2eafc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
